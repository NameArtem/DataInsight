{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polic comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/88/a7186ffe1f33570ad3b8cd635996e5a3e3e155736e180ae6a2ad5e826a60/gym-0.15.3.tar.gz (1.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.6MB 20.4MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from gym) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from gym) (1.14.3)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from gym) (1.11.0)\n",
      "Collecting pyglet<=1.3.2,>=1.2.0 (from gym)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl (1.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.0MB 25.5MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting cloudpickle~=1.2.0 (from gym)\n",
      "  Downloading https://files.pythonhosted.org/packages/c1/49/334e279caa3231255725c8e860fa93e72083567625573421db8875846c14/cloudpickle-1.2.2-py2.py3-none-any.whl\n",
      "Collecting future (from pyglet<=1.3.2,>=1.2.0->gym)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/bf/57733d44afd0cf67580658507bd11d3ec629612d5e0e432beb4b8f6fbb04/future-0.18.1.tar.gz (828kB)\n",
      "\u001b[K    100% |████████████████████████████████| 829kB 30.6MB/s ta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: gym, future\n",
      "  Running setup.py bdist_wheel for gym ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/8a/71/10/30f9b16332ecfd6318ac290445c696fe809bcbe40a05f9a799\n",
      "  Running setup.py bdist_wheel for future ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/74/bc/50/ae030267a192919b289f84661cfeb5573cf383841c543e8696\n",
      "Successfully built gym future\n",
      "Installing collected packages: future, pyglet, cloudpickle, gym\n",
      "  Found existing installation: cloudpickle 0.5.3\n",
      "    Uninstalling cloudpickle-0.5.3:\n",
      "      Successfully uninstalled cloudpickle-0.5.3\n",
      "Successfully installed cloudpickle-1.2.2 future-0.18.1 gym-0.15.3 pyglet-1.3.2\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import time\n",
    "import gym "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly select policy among the best action method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution\n",
    "def execute(env, policy, episodeLength=100, render=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      policy: [S, A] shaped matrix representing the policy.\n",
    "      env: OpenAI gym env.\n",
    "      render: boolean to turn rendering on/off.\n",
    "    \"\"\" \n",
    "    totalReward = 0\n",
    "    start = env.reset()\n",
    "    for t in range(episodeLength):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action = policy[start]\n",
    "        start, reward, done, _ = env.step(action)\n",
    "        totalReward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return totalReward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def evaluatePolicy(env, policy, n_episodes=100):\n",
    "    totalReward = 0.0\n",
    "    for _ in range(n_episodes):\n",
    "        totalReward += execute(env, policy)\n",
    "    return totalReward / n_episodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for a random policy\n",
    "def gen_random_policy():\n",
    "    return numpy.random.choice(4, size=((16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score = 0.52. Time taken = 12.0044 seconds\n"
     ]
    }
   ],
   "source": [
    "# Generate GymAI environment\n",
    "env = gym.make('FrozenLake-v0')\n",
    "\n",
    "# Policy search\n",
    "n_policies   = 1000\n",
    "startTime    = time.time()\n",
    "policy_set   = [gen_random_policy() for _ in range(n_policies)]\n",
    "policy_score = [evaluatePolicy(env, p) for p in policy_set]\n",
    "endTime = time.time()\n",
    "print(\"Best score = %0.2f. Time taken = %4.4f seconds\" %(numpy.max(policy_score) ,\n",
    "endTime - startTime)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best score = 0.52. Time taken = 12.0044 seconds.\n",
    "\n",
    "This searches the environment for best policy in a random set of 1000 solutions and evaluates them. The best policy score we get is 0.40 in 12.0044 seconds. It means that the chance of agent reaching the goal is 52%. It is not even close to our goal. Random search does not work well\n",
    "for complex problems where the search space is huge.\n",
    "\n",
    "The goal of the Agent is to pick the best policy that will maximize the total rewards received from\n",
    "the environment. \n",
    "\n",
    "### Value-Iteration method select policy among the best action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(env, policy, gamma=1.0, render=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      policy: [S, A] shaped matrix representing the policy.\n",
    "      env: OpenAI gym env.\n",
    "      env.P represents the transition probabilities of the environment.\n",
    "      env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "      env.nS is a number of states in the environment.\n",
    "      env.nA is a number of actions in the environment.\n",
    "      gamma: Gamma discount factor.\n",
    "      render: boolean to turn rendering on/off.\n",
    "    \"\"\" \n",
    "    start = env.reset()\n",
    "    totalReward = 0\n",
    "    stepIndex = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render() \n",
    "        action = int(policy[start])\n",
    "        start, reward, done, _ = env.step(action)\n",
    "        totalReward += (gamma ** stepIndex * reward)\n",
    "        stepIndex += 1\n",
    "        if done:\n",
    "            break\n",
    "    return totalReward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluates a policy by running it n times.returns:average total reward\n",
    "def evaluatePolicy(env, policy, gamma=1.0, n=100):\n",
    "    scores = [execute(env, policy, gamma=gamma, render=False) for _ in range(n)]\n",
    "    return numpy.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing the policy given a value-function\n",
    "def calculatePolicy(v, gamma=1.0):\n",
    "    policy = numpy.zeros(env.env.nS)\n",
    "\n",
    "    for s in range(env.env.nS):\n",
    "        q_sa = numpy.zeros(env.action_space.n)\n",
    "\n",
    "        for a in range(env.action_space.n):\n",
    "\n",
    "            for next_sr in env.env.P[s][a]:\n",
    "                # next_sr is a tuple of (probability, next state, reward, done)\n",
    "                p, s_, r, _ = next_sr\n",
    "                # Bellman equation\n",
    "                q_sa[a] += (p * (r + gamma * v[s_]))\n",
    "\n",
    "        policy[s] = numpy.argmax(q_sa)\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Iteration Agorithm\n",
    "def valueIteration(env, gamma=1.0):\n",
    "    value = numpy.zeros(env.env.nS) # initialize value-function\n",
    "    max_iterations = 10000\n",
    "    eps = 1e-20\n",
    " \n",
    "    for i in range(max_iterations):\n",
    "        prev_v = numpy.copy(value)\n",
    "        \n",
    "        for s in range(env.env.nS): \n",
    "            q_sa = [sum([p * (r + prev_v[s_]) for p, s_, r, _ in env.env.P[s][a]]) for a in range(env.env.nA)] \n",
    "            value[s] = max(q_sa)  \n",
    "\n",
    "        if (numpy.sum(numpy.fabs(prev_v - value)) <= eps):\n",
    "            print('Value-iteration converged at # %d.' % (i + 1))\n",
    "            break\n",
    "\n",
    "    return value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value-iteration converged at # 1373.\n",
      "Best score = 0.76. Time taken = 0.5340 seconds\n"
     ]
    }
   ],
   "source": [
    "gamma = 1.0\n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "optimalValue = valueIteration(env, gamma);\n",
    "startTime = time.time()\n",
    "policy = calculatePolicy(optimalValue, gamma)\n",
    "policy_score = evaluatePolicy(env, policy, gamma, n=1000)\n",
    "endTime = time.time()\n",
    "print(\"Best score = %0.2f. Time taken = %4.4f seconds\" % (numpy.mean(policy_score), endTime - startTime)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "Value-iteration converged at # 1373.\n",
    "\n",
    "Best score = 0.76. Time taken = 0.4184 seconds.\n",
    "\n",
    "### Policy-Iteration method\n",
    "\n",
    "The value-iteration algorithm keeps improving the value function at each iteration, until the valuefunction converges. Since the agent only cares about the finding the optimal policy, sometimes the optimal policy will converge before the value function. Therefore, we have another algorithm called policy-iteration. Instead of repeatedly improving the value-function estimate, it will re-define the policy at each step and compute the value according to this new policy until the policy converges. Policy iteration is also guaranteed to converge to the optimal policy and it often takes less iterations to converge than the value-iteration algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# executes an episode\n",
    "def execute(env, policy, gamma = 1.0, render = False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      policy: [S, A] shaped matrix representing the policy.\n",
    "      env: OpenAI gym env.\n",
    "      env.P represents the transition probabilities of the environment.\n",
    "      env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "      env.nS is a number of states in the environment.\n",
    "      env.nA is a number of actions in the environment.\n",
    "      gamma: Gamma discount factor.\n",
    "      render: boolean to turn rendering on/off.\n",
    "    \"\"\" \n",
    "    start = env.reset()\n",
    "    totalReward = 0\n",
    "    stepIndex = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        action = int(policy[start])\n",
    "        start, reward, done , _ = env.step(action)\n",
    "        totalReward += (gamma ** stepIndex * reward)\n",
    "        stepIndex += 1\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "      \n",
    "    return totalReward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# executes an episode\n",
    "def execute(env, policy, gamma = 1.0, render = False):\n",
    "    start = env.reset()\n",
    "    totalReward = 0\n",
    "    stepIndex = 0\n",
    "\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        action = int(policy[start])\n",
    "        start, reward, done , _ = env.step(action)\n",
    "        totalReward += (gamma ** stepIndex * reward)\n",
    "        stepIndex += 1\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    return totalReward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluatePolicy(env, policy, gamma = 1.0, n = 100):\n",
    "    scores = [execute(env, policy, gamma, False) for _ in range(n)]\n",
    "    return numpy.mean(scores) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the policy given a value-function\n",
    "def extractPolicy(v, gamma = 1.0):\n",
    "    policy = numpy.zeros(env.env.nS)\n",
    "\n",
    "    for s in range(env.env.nS):\n",
    "        q_sa = numpy.zeros(env.env.nA)\n",
    "\n",
    "        for a in range(env.env.nA):\n",
    "            q_sa[a] = sum([p * (r + gamma * v[s_]) for p, s_, r, _ in env.env.P[s][a]])\n",
    "\n",
    "        policy[s] = numpy.argmax(q_sa)\n",
    "        \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteratively calculates the value-function under policy.\n",
    "def CalcPolicyValue(env, policy, gamma=1.0):\n",
    "    value = numpy.zeros(env.env.nS)\n",
    "    eps = 1e-10\n",
    "    \n",
    "    while True:\n",
    "        previousValue = numpy.copy(value)\n",
    "\n",
    "        for states in range(env.env.nS):\n",
    "            policy_a = policy[states]\n",
    "            value[states] = sum([p * (r + gamma * previousValue[s_]) for p, s_, r, _ in env.env.P[states][policy_a]])\n",
    "        if (numpy.sum((numpy.fabs(previousValue - value))) <= eps):\n",
    "            # value converged\n",
    "            break\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PolicyIteration algorithm\n",
    "def policyIteration(env, gamma = 1.0):\n",
    "    policy = numpy.random.choice(env.env.nA, size=(env.env.nS)) # initialize a random policy\n",
    "    maxIterations = 1000\n",
    "    gamma = 1.0\n",
    "\n",
    "    for i in range(maxIterations):\n",
    "        oldPolicyValue = CalcPolicyValue(env, policy, gamma)\n",
    "        newPolicy = extractPolicy(oldPolicyValue, gamma)\n",
    "    \n",
    "        if (numpy.all(policy == newPolicy)):\n",
    "            print ('Policy Iteration converged at %d.' %(i+1))\n",
    "            break\n",
    "    \n",
    "        policy = newPolicy\n",
    "\n",
    "    return policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration converged at 4.\n",
      "Best score = 0.64. Time taken = 0.1731 seconds\n"
     ]
    }
   ],
   "source": [
    "env_name = 'FrozenLake-v0'\n",
    "env = gym.make(env_name)\n",
    "start = time.time()\n",
    "optimalPolicy = policyIteration(env, gamma = 1.0)\n",
    "scores = evaluatePolicy(env, optimalPolicy, gamma = 1.0)\n",
    "end = time.time()\n",
    "print(\"Best score = %0.2f. Time taken = %4.4f seconds\" %(numpy.max(scores) , end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "Policy-Iteration converged at step 5.\n",
    "\n",
    "Best score = 0.64. Time taken = 0.1731 seconds\n",
    "\n",
    "### Value-Iteration vs Policy-Iteration\n",
    "\n",
    "Both value-iteration and policy-iteration algorithms can be used for offline planning where the agent is assumed to have prior knowledge about the effects of its actions on the environment (they assume the MDP model is known). Comparing each other, policy-iteration is computationally efficient as it often takes considerably fewer number of iterations to converge although each iteration is more computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
