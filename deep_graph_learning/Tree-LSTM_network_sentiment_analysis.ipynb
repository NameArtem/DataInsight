{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-LSTM networks for sentiment analysis \n",
    "\n",
    "We will use Constituency Binary Tree-LSTM (instead of Child-Sum) which is  a generalization of long short-term memory (LSTM) networks to tree-structured network topologies using DGL to build a \"latent tree\".\n",
    "\n",
    "### Stanford Sentiment Treebank Dataset\n",
    "\n",
    "The dataset provides a fine-grained, tree-level sentiment annotation. There are five classes: Very negative, negative, neutral, positive, and very positive, which indicate the sentiment in the current subtree. Note: Non-leaf nodes in a constituency tree do not contain words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "from dgl.data.tree import SST\n",
    "from dgl.data import SSTBatch\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as pl\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "\n",
    "import dgl.function as fn\n",
    "import torch as th\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Dataset creation finished. #Trees: 5\n"
     ]
    }
   ],
   "source": [
    "# Each sample in the dataset is a constituency tree. The leaf nodes\n",
    "# represent words. The word is an int value stored in the \"x\" field.\n",
    "# The non-leaf nodes have a special word PAD_WORD. The sentiment\n",
    "# label is stored in the \"y\" feature field.\n",
    "trainset = SST(mode='tiny')  # the \"tiny\" set has only five trees\n",
    "tiny_sst = trainset.trees\n",
    "num_vocabs = trainset.num_vocabs\n",
    "num_classes = trainset.num_classes\n",
    "\n",
    "vocab = trainset.vocab # vocabulary dict: key -> id\n",
    "inv_vocab = {v: k for k, v in vocab.items()} # inverted vocabulary dict: id -> word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the rock is destined to be the 21st century 's new `` conan '' and that he 's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . "
     ]
    }
   ],
   "source": [
    "a_tree = tiny_sst[0]\n",
    "for token in a_tree.ndata['x'].tolist():\n",
    "    if token != trainset.PAD_WORD:\n",
    "        print(inv_vocab[token], end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = dgl.batch(tiny_sst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tree(g):\n",
    "    # this plot requires pygraphviz package\n",
    "    pos = nx.nx_agraph.graphviz_layout(g, prog='dot')\n",
    "    nx.draw(g, pos, with_labels=False, node_size=10,\n",
    "            node_color=[[.5, .5, .5]], arrowsize=4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_tree(graph.to_networkx())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeLSTMCell(nn.Module):\n",
    "    def __init__(self, x_size, h_size):\n",
    "        super(TreeLSTMCell, self).__init__()\n",
    "        self.W_iou = nn.Linear(x_size, 3 * h_size, bias=False)\n",
    "        self.U_iou = nn.Linear(2 * h_size, 3 * h_size, bias=False)\n",
    "        self.b_iou = nn.Parameter(th.zeros(1, 3 * h_size))\n",
    "        self.U_f = nn.Linear(2 * h_size, 2 * h_size)\n",
    "\n",
    "    def message_func(self, edges):\n",
    "        return {'h': edges.src['h'], 'c': edges.src['c']}\n",
    "\n",
    "    def reduce_func(self, nodes):\n",
    "        # concatenate h_jl for equation (1), (2), (3), (4)\n",
    "        h_cat = nodes.mailbox['h'].view(nodes.mailbox['h'].size(0), -1)\n",
    "        # equation (2)\n",
    "        f = th.sigmoid(self.U_f(h_cat)).view(*nodes.mailbox['h'].size())\n",
    "        # second term of equation (5)\n",
    "        c = th.sum(f * nodes.mailbox['c'], 1)\n",
    "        return {'iou': self.U_iou(h_cat), 'c': c}\n",
    "\n",
    "    def apply_node_func(self, nodes):\n",
    "        # equation (1), (3), (4)\n",
    "        iou = nodes.data['iou'] + self.b_iou\n",
    "        i, o, u = th.chunk(iou, 3, 1)\n",
    "        i, o, u = th.sigmoid(i), th.sigmoid(o), th.tanh(u)\n",
    "        # equation (5)\n",
    "        c = i * u + nodes.data['c']\n",
    "        # equation (6)\n",
    "        h = o * th.tanh(c)\n",
    "        return {'h' : h, 'c' : c}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traversing one tree:\n",
      "(tensor([ 2,  3,  6,  8, 13, 15, 17, 19, 22, 23, 25, 27, 28, 29, 30, 32, 34, 36,\n",
      "        38, 40, 43, 46, 47, 49, 50, 52, 58, 59, 60, 62, 64, 65, 66, 68, 69, 70]), tensor([ 1, 21, 26, 45, 48, 57, 63, 67]), tensor([24, 44, 56, 61]), tensor([20, 42, 55]), tensor([18, 54]), tensor([16, 53]), tensor([14, 51]), tensor([12, 41]), tensor([11, 39]), tensor([10, 37]), tensor([35]), tensor([33]), tensor([31]), tensor([9]), tensor([7]), tensor([5]), tensor([4]), tensor([0]))\n",
      "Traversing many trees at the same time:\n",
      "(tensor([  2,   3,   6,   8,  13,  15,  17,  19,  22,  23,  25,  27,  28,  29,\n",
      "         30,  32,  34,  36,  38,  40,  43,  46,  47,  49,  50,  52,  58,  59,\n",
      "         60,  62,  64,  65,  66,  68,  69,  70,  74,  76,  78,  79,  82,  83,\n",
      "         85,  88,  90,  92,  93,  95,  96, 100, 102, 103, 105, 109, 110, 112,\n",
      "        113, 117, 118, 119, 121, 125, 127, 129, 130, 132, 133, 135, 138, 140,\n",
      "        141, 142, 143, 150, 152, 153, 155, 158, 159, 161, 162, 164, 168, 170,\n",
      "        171, 174, 175, 178, 179, 182, 184, 185, 187, 189, 190, 191, 192, 195,\n",
      "        197, 198, 200, 202, 205, 208, 210, 212, 213, 214, 216, 218, 219, 220,\n",
      "        223, 225, 227, 229, 230, 232, 235, 237, 240, 242, 244, 246, 248, 249,\n",
      "        251, 253, 255, 256, 257, 259, 262, 263, 267, 269, 270, 271, 272]), tensor([  1,  21,  26,  45,  48,  57,  63,  67,  77,  81,  91,  94, 101, 108,\n",
      "        111, 116, 128, 131, 139, 151, 157, 160, 169, 173, 177, 183, 188, 196,\n",
      "        211, 217, 228, 247, 254, 261, 268]), tensor([ 24,  44,  56,  61,  75,  89,  99, 107, 115, 126, 137, 149, 156, 167,\n",
      "        181, 186, 194, 209, 215, 226, 245, 252, 266]), tensor([ 20,  42,  55,  73,  87, 124, 136, 154, 180, 207, 224, 243, 250, 265]), tensor([ 18,  54,  86, 123, 134, 148, 176, 206, 222, 241, 264]), tensor([ 16,  53,  84, 122, 172, 204, 239, 260]), tensor([ 14,  51,  80, 120, 166, 203, 238, 258]), tensor([ 12,  41,  72, 114, 165, 201, 236]), tensor([ 11,  39, 106, 163, 199, 234]), tensor([ 10,  37, 104, 147, 193, 233]), tensor([ 35,  98, 146, 231]), tensor([ 33,  97, 145, 221]), tensor([ 31,  71, 144]), tensor([9]), tensor([7]), tensor([5]), tensor([4]), tensor([0]))\n"
     ]
    }
   ],
   "source": [
    "print('Traversing one tree:')\n",
    "print(dgl.topological_nodes_generator(a_tree))\n",
    "\n",
    "print('Traversing many trees at the same time:')\n",
    "print(dgl.topological_nodes_generator(graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.ndata['a'] = th.ones(graph.number_of_nodes(), 1)\n",
    "graph.register_message_func(fn.copy_src('a', 'a'))\n",
    "graph.register_reduce_func(fn.sum('a', 'a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "traversal_order = dgl.topological_nodes_generator(graph)\n",
    "graph.prop_nodes(traversal_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeLSTM(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_vocabs,\n",
    "                 x_size,\n",
    "                 h_size,\n",
    "                 num_classes,\n",
    "                 dropout,\n",
    "                 pretrained_emb=None):\n",
    "        super(TreeLSTM, self).__init__()\n",
    "        self.x_size = x_size\n",
    "        self.embedding = nn.Embedding(num_vocabs, x_size)\n",
    "        if pretrained_emb is not None:\n",
    "            print('Using glove')\n",
    "            self.embedding.weight.data.copy_(pretrained_emb)\n",
    "            self.embedding.weight.requires_grad = True\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(h_size, num_classes)\n",
    "        self.cell = TreeLSTMCell(x_size, h_size)\n",
    "\n",
    "    def forward(self, batch, h, c):\n",
    "        \"\"\"Compute tree-lstm prediction given a batch.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch : dgl.data.SSTBatch\n",
    "            The data batch.\n",
    "        h : Tensor\n",
    "            Initial hidden state.\n",
    "        c : Tensor\n",
    "            Initial cell state.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        logits : Tensor\n",
    "            The prediction of each node.\n",
    "        \"\"\"\n",
    "        g = batch.graph\n",
    "        g.register_message_func(self.cell.message_func)\n",
    "        g.register_reduce_func(self.cell.reduce_func)\n",
    "        g.register_apply_node_func(self.cell.apply_node_func)\n",
    "        # feed embedding\n",
    "        embeds = self.embedding(batch.wordid * batch.mask)\n",
    "        g.ndata['iou'] = self.cell.W_iou(self.dropout(embeds)) * batch.mask.float().unsqueeze(-1)\n",
    "        g.ndata['h'] = h\n",
    "        g.ndata['c'] = c\n",
    "        # propagate\n",
    "        dgl.prop_nodes_topo(g)\n",
    "        # compute logits\n",
    "        h = self.dropout(g.ndata.pop('h'))\n",
    "        logits = self.linear(h)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = th.device('cpu')\n",
    "# hyper parameters\n",
    "x_size = 256\n",
    "h_size = 256\n",
    "dropout = 0.5\n",
    "lr = 0.05\n",
    "weight_decay = 1e-4\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TreeLSTM(\n",
      "  (embedding): Embedding(19536, 256)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (linear): Linear(in_features=256, out_features=5, bias=True)\n",
      "  (cell): TreeLSTMCell(\n",
      "    (W_iou): Linear(in_features=256, out_features=768, bias=False)\n",
      "    (U_iou): Linear(in_features=512, out_features=768, bias=False)\n",
      "    (U_f): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = TreeLSTM(trainset.num_vocabs,\n",
    "                 x_size,\n",
    "                 h_size,\n",
    "                 trainset.num_classes,\n",
    "                 dropout)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the optimizer\n",
    "optimizer = th.optim.Adagrad(model.parameters(),\n",
    "                          lr=lr,\n",
    "                          weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batcher(dev):\n",
    "    def batcher_dev(batch):\n",
    "        batch_trees = dgl.batch(batch)\n",
    "        return SSTBatch(graph=batch_trees,\n",
    "                        mask=batch_trees.ndata['mask'].to(device),\n",
    "                        wordid=batch_trees.ndata['x'].to(device),\n",
    "                        label=batch_trees.ndata['y'].to(device))\n",
    "    return batcher_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=tiny_sst,\n",
    "                          batch_size=5,\n",
    "                          collate_fn=batcher(device),\n",
    "                          shuffle=False,\n",
    "                          num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Step 00000 | Loss 442.1177 | Acc 0.2125 |\n",
      "Epoch 00001 | Step 00000 | Loss 276.3337 | Acc 0.7436 |\n",
      "Epoch 00002 | Step 00000 | Loss 1202.9537 | Acc 0.3370 |\n",
      "Epoch 00003 | Step 00000 | Loss 502.2912 | Acc 0.5311 |\n",
      "Epoch 00004 | Step 00000 | Loss 170.1425 | Acc 0.8352 |\n",
      "Epoch 00005 | Step 00000 | Loss 232.8949 | Acc 0.7436 |\n",
      "Epoch 00006 | Step 00000 | Loss 167.7142 | Acc 0.8425 |\n",
      "Epoch 00007 | Step 00000 | Loss 105.0815 | Acc 0.8755 |\n",
      "Epoch 00008 | Step 00000 | Loss 82.5724 | Acc 0.9377 |\n",
      "Epoch 00009 | Step 00000 | Loss 62.3340 | Acc 0.9377 |\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(epochs):\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        g = batch.graph\n",
    "        n = g.number_of_nodes()\n",
    "        h = th.zeros((n, h_size))\n",
    "        c = th.zeros((n, h_size))\n",
    "        logits = model(batch, h, c)\n",
    "        logp = F.log_softmax(logits, 1)\n",
    "        loss = F.nll_loss(logp, batch.label, reduction='sum')\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pred = th.argmax(logits, 1)\n",
    "        acc = float(th.sum(th.eq(batch.label, pred))) / len(batch.label)\n",
    "        print(\"Epoch {:05d} | Step {:05d} | Loss {:.4f} | Acc {:.4f} |\".format(\n",
    "            epoch, step, loss.item(), acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source:\n",
    "    - https://github.com/dmlc/dgl/tree/master/examples/pytorch/tree_lstm\n",
    "    - https://docs.dgl.ai/tutorials/models/2_small_graph/3_tree-lstm.html#sphx-glr-tutorials-models-2-small-graph-3-tree-lstm-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
