{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning: Q-learning with value iteration\n",
    "\n",
    "### Frozen Lake Game\n",
    "\n",
    "winter is here. You and your friends where tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is splippery, so you won't always move in the diretion you intended. Teh surface is described by using a grid like the following:\n",
    "    \n",
    "    SFFF\n",
    "    FHFH\n",
    "    FFFH\n",
    "    HFFG\n",
    "    \n",
    "   Where S stands for start where the agent begins, F for frozen surface (safe), H for hole, and G for goal of getting frisbee. \n",
    "    \n",
    "\n",
    "\n",
    "### Setting library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can query information about the environment, sample states and actions, \n",
    "# retreive rewards, and have agent navigate the envorinment\n",
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create the Q-table and initialize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of the action space \n",
    "action_space_size = env.action_space.n\n",
    "# size of the state space in the environment\n",
    "state_space_size = env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build Q-table and fill it with zeros\n",
    "q_table = np.zeros((state_space_size, action_space_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Initializing Q-learning hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and initialize all the parameters needed to implement the Q-learning algorithm.\n",
    "num_episodes = 10000 # number of plays during training\n",
    "max_steps_per_episode = 100 # max number of steps agent allowed to take in a single episode\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "\n",
    "# related to the exploration-exploitation trade-off using epsilon-greedy policy\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01   # max and min are the bounds to how large or small our exploration rate can be\n",
    "exploration_decay_rate = 0.001\n",
    "\n",
    "# Note: exploration rate was represented by epsilon "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The lower the exploration decay rate, the longer the agent will be able to explore. With 0.01 as the decay rate, the agent was only able to explore for a relatively short amount of time until it went into full exploitation mode without having a chance to fully explore and learn about the environment. Decreasing the decay rate to 0.001 allowed the agent to explore for longer and learn more.\n",
    "\n",
    "If the exploration decay rate is too large, the 2nd term in the “exploration-rate update” is ≈ 0 (because the exponential term is ≈ 0). The impact is that subsequent epsilon-greedy searches get stuck in an “exploitation” mode, since the exploration rate converges to \"min_exploration_rate\" (little to no exploration occurs). \n",
    "\n",
    "These behavior would come much more clear if the game was deterministic (no slipping on ice), since the slippery situation adds a randomness which contributes to hide the phenomenon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: The Q learning algorithm\n",
    "\n",
    "Source:\n",
    "    - https://www.kaggle.com/sandovaledwin/q-learning-algorithm-for-solving-frozenlake-game/code\n",
    "    - http://deeplizard.com/learn/video/HGeI30uATws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list to hold all of the rewards we’ll get from each episode, to see how our game score changes over time.\n",
    "rewards_all_episodes = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Q-learning algorithm\n",
    "\n",
    "This first for-loop contains everything that happens within a single episode. This second nested loop contains everything that happens for a single time-step.\n",
    "\n",
    "\n",
    "##### Update Q-value formula:\n",
    "\n",
    "$$q(s,a)\\;=\\;(1 - \\alpha) + \\alpha(R_{t+1} + \\gamma*max_{a'}\\;q(s',a'))$$\n",
    "\n",
    "Used to update the Q-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Q-learing algorithm \n",
    "\n",
    "# everything that happends within a single episode\n",
    "for episode in range(num_episodes):\n",
    "    # reset the state of the environment back to the starting state\n",
    "    state = env.reset() \n",
    "    \n",
    "    # initialize new episode params\n",
    "    done = False # keeps track of whether or not the episode is finished so initialize to false for the starting\n",
    "    \n",
    "    # keep track of the rewards within the current episode \n",
    "    rewards_current_episode = 0 # set to zero since we start out with no rewards at the beginning of each episode\n",
    "    \n",
    "    # everything that happends within a single time-step within each episode\n",
    "    for step in range(max_steps_per_episode):\n",
    "        \n",
    "        # Exploration-exploitation trade-off\n",
    "        exploration_rate_threshold = random.uniform(0, 1) # epsilon: probability of exploring or exploiting the environment on this timne step\n",
    "        \n",
    "        #if exploration_rate_threshold > exploration_rate and ~np.all(q_table[state,:]==0):\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            # Exploitation: Take new action with Greedy Policy, only if the q values for the state are NOT all 0\n",
    "            action = np.argmax(q_table[state,:])\n",
    "        else:\n",
    "            # Explore: Take new action\n",
    "            action = env.action_space.sample()\n",
    "            #print('Exploration')\n",
    "        \n",
    "        # Returns a tuple containing the new state. And 'info' diagnostic information regarding our environment\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Update Q-table for Q(s,a) is a weighted sum of our old value and the “learned value.”\n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) \\\n",
    "        + learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "        \n",
    "        # Transition to the next state\n",
    "        # Set new state\n",
    "        state = new_state\n",
    "        \n",
    "        # Add new reward \n",
    "        rewards_current_episode += reward\n",
    "        \n",
    "        # Check to see if our last action ended the episode (did our agent step in a hole or reach the goal?)\n",
    "        if done == True:\n",
    "            break\n",
    "            \n",
    "    # Exploration rate decay\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "    (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate * episode)\n",
    "     \n",
    "    # Add current episode reward to total rewards list\n",
    "    rewards_all_episodes.append(rewards_current_episode)                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After all episodes complete\n",
    "\n",
    "we now just calculate the average reward per thousand episodes from our list that contains the rewards for all episodes so that we can print it out and see how the rewards changed over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes), num_episodes/1000)\n",
    "count = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** Average reward per thousand episodes ********\n",
      "\n",
      "1000  0.04200000000000003\n",
      "2000  0.22700000000000017\n",
      "3000  0.3880000000000003\n",
      "4000  0.5270000000000004\n",
      "5000  0.6160000000000004\n",
      "6000  0.6430000000000005\n",
      "7000  0.6970000000000005\n",
      "8000  0.6910000000000005\n",
      "9000  0.6970000000000005\n",
      "10000  0.7000000000000005\n"
     ]
    }
   ],
   "source": [
    "print(\"******** Average reward per thousand episodes ********\\n\")\n",
    "for reward in rewards_per_thousand_episodes:\n",
    "    print(count, \"\", str(sum(reward/1000)))\n",
    "    count += 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "******** Q-table ********\n",
      "\n",
      "[[0.55305969 0.50936222 0.51268992 0.51292919]\n",
      " [0.36488135 0.33360739 0.26946678 0.50493763]\n",
      " [0.44790108 0.44540804 0.44219952 0.47713909]\n",
      " [0.22650871 0.21555082 0.29339466 0.4616523 ]\n",
      " [0.57257193 0.34338435 0.42032957 0.42608411]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.41637697 0.19327921 0.18684407 0.13209898]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.36310989 0.45718635 0.29683875 0.60511954]\n",
      " [0.40807201 0.7130411  0.43457842 0.40485577]\n",
      " [0.68275394 0.4480717  0.42565895 0.33215237]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.40610197 0.30692862 0.80346477 0.39405881]\n",
      " [0.76452398 0.89945059 0.80661165 0.79968618]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Print updated Q-table to see how that has transitioned from its initial state of all zeros.\n",
    "print(\"\\n\\n******** Q-table ********\\n\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent played 10,000 episodes, within each time step within an episode the agent received a reward of one if it reached the frisbee and otherwise it received a reward of zero. If the agen did indeed reach the frisbee then the episode finished at that time step. This means that for each episode the total reward received by the agent for the entire episode is either one or zero. For the first thousand episodes we can interpret the first score in the printout as meaning that 5% of the time the agent recieved a reward of one and won the episode. And by the last thousand episodes from a total of 10,000 the agent was winning 70% of the time and from the grid enviroment we can see that it is more likely for the agent to fall in a hole or perhaps reach the  the max number of time steps than it is to reach the frisbee. But reach the frisbee 70% of the time by the end of the training is not too bad especially since the agent had not explicit instructions to reach the frisbee in any case. It learned through reinforcement that this is the correct direction/action to do.\n",
    "\n",
    "We observe from the print out that over time during training we can see the average rewards per thousand episodes did indeed progress over time. When the algorithm first started training for the first thousand episodes only averaged a rewards of 0.005 but the time it got to its last thousand episodes the reward drastically improved to 0.694. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of steps 99\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Use our Q-table to play FrozenLake !\n",
    "\n",
    "# List of rewards\n",
    "rewards = []\n",
    "\n",
    "# Watch our agent play Frozen Lake by playing the best action \n",
    "# from each state according to the Q-table\n",
    "for episode in range(3): # watch the agent play three episodes\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "    \n",
    "    # initialize new episode params\n",
    "    step = 0\n",
    "    done = False\n",
    "    print('****** EPISODE ', episode+1, '*******\\n\\n\\n')\n",
    "    time.sleep(1) # allow print out to be read before disappearing\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        # clear output from jupyter notebook cell\n",
    "        clear_output(wait=True) \n",
    "        \n",
    "        # Show current state of environment on screen\n",
    "        env.render() # render current state of the environment to display where the agent is in the grid (visually see the game grid)\n",
    "        time.sleep(0.3) # sleep 300 milliseconds, will allow to see the current state of the environment before moving on to the next time step\n",
    "        \n",
    "        # Choose action with highest Q-value for current state\n",
    "        # Take the action (index) that have the maximum expected future reward given that state.\n",
    "        # Set the action to the action with the highest Q-value in the Q-table for the current sate.\n",
    "        action = np.argmax(q_table[state,:])\n",
    "        \n",
    "        # Take the new action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info = env.step(action) # take action\n",
    "        \n",
    "        # If done (if we're dead) : finish episode\n",
    "        if done:\n",
    "            # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1: # Agent reached the goal and won episode\n",
    "                print('***** You reached the goal! *****')\n",
    "                time.sleep(3)\n",
    "            else: # Agent stepped in a hole and lost episode\n",
    "                print('**** You fell through a hole! *****')\n",
    "                time.sleep(3)\n",
    "                \n",
    "            # clear output from jupyter cell\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            # We print the number of step it took.\n",
    "            print(\"Number of steps\", step)\n",
    "            break\n",
    "            \n",
    "        # Set our new state\n",
    "        state = new_state\n",
    "\n",
    "# After all three episodes are done, we then close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
