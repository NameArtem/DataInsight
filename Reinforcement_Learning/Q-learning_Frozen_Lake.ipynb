{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement learning with Q-learning \n",
    "\n",
    "### Setting library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can query information about the environment, sample states and actions, \n",
    "# retreive rewards, and have agent navigate the envorinment\n",
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create the Q-table and initialize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of the action space \n",
    "action_space_size = env.action_space.n\n",
    "# size of the state space in the environment\n",
    "state_space_size = env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build Q-table and fill it with zeros\n",
    "q_table = np.zeros((state_space_size, action_space_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Initializing Q-learning hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and initialize all the parameters needed to implement the Q-learning algorithm.\n",
    "num_episodes = 10000 # number of plays during training\n",
    "max_steps_per_episode = 100 # max number of steps agent allowed to take in a single episode\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "\n",
    "# related to the exploration-exploitation trade-off using epsilon-greedy policy\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01   # max and min are the bounds to how large or small our exploration rate can be\n",
    "exploration_decay_rate = 0.001\n",
    "\n",
    "# Note: exploration rate was represented by epsilon "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The lower the exploration decay rate, the longer the agent will be able to explore. With 0.01 as the decay rate, the agent was only able to explore for a relatively short amount of time until it went into full exploitation mode without having a chance to fully explore and learn about the environment. Decreasing the decay rate to 0.001 allowed the agent to explore for longer and learn more.\n",
    "\n",
    "If the exploration decay rate is too large, the 2nd term in the “exploration-rate update” is ≈ 0 (because the exponential term is ≈ 0). The impact is that subsequent epsilon-greedy searches get stuck in an “exploitation” mode, since the exploration rate converges to \"min_exploration_rate\" (little to no exploration occurs). \n",
    "\n",
    "These behavior would come much more clear if the game was deterministic (no slipping on ice), since the slippery situation adds a randomness which contributes to hide the phenomenon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: The Q learning algorithm\n",
    "\n",
    "Source: https://www.kaggle.com/sandovaledwin/q-learning-algorithm-for-solving-frozenlake-game/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_all_episodes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Q-learing algorithm \n",
    "\n",
    "# everything that happends within a single episode\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset() # reset the state of the environment back to the starting state\n",
    "    \n",
    "    done = False # keeps track of whether or not the episode is finished so initialize to false for the starting\n",
    "    rewards_current_episode = 0 # keep track of the current rewards within an episode set to zero at the begining of each episode\n",
    "    \n",
    "    # everything that happends within a single time step within each episode\n",
    "    for step in range(max_steps_per_episode):\n",
    "        \n",
    "        # Exploration-exploitation trade-off\n",
    "        exploration_rate_threshold = random.uniform(0,1) # probability of exploring or exploiting the environment on this timne step\n",
    "        #if exploration_rate_threshold > exploration_rate and ~np.all(q_table[state,:]==0):\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            # Exploit: Greedy Policy, only if the q values for the state are NOT all 0\n",
    "            action = np.argmax(q_table[state,:])\n",
    "        else:\n",
    "            # Explore\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        # info --> diagnostic information regarding the environment\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Update Q-table for Q(s,a)\n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) \\\n",
    "        + learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "        \n",
    "        state = new_state\n",
    "        rewards_current_episode += reward\n",
    "        \n",
    "        if done == True:\n",
    "            break\n",
    "            \n",
    "        # Exploraiton rate decay\n",
    "        exploration_rate = min_exploration_rate + \\\n",
    "        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate * episode)\n",
    "        \n",
    "        rewards_all_episodes.append(rewards_current_episode)                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes), num_episodes/1000)\n",
    "count = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* Average reward per thousand episodes *************\n",
      "\n",
      "1000  0.0\n",
      "2000  0.0\n",
      "3000  0.0\n",
      "4000  0.0\n",
      "5000  0.0\n",
      "6000  0.0\n",
      "7000  0.0\n",
      "8000  0.0\n",
      "9000  0.0\n",
      "10000  0.0\n",
      "\n",
      "\n",
      "********** Updated Q-table **********\n",
      "[[0.54894128 0.49723681 0.51435069 0.50271982]\n",
      " [0.40771546 0.25452937 0.31320233 0.49460324]\n",
      " [0.40632777 0.41912727 0.41237965 0.45605739]\n",
      " [0.31664646 0.26525519 0.30803236 0.44215589]\n",
      " [0.56423568 0.26809739 0.27446237 0.26934401]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.17609613 0.15161969 0.36117057 0.11583256]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.4286229  0.41476959 0.43693872 0.59784548]\n",
      " [0.33796209 0.64692316 0.38619704 0.39963351]\n",
      " [0.69768942 0.43459488 0.31914848 0.38497603]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.49020933 0.59465529 0.76791042 0.60533449]\n",
      " [0.73100878 0.9158948  0.7565433  0.72039824]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"************* Average reward per thousand episodes *************\\n\")\n",
    "for reward in rewards_per_thousand_episodes:\n",
    "    print(count, \"\", str(sum(reward/1000)))\n",
    "    count += 1000\n",
    "    \n",
    "# print update Q-table\n",
    "print('\\n\\n********** Updated Q-table **********')\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent played 10,000 episodes, within each time step within an episode the agent received a reward of one if it reached the frisbee and otherwise it received a reward of zero. If the agen did indeed reach the frisbee then the episode finished at that time step. This means that for each episode the total reward received by the agent for the entire episode is either one or zero. For the first thousand episodes we can interpret the first score in the printout as meaning that 18% of the time the agent recieved a reward of one and won the episode. And by the last thousand episodes from a total of 10,000 the agent was winning 70% of the time and from the grid enviroment we can see that it is more likely for the agent to fall in a hole or perhaps reach the  the max number of time steps than it is to reach the frisbee. But reach the frisbee 70% of the time by the end of the training is not too bad especially since the agent had not explicit instructions to reach the frisbee in any case. It learned through reinforcement that this is the correct direction/action to do.\n",
    "\n",
    "We observe from the print out that over time during training we can see the average rewards per thousand episodes did indeed progress over time. When the algorithm first started training for the first thousand episodes only averaged a rewards of 0.018 but the time it got to its last thousand episodes the reward drastically improved to 0.7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of steps 33\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Use our Q-table to play FrozenLake !\n",
    "\n",
    "# List of rewards\n",
    "rewards = []\n",
    "\n",
    "# 2 For life or until learning is stopped (num_episodes --> 3)\n",
    "for episode in range(3):\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    print('****** EPISODE ', episode+1, '*******\\n\\n\\n')\n",
    "    time.sleep(1) # allow print out to be read before disappearing\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        clear_output(wait=True) # clear output from jupyter notebook cell\n",
    "        env.render() # render current state of the environment to display where the agent is in the grid\n",
    "        time.sleep(0.3) # allows one to see the current state of the environment before moving on to the next time step\n",
    "        \n",
    "         # Take the action (index) that have the maximum expected future reward given that state.\n",
    "        # Set the action to the action with the highest Q-value in the Q-table for the current sate.\n",
    "        action = np.argmax(q_table[state,:])\n",
    "        \n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info = env.step(action) # take action\n",
    "        \n",
    "        # If done (if we're dead) : finish episode\n",
    "        if done:\n",
    "            # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print('***** You reached the goal! *****')\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print('**** You fell through a hole! *****')\n",
    "                time.sleep(3)\n",
    "                \n",
    "            # clear output from jupyter cell\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            # We print the number of step it took.\n",
    "            print(\"Number of steps\", step)\n",
    "            break\n",
    "            \n",
    "        # Our new state is state\n",
    "        state = new_state\n",
    "# close environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
