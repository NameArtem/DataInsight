{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regret_matching_RPS_Nash_Equilibrium.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "gqHkCOlDezGM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Nash Equilibrium strateg Rock-Paper-Scissors with Regret Matching\n",
        "\n",
        "Regret matching (RM) is an algorithm that seeks to minimise regret about its decisions at each step/move of a game. By learning from past behaviours to inform future decisions by favouring the action it regretted not having taken previously. This model moves toward a Nash Equilibrium in a Zero-Sum Game."
      ]
    },
    {
      "metadata": {
        "id": "15N3tiVfetkf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import division\n",
        "from random import random\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9_IWoogYezui",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RPS:\n",
        "  \n",
        "    n_actions = 3\n",
        "    actions = ['ROCK', 'PAPER', 'SCISSORS']\n",
        "    \n",
        "    \n",
        "    utilities = pd.DataFrame([\n",
        "        # ROCK  PAPER  SCISSORS\n",
        "        [ 0,    -1,    1], # ROCK\n",
        "        [ 1,     0,   -1], # PAPER\n",
        "        [-1,     1,    0]  # SCISSORS\n",
        "    ], columns=actions, index=actions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DeFnl4bkgLhI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Player:\n",
        "    def __init__(self, name):\n",
        "        self.strategy, self.avg_strategy,\\\n",
        "        self.strategy_sum, self.regret_sum = np.zeros((4, RPS.n_actions))\n",
        "        self.name = name\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.name\n",
        "\n",
        "    def update_strategy(self):\n",
        "        \"\"\"\n",
        "        set the preference (strategy) of choosing an action to be proportional to positive regrets\n",
        "        e.g, a strategy that prefers PAPER can be [0.2, 0.6, 0.2]\n",
        "        \"\"\"\n",
        "        self.strategy = np.copy(self.regret_sum)\n",
        "        self.strategy[self.strategy < 0] = 0  # reset negative regrets to zero\n",
        "\n",
        "        summation = sum(self.strategy)\n",
        "        if summation > 0:\n",
        "            # normalise\n",
        "            self.strategy /= summation\n",
        "        else:\n",
        "            # uniform distribution to reduce exploitability\n",
        "            self.strategy = np.repeat(1 / RPS.n_actions, RPS.n_actions)\n",
        "\n",
        "        self.strategy_sum += self.strategy\n",
        "\n",
        "    def regret(self, my_action, opp_action):\n",
        "        \"\"\"\n",
        "        here we define the regret of not having chosen an action as the difference between the utility of that action\n",
        "        and the utility of the action we actually chose, with respect to the fixed choices of the other player.\n",
        "        compute the regret and add it to regret sum.\n",
        "        \"\"\"\n",
        "        result = RPS.utilities.loc[my_action, opp_action]\n",
        "        facts = RPS.utilities.loc[:, opp_action].values\n",
        "        regret = facts - result\n",
        "        self.regret_sum += regret\n",
        "\n",
        "    def action(self, use_avg=False):\n",
        "        \"\"\"\n",
        "        select an action according to strategy probabilities\n",
        "        \"\"\"\n",
        "        strategy = self.avg_strategy if use_avg else self.strategy\n",
        "        return np.random.choice(RPS.actions, p=strategy) # p refers to 'probability'\n",
        "\n",
        "    def learn_avg_strategy(self):\n",
        "        # averaged strategy converges to Nash Equilibrium\n",
        "        summation = sum(self.strategy_sum)\n",
        "        if summation > 0:\n",
        "            self.avg_strategy = self.strategy_sum / summation\n",
        "        else:\n",
        "            self.avg_strategy = np.repeat(1/RPS.n_actions, RPS.n_actions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OpHvaI29gL8d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Game:\n",
        "    def __init__(self, max_game=10000):\n",
        "        self.p1 = Player('Alasdair')\n",
        "        self.p2 = Player('Calum')\n",
        "        self.max_game = max_game\n",
        "\n",
        "    def winner(self, a1, a2):\n",
        "        result = RPS.utilities.loc[a1, a2]\n",
        "        if result == 1:     return self.p1\n",
        "        elif result == -1:  return self.p2\n",
        "        else:               return 'Draw'\n",
        "\n",
        "    def play(self, avg_regret_matching=False):\n",
        "        def play_regret_matching():\n",
        "            for i in range(0, self.max_game):\n",
        "                self.p1.update_strategy()\n",
        "                self.p2.update_strategy()\n",
        "                a1 = self.p1.action()\n",
        "                a2 = self.p2.action()\n",
        "                self.p1.regret(a1, a2)\n",
        "                self.p2.regret(a2, a1)\n",
        "\n",
        "                winner = self.winner(a1, a2)\n",
        "                num_wins[winner] += 1\n",
        "\n",
        "        def play_avg_regret_matching():\n",
        "            for i in range(0, self.max_game):\n",
        "                a1 = self.p1.action(use_avg=True)\n",
        "                a2 = self.p2.action(use_avg=True)\n",
        "                winner = self.winner(a1, a2)\n",
        "                num_wins[winner] += 1\n",
        "\n",
        "        num_wins = {\n",
        "            self.p1: 0,\n",
        "            self.p2: 0,\n",
        "            'Draw': 0\n",
        "        }\n",
        "\n",
        "        play_regret_matching() if not avg_regret_matching else play_avg_regret_matching()\n",
        "        print(num_wins)\n",
        "\n",
        "    def conclude(self):\n",
        "        \"\"\"\n",
        "        let two players conclude the average strategy from the previous strategy stats \n",
        "        \"\"\"\n",
        "        self.p1.learn_avg_strategy()\n",
        "        self.p2.learn_avg_strategy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c7ZLiveqhyRN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2862393f-ff77-44aa-e2cd-426da892408f"
      },
      "cell_type": "code",
      "source": [
        "game = Game()\n",
        "print('==== Use simple regret-matching strategy === ')\n",
        "game.play()\n",
        "game.conclude()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==== Use simple regret-matching strategy === \n",
            "{Alasdair: 4333, Calum: 4340, 'Draw': 1327}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Lh-G6UHXiAxe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8f3cd7c6-30c2-4d6a-bbff-116ff6dd65f9"
      },
      "cell_type": "code",
      "source": [
        "print('==== Use averaged regret-matching strategy === ')\n",
        "game.play(avg_regret_matching=True)\n",
        "game.conclude()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==== Use averaged regret-matching strategy === \n",
            "{Alasdair: 3353, Calum: 3300, 'Draw': 3347}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P9IJhKIKjiwG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}