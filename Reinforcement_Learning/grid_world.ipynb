{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "grid_world.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqB53WlG9kOG",
        "colab_type": "text"
      },
      "source": [
        "# Solving Grid World Problem\n",
        "\n",
        "**Set of algorithms used to solve control problem:**\n",
        "- Iterative Policy Evaluation\n",
        "- Policy Improvement \n",
        "- Value Iteration\n",
        "- Monte-Carlo Policy Evaluation\n",
        "- Monte-Carlo Control with Exploring Start \n",
        "- Monte-Carlo Control without Exploring Start \n",
        "- TD(0)\n",
        "- SARSA\n",
        "- Q-Learning\n",
        "- Approximation Monte-Carlo prediction\n",
        "- Approximation TD(0) prediciton\n",
        "- Approximation Semi-Gradient SARSA control\n",
        "- Approximation Q-Learning\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIrHRQFxgVtF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyYGaJoGgiB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Grid: # Environment\n",
        "  def __init__(self, width, height, start):\n",
        "    self.width = width\n",
        "    self.height = height\n",
        "    self.i = start[0]\n",
        "    self.j = start[1]\n",
        "\n",
        "  def set(self, rewards, actions):\n",
        "    # rewards should be a dict of: (i, j): r (row, col): reward\n",
        "    # actions should be a dict of: (i, j): A (row, col): list of possible actions\n",
        "    self.rewards = rewards\n",
        "    self.actions = actions\n",
        "\n",
        "  def set_state(self, s):\n",
        "    self.i = s[0]\n",
        "    self.j = s[1]\n",
        "  \n",
        "  def current_state(self):\n",
        "    return (self.i, self.j)\n",
        "  \n",
        "  def is_terminal(self, s):\n",
        "    return s not in self.actions\n",
        "\n",
        "  def move(self, action):\n",
        "    # check if legal move first\n",
        "    if action in self.actions[(self.i, self.j)]:\n",
        "      if action == \"U\":\n",
        "        self.i -= 1\n",
        "      elif action == \"D\":\n",
        "        self.i += 1\n",
        "      elif action == \"R\":\n",
        "        self.j += 1\n",
        "      elif action == \"L\":\n",
        "        self.j -= 1\n",
        "      \n",
        "    # return a reward (if any)\n",
        "    return self.rewards.get((self.i, self.j), 0)\n",
        "\n",
        "  def undo_move(self, action):\n",
        "    # these are the opposite of what U/D/L/R should normally do\n",
        "    if action == \"U\":\n",
        "      self.i += 1\n",
        "    elif action == \"D\":\n",
        "      self.i -= 1\n",
        "    elif action == \"R\":\n",
        "      self.j -= 1\n",
        "    elif action == \"L\":\n",
        "      self.j += 1\n",
        "    # raise an exception if we arrive somewhere we shouldn't be\n",
        "    # should never happen\n",
        "    assert(self.current_state() in self.all_states())\n",
        "\n",
        "  def game_over(self):\n",
        "    # returns true if game is over, else false\n",
        "    # true if we are in a state where no actions are possible\n",
        "    return (self.i, self.j not in self.actions)\n",
        "  \n",
        "  def all_states(self):\n",
        "    # possibly buggy but simple way to get all states\n",
        "    # either a position that has possible next actions\n",
        "    # or a position that yields a reward\n",
        "    return set(list(self.actions.keys()) + list(self.rewards.keys()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_4SxokEjJBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def standard_grid():\n",
        "  # define a grid that describes the reward for arriving at each state\n",
        "  # and pssible actions at each state\n",
        "  # the grid looks like this\n",
        "  # x means you can't go there\n",
        "  # number means reward at that state\n",
        "  # . . .  1\n",
        "  # . x . -1\n",
        "  # s . . .\n",
        "  g = Grid(3, 4, (2, 0))\n",
        "  rewards = {(0, 3): 1, (1, 3): -1}\n",
        "  actions = {\n",
        "      (0,0): ('D', 'R'),\n",
        "      (0,1): ('L', 'R'),\n",
        "      (0,2): ('L', 'D', 'R'),\n",
        "      (1,0): ('U', 'D'),\n",
        "      (1,2): ('U', 'D', 'R'),\n",
        "      (2,0): ('U', 'R'),\n",
        "      (2,1): ('L', 'R'),\n",
        "      (2,2): ('L', 'R', 'U'),\n",
        "      (2,3): ('L', 'U'),\n",
        "  }\n",
        "  g.set(rewards, actions)\n",
        "  return g"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jztYZfteRMgQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def negative_grid(step_cost=-0.1):\n",
        "  # in this game we want to try to minimize the number of moves\n",
        "  # so we will penalize every move\n",
        "  g = standard_grid()\n",
        "  g.rewards.update({\n",
        "    (0, 0): step_cost,\n",
        "    (0, 1): step_cost,\n",
        "    (0, 2): step_cost,\n",
        "    (1, 0): step_cost,\n",
        "    (1, 2): step_cost,\n",
        "    (2, 0): step_cost,\n",
        "    (2, 1): step_cost,\n",
        "    (2, 2): step_cost,\n",
        "    (2, 3): step_cost,\n",
        "  })\n",
        "  return g"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMgq54PajJEO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_game(agent, env):\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNEWRUdZjJGb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set parameter\n",
        "SMALL_ENOUGH = 10e-4 # threshold for convergence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPVQ5NwYHt65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_values(V, g):\n",
        "  for i in range(g.width):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.height):\n",
        "      v = V.get((i,j), 0)\n",
        "      if v >= 0:\n",
        "        print(\" %.2f|\" % v, end=\"\")\n",
        "      else:\n",
        "        print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "def print_policy(P, g):\n",
        "  for i in range(g.width):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.height):\n",
        "      a = P.get((i,j), ' ')\n",
        "      print(\"  %s  |\" % a, end=\"\")\n",
        "    print(\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6spyZkJF6um0",
        "colab_type": "text"
      },
      "source": [
        "## Iterative Policy Evaluation\n",
        "\n",
        "Given a policy, let's find it's value function V(s) we will do this for both a uniform random policy and fixed policy.\n",
        "\n",
        "Note: There are 2 sources of randomness p(a|s) - deciding what action to take given the state p(s', r|s,a) - the next state and reward given your action-state pair we are only modeling p(a|s) = uniform \n",
        "\n",
        "how would the code change if p(s', r|s,a) is not deterministic?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heeLvRyb5VdK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid = standard_grid()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmuY1FX_7VpR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# states will be positions (i, j)\n",
        "# simpler than tic-tac-toe because we only have one \"game piece\"\n",
        "# that can only be at one position at a time\n",
        "states = grid.all_states()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f57f62bd-0a8a-4e8b-84d0-1eb450d8c242",
        "id": "vpJScOYvIfv8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "  # uniformly random actions ###\n",
        "  # initialize V(s) = 0\n",
        "  V = {}\n",
        "  for s in states:\n",
        "    V[s] = 0\n",
        "  gamma = 1.0 # discount factor\n",
        "  # repeat until convergence\n",
        "  while True:\n",
        "    biggest_change = 0\n",
        "    for s in states:\n",
        "      old_v = V[s]\n",
        "\n",
        "      # V(s) only has value if it's not a terminal state\n",
        "      if s in grid.actions:\n",
        "\n",
        "        new_v = 0 # we will accumulate the answer\n",
        "        p_a = 1.0 / len(grid.actions[s]) # each action has equal probability\n",
        "        for a in grid.actions[s]:\n",
        "          grid.set_state(s)\n",
        "          r = grid.move(a)\n",
        "          new_v += p_a * (r + gamma * V[grid.current_state()])\n",
        "        V[s] = new_v\n",
        "        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
        "\n",
        "    if biggest_change < SMALL_ENOUGH:\n",
        "      break\n",
        "  print(\"values for uniformly random actions:\")\n",
        "  print_values(V, grid)\n",
        "  print(\"\\n\\n\")"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "values for uniformly random actions:\n",
            "---------------------------\n",
            "-0.03| 0.09| 0.22| 0.00|\n",
            "---------------------------\n",
            "-0.16| 0.00|-0.44| 0.00|\n",
            "---------------------------\n",
            "-0.29|-0.41|-0.54|-0.77|\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xqdnYSxAfkR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "3a261ccd-9b67-40e3-8f5b-507126d7eb56"
      },
      "source": [
        "# fixed policy\n",
        "  policy = {\n",
        "    (2, 0): 'U',\n",
        "    (1, 0): 'U',\n",
        "    (0, 0): 'R',\n",
        "    (0, 1): 'R',\n",
        "    (0, 2): 'R',\n",
        "    (1, 2): 'R',\n",
        "    (2, 1): 'R',\n",
        "    (2, 2): 'R',\n",
        "    (2, 3): 'U',\n",
        "  }\n",
        "  print_policy(policy, grid)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  R  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  R  |  U  |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWqIEHQLCzUd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize V(s) = 0\n",
        "V = {}\n",
        "for s in states:\n",
        "  V[s] = 0\n",
        "\n",
        "# let's see how V(s) changes as we get further away from the reward\n",
        "gamma = 0.9 # discount factor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD9wWoAwDKQB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "3862fbd5-cf8e-4008-b5f8-aeabfbe03c6c"
      },
      "source": [
        " # repeat until convergence\n",
        "  while True:\n",
        "    biggest_change = 0\n",
        "    for s in states:\n",
        "      old_v = V[s]\n",
        "\n",
        "      # V(s) only has value if it's not a terminal state\n",
        "      if s in policy:\n",
        "        a = policy[s]\n",
        "        grid.set_state(s)\n",
        "        r = grid.move(a)\n",
        "        V[s] = r + gamma * V[grid.current_state()]\n",
        "        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
        "\n",
        "    if biggest_change < SMALL_ENOUGH:\n",
        "      break\n",
        "  print(\"values for fixed policy:\")\n",
        "  print_values(V, grid)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "values for fixed policy:\n",
            "---------------------------\n",
            " 0.81| 0.90| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.73| 0.00|-1.00| 0.00|\n",
            "---------------------------\n",
            " 0.66|-0.81|-0.90|-1.00|\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BG10sttlKz-l",
        "colab_type": "text"
      },
      "source": [
        "## Policy Improvement - Control Problem\n",
        "\n",
        "How to find better policies --> optimal policy.\n",
        "\n",
        "This approach is greedy, never considers globally the value-function at all states, it only looks at the current state s. \n",
        "\n",
        "When we've found the optimal policy, the policy $\\pi$ won't change w.r.t the value-function. Additionally, the value-funciton will no longer improve it'll stay constant.\n",
        "\n",
        "**Policy Iteration**\n",
        "\n",
        "Policy iteration is an algorithm used to find the optimal policy. In the case of iterative-policy-evaluation when we change the policy, the value-funciton becomes out of date. Alternating between policy evalution and policy improvement until policy doesn't change. We don't need to check value-funciton for convergence, because once policy becomes constant so will the value-function.\n",
        "\n",
        "**Policy Iteration** calls on the **iterative-policy-evaluation** of the policy $\\pi$ to find the current V(s) and then does a **policy improvement**. \n",
        "\n",
        "A disadvantage of policy iteration is that it is an iterative algorithm inside another iterative algorithm.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnWTKQ8aIrHo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define constants\n",
        "SMALL_ENOUGH = 1e-3\n",
        "GAMMA = 0.9\n",
        "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQYIVlJQQyil",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this is deterministic\n",
        "# all p(s',r|s,a) = 1 or 0\n",
        "\n",
        "# this grid gives you a reward of -0.1 for every non-terminal state\n",
        "# we want to see if this will encourage finding a shorter path to the goal\n",
        "grid = negative_grid()\n",
        "#grid = standard_grid()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epFjUaCGQ0k9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "36fd1ec7-2263-4aa1-a023-92aa8b2d4eb7"
      },
      "source": [
        "# print rewards\n",
        "print(\"rewards:\")\n",
        "print_values(grid.rewards, grid)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rewards:\n",
            "---------------------------\n",
            "-0.10|-0.10|-0.10| 1.00|\n",
            "---------------------------\n",
            "-0.10| 0.00|-0.10|-1.00|\n",
            "---------------------------\n",
            "-0.10|-0.10|-0.10|-0.10|\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psDnx2WXRmvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# state -> action (determinstic random policy)\n",
        "# we'll randomly choose an action and update as we learn\n",
        "policy = {}\n",
        "for s in grid.actions.keys():\n",
        "  policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nwz4KvCtRqwT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "f7444977-9ec0-4d6c-fb98-1706f0d7084b"
      },
      "source": [
        "# initial policy\n",
        "print(\"initial policy:\")\n",
        "print_policy(policy, grid)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initial policy:\n",
            "---------------------------\n",
            "  L  |  U  |  D  |     |\n",
            "---------------------------\n",
            "  R  |     |  L  |     |\n",
            "---------------------------\n",
            "  D  |  R  |  D  |  R  |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_-FlxJFRtQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize V(s)\n",
        "V = {}\n",
        "states = grid.all_states()\n",
        "for s in states:\n",
        "  # V[s] = 0\n",
        "  if s in grid.actions:\n",
        "    V[s] = np.random.random()\n",
        "  else:\n",
        "    # terminal state\n",
        "    V[s] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOsRQi1VSqol",
        "colab_type": "text"
      },
      "source": [
        "Alternate between policy evaluation and policy improvement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeQqLu4iRwKt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "8dc00153-0037-456d-93ab-a7f8581583d4"
      },
      "source": [
        "# repeat until convergence - will break out when policy does not change\n",
        "  while True:\n",
        "\n",
        "    # 1. policy evaluation step - we already know how to do this!\n",
        "    while True:\n",
        "      biggest_change = 0\n",
        "      for s in states:\n",
        "        old_v = V[s]\n",
        "\n",
        "        # V(s) only has value if it's not a terminal state\n",
        "        if s in policy: # for a given policy\n",
        "          a = policy[s]\n",
        "          grid.set_state(s)\n",
        "          r = grid.move(a)\n",
        "          V[s] = r + GAMMA * V[grid.current_state()] # update the value-fuction\n",
        "          biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
        "\n",
        "      if biggest_change < SMALL_ENOUGH:\n",
        "        break\n",
        "\n",
        "    # 2. policy improvement step\n",
        "    is_policy_converged = True\n",
        "    for s in states:\n",
        "      if s in policy: # for a given policy\n",
        "        old_a = policy[s]\n",
        "        new_a = None\n",
        "        best_value = float('-inf')\n",
        "        # loop through all possible actions to find the best current action\n",
        "        for a in ALL_POSSIBLE_ACTIONS:\n",
        "          grid.set_state(s)\n",
        "          r = grid.move(a)\n",
        "          v = r + GAMMA * V[grid.current_state()] # make use of updated value-fuction performed in policy-evaluation\n",
        "          if v > best_value:\n",
        "            best_value = v\n",
        "            new_a = a\n",
        "        policy[s] = new_a # update and improve policy\n",
        "        if new_a != old_a:\n",
        "          is_policy_converged = False\n",
        "\n",
        "    if is_policy_converged:\n",
        "      break\n",
        "\n",
        "  print(\"values:\")\n",
        "  print_values(V, grid)\n",
        "  print(\"policy:\")\n",
        "  print_policy(policy, grid)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "values:\n",
            "---------------------------\n",
            " 0.62| 0.80| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.46| 0.00| 0.80| 0.00|\n",
            "---------------------------\n",
            " 0.31| 0.46| 0.62| 0.46|\n",
            "policy:\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  U  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  U  |  L  |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da2mOwXhUk6b",
        "colab_type": "text"
      },
      "source": [
        "## Value Iteration\n",
        "\n",
        "Alternative technique for solving the \"control problem\" called value-iteration. Policy evaluation step ends when V converges. There is a point before V converges, such that the result greedy policy wouldn't change. There is no need to wait for the policy evaluation to finish, since the policy improvement step will find the same policy anyways. \n",
        "\n",
        "Value-iteration combines policy evaluation and policy improvement into one step. This method is similar to policy evaluation equation except we are taking the max over all possible actions. Since policy improvement uses argmax, by taking the max, we're just doing the next policy evaluation step without calculating policy explicitly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMENeF3WUkVh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SMALL_ENOUGH = 1e-3\n",
        "GAMMA = 0.9\n",
        "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
        "\n",
        "# this is deterministic\n",
        "# all p(s',r|s,a) = 1 or 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkbwqRkygTwN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this grid gives you a reward of -0.1 for every non-terminal state\n",
        "# we want to see if this will encourage finding a shorter path to the goal\n",
        "grid = negative_grid()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWQOl1oAgan1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "57537d04-cd5e-408d-9ab4-a9719df46575"
      },
      "source": [
        "# print rewards\n",
        "print(\"rewards:\")\n",
        "print_values(grid.rewards, grid)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rewards:\n",
            "---------------------------\n",
            "-0.10|-0.10|-0.10| 1.00|\n",
            "---------------------------\n",
            "-0.10| 0.00|-0.10|-1.00|\n",
            "---------------------------\n",
            "-0.10|-0.10|-0.10|-0.10|\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJ1Rw_-BgdA1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# state -> action\n",
        "# we'll randomly choose an action and update as we learn\n",
        "policy = {} \n",
        "for s in grid.actions.keys():\n",
        "  policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS) # initialize a random policy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTmFM-2EghjR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "4d641521-8a1a-488b-c9e1-859e2ab80d6a"
      },
      "source": [
        "# initial policy\n",
        "print(\"initial policy:\")\n",
        "print_policy(policy, grid)"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initial policy:\n",
            "---------------------------\n",
            "  D  |  L  |  L  |     |\n",
            "---------------------------\n",
            "  L  |     |  D  |     |\n",
            "---------------------------\n",
            "  R  |  R  |  R  |  D  |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hskhSWRgkTd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # initialize V(s)\n",
        "  V = {}\n",
        "  states = grid.all_states()\n",
        "  for s in states:\n",
        "    # V[s] = 0\n",
        "    if s in grid.actions:\n",
        "      V[s] = np.random.random()  # randomly initialized value-function\n",
        "    else:\n",
        "      # terminal state\n",
        "      V[s] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAd5Eo5jhZNl",
        "colab_type": "text"
      },
      "source": [
        "Value iteration loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuE_-gAsgong",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # repeat until convergence\n",
        "  # V[s] = max[a]{ sum[s',r] { p(s',r|s,a)[r + gamma*V[s']] } }\n",
        "  while True:\n",
        "    biggest_change = 0\n",
        "    for s in states:\n",
        "      old_v = V[s]\n",
        "\n",
        "      # V(s) only has value if it's not a terminal state\n",
        "      if s in policy:\n",
        "        new_v = float('-inf')\n",
        "        for a in ALL_POSSIBLE_ACTIONS:\n",
        "          grid.set_state(s)\n",
        "          r = grid.move(a)\n",
        "          v = r + GAMMA * V[grid.current_state()]\n",
        "          if v > new_v:\n",
        "            new_v = v\n",
        "        V[s] = new_v\n",
        "        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
        "\n",
        "    if biggest_change < SMALL_ENOUGH:\n",
        "      break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwhLohmsgtWK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "2dc404e7-60db-41fb-cca5-81d8af573573"
      },
      "source": [
        "  # find a policy that leads to optimal value function\n",
        "  for s in policy.keys():\n",
        "    best_a = None\n",
        "    best_value = float('-inf')\n",
        "    # loop through all possible actions to find the best current action\n",
        "    for a in ALL_POSSIBLE_ACTIONS:\n",
        "      grid.set_state(s)\n",
        "      r = grid.move(a)\n",
        "      v = r + GAMMA * V[grid.current_state()]\n",
        "      if v > best_value:\n",
        "        best_value = v\n",
        "        best_a = a\n",
        "    policy[s] = best_a\n",
        "\n",
        "  # our goal here is to verify that we get the same answer as with policy iteration\n",
        "  print(\"values:\")\n",
        "  print_values(V, grid)\n",
        "  print(\"policy:\")\n",
        "  print_policy(policy, grid)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "values:\n",
            "---------------------------\n",
            " 0.62| 0.80| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.46| 0.00| 0.80| 0.00|\n",
            "---------------------------\n",
            " 0.31| 0.46| 0.62| 0.46|\n",
            "policy:\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  U  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  U  |  L  |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P545DlGbdTzG",
        "colab_type": "text"
      },
      "source": [
        "**Windy Grid World**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYPwxc9ihMj6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# next state and reward will now have some randomness\n",
        "# you'll go in your desired direction with probability 0.5\n",
        "# you'll go in a random direction a' != a with probability 0.5/3\n",
        "\n",
        "  # this grid gives you a reward of -0.1 for every non-terminal state\n",
        "  # we want to see if this will encourage finding a shorter path to the goal\n",
        "  grid = negative_grid(step_cost=-1.0)\n",
        "  # grid = negative_grid(step_cost=-0.1)\n",
        "  # grid = standard_grid()\n",
        "\n",
        "  # print rewards\n",
        "  print(\"rewards:\")\n",
        "  print_values(grid.rewards, grid)\n",
        "\n",
        "  # state -> action\n",
        "  # we'll randomly choose an action and update as we learn\n",
        "  policy = {}\n",
        "  for s in grid.actions.keys():\n",
        "    policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
        "\n",
        "  # initial policy\n",
        "  print(\"initial policy:\")\n",
        "  print_policy(policy, grid)\n",
        "\n",
        "  # initialize V(s)\n",
        "  V = {}\n",
        "  states = grid.all_states()\n",
        "  for s in states:\n",
        "    # V[s] = 0\n",
        "    if s in grid.actions:\n",
        "      V[s] = np.random.random()\n",
        "    else:\n",
        "      # terminal state\n",
        "      V[s] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55ISbI2Ydhzi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # repeat until convergence - will break out when policy does not change\n",
        "  while True:\n",
        "\n",
        "    # policy evaluation step - we already know how to do this!\n",
        "    while True:\n",
        "      biggest_change = 0\n",
        "      for s in states:\n",
        "        old_v = V[s]\n",
        "\n",
        "        # V(s) only has value if it's not a terminal state\n",
        "        new_v = 0\n",
        "        if s in policy:\n",
        "          for a in ALL_POSSIBLE_ACTIONS:\n",
        "            if a == policy[s]:\n",
        "              p = 0.5\n",
        "            else:\n",
        "              p = 0.5/3\n",
        "            grid.set_state(s)\n",
        "            r = grid.move(a)\n",
        "            new_v += p*(r + GAMMA * V[grid.current_state()])\n",
        "          V[s] = new_v\n",
        "          biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
        "\n",
        "      if biggest_change < SMALL_ENOUGH:\n",
        "        break\n",
        "\n",
        "    # policy improvement step\n",
        "    is_policy_converged = True\n",
        "    for s in states:\n",
        "      if s in policy:\n",
        "        old_a = policy[s]\n",
        "        new_a = None\n",
        "        best_value = float('-inf')\n",
        "        # loop through all possible actions to find the best current action\n",
        "        for a in ALL_POSSIBLE_ACTIONS: # chosen action\n",
        "          v = 0\n",
        "          for a2 in ALL_POSSIBLE_ACTIONS: # resulting action\n",
        "            if a == a2: # below we add in the state-transition probabilities\n",
        "              p = 0.5\n",
        "            else:\n",
        "              p = 0.5/3\n",
        "            grid.set_state(s)\n",
        "            r = grid.move(a2)\n",
        "            v += p*(r + GAMMA * V[grid.current_state()])\n",
        "          if v > best_value:\n",
        "            best_value = v\n",
        "            new_a = a\n",
        "        policy[s] = new_a\n",
        "        if new_a != old_a:\n",
        "          is_policy_converged = False\n",
        "\n",
        "    if is_policy_converged:\n",
        "      break\n",
        "\n",
        "  print(\"values:\")\n",
        "  print_values(V, grid)\n",
        "  print(\"policy:\")\n",
        "  print_policy(policy, grid)\n",
        "  # result: every move is as bad as losing, so lose as quickly as possible"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dmid54v4fvn1",
        "colab_type": "text"
      },
      "source": [
        "## Monte-Carlo Policy Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8u3SVsYefu-a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NOTE: this is only policy evaluation, not optimization\n",
        "\n",
        "def play_game(grid, policy):\n",
        "  # returns a list of states and corresponding returns\n",
        "\n",
        "  # reset game to start at a random position\n",
        "  # we need to do this, because given our current deterministic policy\n",
        "  # we would never end up at certain states, but we still want to measure their value\n",
        "  start_states = list(grid.actions.keys())\n",
        "  start_idx = np.random.choice(len(start_states)) # employ the exploring-starts method\n",
        "  grid.set_state(start_states[start_idx])\n",
        "\n",
        "  s = grid.current_state() # Next, we play the game\n",
        "  states_and_rewards = [(s, 0)] # list of tuples of (state, reward)\n",
        "  while not grid.game_over():\n",
        "    a = policy[s]\n",
        "    r = grid.move(a)\n",
        "    s = grid.current_state()\n",
        "    states_and_rewards.append((s, r))\n",
        "  # calculate the returns by working backwards from the terminal state\n",
        "  G = 0\n",
        "  states_and_returns = []\n",
        "  first = True\n",
        "  for s, r in reversed(states_and_rewards):\n",
        "    # the value of the terminal state is 0 by definition\n",
        "    # we should ignore the first state we encounter\n",
        "    # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
        "    if first:\n",
        "      first = False\n",
        "    else:\n",
        "      states_and_returns.append((s, G))\n",
        "    G = r + GAMMA*G\n",
        "  states_and_returns.reverse() # we want it to be in order of state visited\n",
        "  return states_and_returns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhCh-BI5iZca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SMALL_ENOUGH = 1e-3\n",
        "GAMMA = 0.9\n",
        "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4y-7eNPeh7Nv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "ce636b70-e615-4ba1-de85-31165a18fde4"
      },
      "source": [
        "  # use the standard grid again (0 for every step) so that we can compare\n",
        "  # to iterative policy evaluation\n",
        "  grid = standard_grid() # Step 1. get the grid\n",
        "\n",
        "  # print rewards\n",
        "  print(\"rewards:\")\n",
        "  print_values(grid.rewards, grid) # Step 2. print out the rewards\n",
        "\n",
        "  # state -> action\n",
        "  policy = { # Step 3. create the policy\n",
        "    (2, 0): 'U',\n",
        "    (1, 0): 'U',\n",
        "    (0, 0): 'R',\n",
        "    (0, 1): 'R',\n",
        "    (0, 2): 'R',\n",
        "    (1, 2): 'R',\n",
        "    (2, 1): 'R',\n",
        "    (2, 2): 'R',\n",
        "    (2, 3): 'U',\n",
        "  }\n",
        "\n",
        "  # initialize V(s) and returns\n",
        "  V = {} \n",
        "  returns = {} # dictionary of state -> list of returns we've received\n",
        "  states = grid.all_states()\n",
        "  for s in states:\n",
        "    if s in grid.actions:\n",
        "      returns[s] = [] # Step 4. initialize the returns\n",
        "    else:\n",
        "      # terminal state or state we can't otherwise get to\n",
        "      V[s] = 0"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rewards:\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 1.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00|-1.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHoUnsnUX2cY",
        "colab_type": "text"
      },
      "source": [
        "We get the same answer as iteratively policy evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uq5cwhLDh9c3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "00be36c5-1fdc-44cb-8a41-40faa88a1946"
      },
      "source": [
        "  # repeat\n",
        "  for t in range(100): # Step 5. Monte-Carlo loop this plays a game and gets the states and the returns list\n",
        "\n",
        "    # generate an episode using pi\n",
        "    states_and_returns = play_game(grid, policy)\n",
        "    seen_states = set()\n",
        "    for s, G in states_and_returns: # check if to see if this state has been previously visited in the currrent episode (for first-visit MC methods)\n",
        "      # check if we have already seen s\n",
        "      # called \"first-visit\" MC policy evaluation\n",
        "      if s not in seen_states:\n",
        "        returns[s].append(G)\n",
        "        V[s] = np.mean(returns[s])\n",
        "        seen_states.add(s)\n",
        "\n",
        "  print(\"values:\")\n",
        "  print_values(V, grid)\n",
        "  print(\"policy:\")\n",
        "  print_policy(policy, grid)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "values:\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "policy:\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  R  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  R  |  U  |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdCjG-hcYxTR",
        "colab_type": "text"
      },
      "source": [
        "**Windy Grid World with Monte-Carlo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSlGgximYw2F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_action(a):\n",
        "  # choose given a with probability 0.5\n",
        "  # choose some other a' != a with probability 0.5/3\n",
        "  p = np.random.random()\n",
        "  if p < 0.5:\n",
        "    return a\n",
        "  else:\n",
        "    tmp = list(ALL_POSSIBLE_ACTIONS)\n",
        "    tmp.remove(a)\n",
        "    return np.random.choice(tmp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9C1V_28OY-PD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_game(grid, policy):\n",
        "  # returns a list of states and corresponding returns\n",
        "\n",
        "  # reset game to start at a random position\n",
        "  # we need to do this, because given our current deterministic policy\n",
        "  # we would never end up at certain states, but we still want to measure their value\n",
        "  start_states = list(grid.actions.keys())\n",
        "  start_idx = np.random.choice(len(start_states))\n",
        "  grid.set_state(start_states[start_idx])\n",
        "\n",
        "  s = grid.current_state()\n",
        "  states_and_rewards = [(s, 0)] # list of tuples of (state, reward)\n",
        "  while not grid.game_over():\n",
        "    a = policy[s]\n",
        "    a = random_action(a)\n",
        "    r = grid.move(a)\n",
        "    s = grid.current_state()\n",
        "    states_and_rewards.append((s, r))\n",
        "  # calculate the returns by working backwards from the terminal state\n",
        "  G = 0\n",
        "  states_and_returns = []\n",
        "  first = True\n",
        "  for s, r in reversed(states_and_rewards):\n",
        "    # the value of the terminal state is 0 by definition\n",
        "    # we should ignore the first state we encounter\n",
        "    # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
        "    if first:\n",
        "      first = False\n",
        "    else:\n",
        "      states_and_returns.append((s, G))\n",
        "    G = r + GAMMA*G\n",
        "  states_and_returns.reverse() # we want it to be in order of state visited\n",
        "  return states_and_returns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rm2tQ6J_Y-SR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "0f0732b1-0fec-4dd6-97dc-b2637b4fb418"
      },
      "source": [
        "  # use the standard grid again (0 for every step) so that we can compare\n",
        "  # to iterative policy evaluation\n",
        "  grid = standard_grid()\n",
        "\n",
        "  # print rewards\n",
        "  print(\"rewards:\")\n",
        "  print_values(grid.rewards, grid)\n",
        "\n",
        "  # state -> action\n",
        "  # found by policy_iteration_random (can use as a sanity check this approach) on standard_grid\n",
        "  # MC method won't get exactly this, but should be close\n",
        "  # values:\n",
        "  # ---------------------------\n",
        "  #  0.43|  0.56|  0.72|  0.00|\n",
        "  # ---------------------------\n",
        "  #  0.33|  0.00|  0.21|  0.00|\n",
        "  # ---------------------------\n",
        "  #  0.25|  0.18|  0.11| -0.17|\n",
        "  # policy:\n",
        "  # ---------------------------\n",
        "  #   R  |   R  |   R  |      |\n",
        "  # ---------------------------\n",
        "  #   U  |      |   U  |      |\n",
        "  # ---------------------------\n",
        "  #   U  |   L  |   U  |   L  |\n",
        "  policy = {\n",
        "    (2, 0): 'U',\n",
        "    (1, 0): 'U',\n",
        "    (0, 0): 'R',\n",
        "    (0, 1): 'R',\n",
        "    (0, 2): 'R',\n",
        "    (1, 2): 'U',\n",
        "    (2, 1): 'L',\n",
        "    (2, 2): 'U',\n",
        "    (2, 3): 'L',\n",
        "  }\n",
        "\n",
        "  # initialize V(s) and returns\n",
        "  V = {}\n",
        "  returns = {} # dictionary of state -> list of returns we've received\n",
        "  states = grid.all_states()\n",
        "  for s in states:\n",
        "    if s in grid.actions:\n",
        "      returns[s] = []\n",
        "    else:\n",
        "      # terminal state or state we can't otherwise get to\n",
        "      V[s] = 0"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rewards:\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 1.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00|-1.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBID8oWdZGlf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "84202db9-df84-4b0b-b1de-643afdca0f66"
      },
      "source": [
        "    # generate an episode using pi\n",
        "    states_and_returns = play_game(grid, policy)\n",
        "    seen_states = set()\n",
        "    for s, G in states_and_returns:\n",
        "      # check if we have already seen s\n",
        "      # called \"first-visit\" MC policy evaluation\n",
        "      if s not in seen_states:\n",
        "        returns[s].append(G)\n",
        "        V[s] = np.mean(returns[s])\n",
        "        seen_states.add(s)\n",
        "\n",
        "    print(\"values:\")\n",
        "    print_values(V, grid)\n",
        "    print(\"policy:\")\n",
        "    print_policy(policy, grid)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "values:\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "policy:\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  U  |     |\n",
            "---------------------------\n",
            "  U  |  L  |  U  |  L  |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7p980amf4-g",
        "colab_type": "text"
      },
      "source": [
        "##  Monte-Carlo Control without Exploring Start\n",
        "\n",
        "NOTE: We implement the Monte Carlo control problem with Exploring-Starts method for finding the optimal policy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCqxrqIhgJqP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_game(grid, policy):\n",
        "  # returns a list of states and corresponding returns\n",
        "\n",
        "  # reset game to start at a random position\n",
        "  # we need to do this if we have a deterministic policy\n",
        "  # we would never end up at certain states, but we still want to measure their value\n",
        "  # this is called the \"exploring starts\" method\n",
        "  start_states = list(grid.actions.keys())\n",
        "  start_idx = np.random.choice(len(start_states))\n",
        "  grid.set_state(start_states[start_idx])\n",
        "\n",
        "  s = grid.current_state()\n",
        "  a = np.random.choice(ALL_POSSIBLE_ACTIONS) # first action is uniformly random\n",
        "\n",
        "  # be aware of the timing\n",
        "  # each triple is s(t), a(t), r(t)\n",
        "  # but r(t) results from taking action a(t-1) from s(t-1) and landing in s(t)\n",
        "  states_actions_rewards = [(s, a, 0)]\n",
        "  seen_states = set()\n",
        "  seen_states.add(grid.current_state())\n",
        "  num_steps = 0\n",
        "  while True:\n",
        "    r = grid.move(a)\n",
        "    num_steps += 1\n",
        "    s = grid.current_state()\n",
        "\n",
        "    if s in seen_states:\n",
        "      # hack so that we don't end up in an infinitely long episode\n",
        "      # bumping into the wall repeatedly\n",
        "      # if num_steps == 1 -> bumped into a wall and haven't moved anywhere\n",
        "      #   reward = -10\n",
        "      # else:\n",
        "      #   reward = falls off by 1 / num_steps\n",
        "      reward = -10. / num_steps\n",
        "      states_actions_rewards.append((s, None, reward))\n",
        "      #states_actions_rewards.append((s, None, -100)) # to prevent agent from choosing this action to avoid bumping into the wall!\n",
        "      break\n",
        "    elif grid.game_over():\n",
        "      states_actions_rewards.append((s, None, r))\n",
        "      break\n",
        "    else:\n",
        "      a = policy[s]\n",
        "      states_actions_rewards.append((s, a, r))\n",
        "    seen_states.add(s)\n",
        "\n",
        "  # calculate the returns by working backwards from the terminal state\n",
        "  G = 0\n",
        "  states_actions_returns = []\n",
        "  first = True\n",
        "  for s, a, r in reversed(states_actions_rewards):\n",
        "    # the value of the terminal state is 0 by definition\n",
        "    # we should ignore the first state we encounter\n",
        "    # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
        "    if first:\n",
        "      first = False\n",
        "    else:\n",
        "      states_actions_returns.append((s, a, G))\n",
        "    G = r + GAMMA*G\n",
        "  states_actions_returns.reverse() # we want it to be in order of state visited\n",
        "  return states_actions_returns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvluHDUjcsyx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_dict(d):\n",
        "  # returns the argmax (key) and max (value) from a dictionary\n",
        "  # put this into a function since we are using it so often\n",
        "  max_key = None\n",
        "  max_val = float('-inf')\n",
        "  for k, v in d.items():\n",
        "    if v > max_val:\n",
        "      max_val = v\n",
        "      max_key = k\n",
        "  return max_key, max_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl6kjL1gcTL-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GAMMA = 0.9\n",
        "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lODY32_Rcqop",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "24fc67b0-86bb-4082-b626-831b6213aeb6"
      },
      "source": [
        "  # use the standard grid again (0 for every step) so that we can compare\n",
        "  # to iterative policy evaluation\n",
        "  # grid = standard_grid()\n",
        "  # try the negative grid too, to see if agent will learn to go past the \"bad spot\"\n",
        "  # in order to minimize number of steps\n",
        "  grid = negative_grid(step_cost=-0.9)\n",
        "\n",
        "  # print rewards\n",
        "  print(\"rewards:\")\n",
        "  print_values(grid.rewards, grid)\n",
        "\n",
        "  # state -> action\n",
        "  # initialize a random policy\n",
        "  policy = {}\n",
        "  for s in grid.actions.keys():\n",
        "    policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rewards:\n",
            "---------------------------\n",
            "-0.90|-0.90|-0.90| 1.00|\n",
            "---------------------------\n",
            "-0.90| 0.00|-0.90|-1.00|\n",
            "---------------------------\n",
            "-0.90|-0.90|-0.90|-0.90|\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoNLzQ8Ocy7P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # initialize Q(s,a) and returns\n",
        "  Q = {}\n",
        "  returns = {} # dictionary of state -> list of returns we've received\n",
        "  states = grid.all_states()\n",
        "  for s in states:\n",
        "    if s in grid.actions: # not a terminal state\n",
        "      Q[s] = {}\n",
        "      for a in ALL_POSSIBLE_ACTIONS:\n",
        "        Q[s][a] = 0 # needs to be initialized to something so we can argmax it\n",
        "        returns[(s,a)] = []\n",
        "    else:\n",
        "      # terminal state or state we can't otherwise get to\n",
        "      pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7KrKd28c4KK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724
        },
        "outputId": "99b082da-03ed-4cc6-f6de-0c0356a93929"
      },
      "source": [
        "  # repeat until convergence\n",
        "  deltas = []\n",
        "  for t in range(2000):\n",
        "    if t % 100 == 0:\n",
        "      print(t)\n",
        "\n",
        "    # generate an episode using pi\n",
        "    biggest_change = 0\n",
        "    states_actions_returns = play_game(grid, policy)\n",
        "    seen_state_action_pairs = set()\n",
        "    for s, a, G in states_actions_returns:\n",
        "      # check if we have already seen s\n",
        "      # called \"first-visit\" MC policy evaluation\n",
        "      sa = (s, a)\n",
        "      if sa not in seen_state_action_pairs:\n",
        "        old_q = Q[s][a]\n",
        "        returns[sa].append(G)\n",
        "        Q[s][a] = np.mean(returns[sa])\n",
        "        biggest_change = max(biggest_change, np.abs(old_q - Q[s][a]))\n",
        "        seen_state_action_pairs.add(sa)\n",
        "    deltas.append(biggest_change)\n",
        "\n",
        "    # update policy\n",
        "    for s in policy.keys():\n",
        "      policy[s] = max_dict(Q[s])[0]\n",
        "\n",
        "  plt.plot(deltas)\n",
        "  plt.show()\n",
        "\n",
        "  print(\"final policy:\")\n",
        "  print_policy(policy, grid)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQKIIcABIFAEOAIkiwAEgUQQ4ACTq/wGtOov8nFOUmgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "final policy:\n",
            "---------------------------\n",
            "  D  |  L  |  R  |     |\n",
            "---------------------------\n",
            "  D  |     |  D  |     |\n",
            "---------------------------\n",
            "  R  |  L  |  L  |  L  |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhzI6HejfgHn",
        "colab_type": "text"
      },
      "source": [
        "Notice how we have a few 100s in the deltas, this is because our initial policy contained actions that led to bumping into walls. \n",
        "\n",
        "The final policy makes sense, as do the values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCqTOot2c57n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "761699f5-b181-49ac-9534-b0ec427308e8"
      },
      "source": [
        " # find V\n",
        "  V = {}\n",
        "  for s, Qs in Q.items():\n",
        "    V[s] = max_dict(Q[s])[1]\n",
        "\n",
        "  print(\"final values:\")\n",
        "  print_values(V, grid)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final values:\n",
            "---------------------------\n",
            "-0.90|-0.90| 1.00| 0.00|\n",
            "---------------------------\n",
            "-0.90| 0.00|-0.90| 0.00|\n",
            "---------------------------\n",
            "-0.90|-0.90|-0.90|-0.90|\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTaLO-FEgKeP",
        "colab_type": "text"
      },
      "source": [
        "**Monte-Carlo Control without Exploring Start**\n",
        "\n",
        "We can do this by using the $\\epsilon$-greedy policy instead, that is there is a small chance that the agent will sometimes choose a random action instead of the action according to the greedy policy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCPBGoiRcaZn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GAMMA = 0.9\n",
        "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
        "\n",
        "# NOTE: find optimal policy and value function\n",
        "#       using on-policy first-visit MC"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAAJHzKng8qj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_action(a, eps=0.1):\n",
        "  # choose given a with probability 1 - eps + eps/4\n",
        "  # choose some other a' != a with probability eps/4\n",
        "  p = np.random.random()\n",
        "  # if p < (1 - eps + eps/len(ALL_POSSIBLE_ACTIONS)):\n",
        "  #   return a\n",
        "  # else:\n",
        "  #   tmp = list(ALL_POSSIBLE_ACTIONS)\n",
        "  #   tmp.remove(a)\n",
        "  #   return np.random.choice(tmp)\n",
        "  #\n",
        "  # this is equivalent to the above\n",
        "  if p < (1 - eps):\n",
        "    return a\n",
        "  else:\n",
        "    return np.random.choice(ALL_POSSIBLE_ACTIONS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkRECkt_g9J8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_game(grid, policy):\n",
        "  # returns a list of states and corresponding returns\n",
        "  # in this version we will NOT use \"exploring starts\" method\n",
        "  # instead we will explore using an epsilon-soft policy\n",
        "  s = (2, 0) # fixed start location (randome start removed)\n",
        "  grid.set_state(s)\n",
        "  a = random_action(policy[s])\n",
        "\n",
        "  # be aware of the timing\n",
        "  # each triple is s(t), a(t), r(t)\n",
        "  # but r(t) results from taking action a(t-1) from s(t-1) and landing in s(t)\n",
        "  states_actions_rewards = [(s, a, 0)]\n",
        "  while True:\n",
        "    r = grid.move(a)\n",
        "    s = grid.current_state()\n",
        "    if grid.game_over():\n",
        "      states_actions_rewards.append((s, None, r))\n",
        "      break\n",
        "    else: # epsilon-greedy polic where we make use of random action from above\n",
        "      a = random_action(policy[s]) # the next state is stochastic\n",
        "      states_actions_rewards.append((s, a, r))\n",
        "\n",
        "  # calculate the returns by working backwards from the terminal state\n",
        "  G = 0\n",
        "  states_actions_returns = []\n",
        "  first = True\n",
        "  for s, a, r in reversed(states_actions_rewards):\n",
        "    # the value of the terminal state is 0 by definition\n",
        "    # we should ignore the first state we encounter\n",
        "    # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
        "    if first:\n",
        "      first = False\n",
        "    else:\n",
        "      states_actions_returns.append((s, a, G))\n",
        "    G = r + GAMMA*G\n",
        "  states_actions_returns.reverse() # we want it to be in order of state visited\n",
        "  return states_actions_returns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khQEXuY8g-9W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "8ab69738-4449-4758-efa1-09ff7a6b4ad0"
      },
      "source": [
        "  # use the standard grid again (0 for every step) so that we can compare\n",
        "  # to iterative policy evaluation\n",
        "  # grid = standard_grid()\n",
        "  # try the negative grid too, to see if agent will learn to go past the \"bad spot\"\n",
        "  # in order to minimize number of steps\n",
        "  grid = negative_grid(step_cost=-0.1)\n",
        "\n",
        "  # print rewards\n",
        "  print(\"rewards:\")\n",
        "  print_values(grid.rewards, grid)\n",
        "\n",
        "  # state -> action\n",
        "  # initialize a random policy\n",
        "  policy = {}\n",
        "  for s in grid.actions.keys():\n",
        "    policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rewards:\n",
            "---------------------------\n",
            "-0.10|-0.10|-0.10| 1.00|\n",
            "---------------------------\n",
            "-0.10| 0.00|-0.10|-1.00|\n",
            "---------------------------\n",
            "-0.10|-0.10|-0.10|-0.10|\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5ALRjOWhGhw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # initialize Q(s,a) and returns\n",
        "  Q = {}\n",
        "  returns = {} # dictionary of state -> list of returns we've received\n",
        "  states = grid.all_states()\n",
        "  for s in states:\n",
        "    if s in grid.actions: # not a terminal state\n",
        "      Q[s] = {}\n",
        "      for a in ALL_POSSIBLE_ACTIONS:\n",
        "        Q[s][a] = 0\n",
        "        returns[(s,a)] = []\n",
        "    else:\n",
        "      # terminal state or state we can't otherwise get to\n",
        "      pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RGEsRbDhLE4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "87b2cc85-68d9-4c3a-d054-ede8e819838d"
      },
      "source": [
        "  # repeat until convergence\n",
        "  deltas = []\n",
        "  for t in range(5000):\n",
        "    if t % 1000 == 0:\n",
        "      print(t)\n",
        "\n",
        "    # generate an episode using pi\n",
        "    biggest_change = 0\n",
        "    states_actions_returns = play_game(grid, policy)\n",
        "\n",
        "    # calculate Q(s,a)\n",
        "    seen_state_action_pairs = set()\n",
        "    for s, a, G in states_actions_returns:\n",
        "      # check if we have already seen s\n",
        "      # called \"first-visit\" MC policy evaluation\n",
        "      sa = (s, a)\n",
        "      if sa not in seen_state_action_pairs:\n",
        "        old_q = Q[s][a]\n",
        "        returns[sa].append(G)\n",
        "        Q[s][a] = np.mean(returns[sa])\n",
        "        biggest_change = max(biggest_change, np.abs(old_q - Q[s][a]))\n",
        "        seen_state_action_pairs.add(sa)\n",
        "    deltas.append(biggest_change)\n",
        "\n",
        "    # calculate new policy pi(s) = argmax[a]{ Q(s,a) }\n",
        "    for s in policy.keys():\n",
        "      a, _ = max_dict(Q[s])\n",
        "      policy[s] = a"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNL1ap2HhRjp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "9d6b89fd-bdbd-4dc5-9a3e-fe44a9e35bda"
      },
      "source": [
        "plt.plot(deltas)\n",
        "plt.show()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UWR5DnuhTnW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "e87e416e-1b41-4990-ee26-7926c45cc5f1"
      },
      "source": [
        "  # find the optimal state-value function\n",
        "  # V(s) = max[a]{ Q(s,a) }\n",
        "  V = {}\n",
        "  for s in policy.keys():\n",
        "    V[s] = max_dict(Q[s])[1]\n",
        "\n",
        "  print(\"final values:\")\n",
        "  print_values(V, grid)\n",
        "  print(\"final policy:\")\n",
        "  print_policy(policy, grid)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final values:\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            "-0.10| 0.00| 0.00| 0.00|\n",
            "final policy:\n",
            "---------------------------\n",
            "  U  |  U  |  U  |     |\n",
            "---------------------------\n",
            "  U  |     |  U  |     |\n",
            "---------------------------\n",
            "  R  |  U  |  U  |  U  |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DDMtBBfyZpU",
        "colab_type": "text"
      },
      "source": [
        "## TD(0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qb-yNJs8hVhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SMALL_ENOUGH = 1e-3\n",
        "GAMMA = 0.9\n",
        "ALPHA = 0.1\n",
        "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_oWFQjdy2xy",
        "colab_type": "text"
      },
      "source": [
        "We have the same issue with TD as we do with MC and this is we never visit a state we'll never update its value, so if we have a deterministic policy without any explorationk some states won't have values, which is fine if we do not care about those values, but we want to add some exploraiton to make it more interesting. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qNgEn6eydro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_action(a, eps=0.1):\n",
        "  # we'll use epsilon-soft to ensure all states are visited\n",
        "  # what happens if you don't do this? i.e. eps=0\n",
        "  p = np.random.random()\n",
        "  if p < (1 - eps):\n",
        "    return a\n",
        "  else:\n",
        "    return np.random.choice(ALL_POSSIBLE_ACTIONS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W10WpP-HygMs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_game(grid, policy):\n",
        "  # returns a list of states and corresponding rewards (not returns as in MC)\n",
        "  # start at the designated start state\n",
        "  s = (2, 0)\n",
        "  grid.set_state(s)\n",
        "  states_and_rewards = [(s, 0)] # list of tuples of (state, reward)\n",
        "  while not grid.game_over():\n",
        "    a = policy[s]\n",
        "    a = random_action(a)\n",
        "    r = grid.move(a)\n",
        "    s = grid.current_state()\n",
        "    states_and_rewards.append((s, r))\n",
        "  return states_and_rewards"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgcFB76ZygPz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "4d77807c-7825-42d0-bcfc-aaaa209e919f"
      },
      "source": [
        "  # use the standard grid again (0 for every step) so that we can compare\n",
        "  # to iterative policy evaluation\n",
        "  grid = standard_grid()\n",
        "\n",
        "  # print rewards\n",
        "  print(\"rewards:\")\n",
        "  print_values(grid.rewards, grid)\n",
        "\n",
        "  # state -> action\n",
        "  policy = {\n",
        "    (2, 0): 'U',\n",
        "    (1, 0): 'U',\n",
        "    (0, 0): 'R',\n",
        "    (0, 1): 'R',\n",
        "    (0, 2): 'R',\n",
        "    (1, 2): 'R',\n",
        "    (2, 1): 'R',\n",
        "    (2, 2): 'R',\n",
        "    (2, 3): 'U',\n",
        "  }\n",
        "\n",
        "  # initialize V(s) and returns\n",
        "  V = {}\n",
        "  states = grid.all_states()\n",
        "  for s in states:\n",
        "    V[s] = 0"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rewards:\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 1.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00|-1.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZFveXIoymjA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "451a57b9-db1b-46ba-b132-122640d0b132"
      },
      "source": [
        "  # repeat until convergence\n",
        "  for it in range(1000):\n",
        "\n",
        "    # generate an episode using pi\n",
        "    states_and_rewards = play_game(grid, policy)\n",
        "    # the first (s, r) tuple is the state we start in and 0\n",
        "    # (since we don't get a reward) for simply starting the game\n",
        "    # the last (s, r) tuple is the terminal state and the final reward\n",
        "    # the value for the terminal state is by definition 0, so we don't\n",
        "    # care about updating it.\n",
        "    for t in range(len(states_and_rewards) - 1):\n",
        "      s, _ = states_and_rewards[t]\n",
        "      s2, r = states_and_rewards[t+1]\n",
        "      # we will update V(s) AS we experience the episode\n",
        "      V[s] = V[s] + ALPHA*(r + GAMMA*V[s2] - V[s])\n",
        "\n",
        "  print(\"values:\")\n",
        "  print_values(V, grid)\n",
        "  print(\"policy:\")\n",
        "  print_policy(policy, grid)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "values:\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "policy:\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  R  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  R  |  U  |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrqhXFyY3UdN",
        "colab_type": "text"
      },
      "source": [
        "## SARSA\n",
        "\n",
        " NOTE: if we use the standard grid, there's a good chance we will end up with\n",
        "   suboptimal policies\n",
        "   e.g.\n",
        "   ---------------------------\n",
        "     R  |   R  |   R  |      |\n",
        "   ---------------------------\n",
        "     R* |      |   U  |      |\n",
        "   ---------------------------\n",
        "     U  |   R  |   U  |   L  |\n",
        "   since going R at (1,0) (shown with a *) incurs no cost, it's OK to keep doing that.\n",
        "   we'll either end up staying in the same spot, or back to the start (2,0), at which\n",
        "   point we whould then just go back up, or at (0,0), at which point we can continue\n",
        "   on right.\n",
        "   instead, let's penalize each movement so the agent will find a shorter route.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0i4knwV6q684",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GAMMA = 0.9\n",
        "ALPHA = 0.1\n",
        "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJpMYCcz4J-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_dict(d):\n",
        "  # returns the argmax (key) and max (value) from a dictionary\n",
        "  # put this into a function since we are using it so often\n",
        "  max_key = None\n",
        "  max_val = float('-inf')\n",
        "  for k, v in d.items():\n",
        "    if v > max_val:\n",
        "      max_val = v\n",
        "      max_key = k\n",
        "  return max_key, max_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TI8yw63U3XZA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "2e1c5370-403c-429c-9598-32cc5acaeabc"
      },
      "source": [
        "  # grid = standard_grid()\n",
        "  grid = negative_grid(step_cost=-0.1)\n",
        "\n",
        "  # print rewards\n",
        "  print(\"rewards:\")\n",
        "  print_values(grid.rewards, grid)\n",
        "\n",
        "  # no policy initialization, we will derive our policy from most recent Q\n",
        "\n",
        "  # initialize Q(s,a)\n",
        "  Q = {}\n",
        "  states = grid.all_states()\n",
        "  for s in states:\n",
        "    Q[s] = {}\n",
        "    for a in ALL_POSSIBLE_ACTIONS:\n",
        "      Q[s][a] = 0\n",
        "\n",
        "  # let's also keep track of how many times Q[s] has been updated \n",
        "  update_counts = {} # (for debugging purposes)\n",
        "  update_counts_sa = {} # (for the adaptive learning rate)\n",
        "  for s in states:\n",
        "    update_counts_sa[s] = {}\n",
        "    for a in ALL_POSSIBLE_ACTIONS:\n",
        "      update_counts_sa[s][a] = 1.0"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rewards:\n",
            "---------------------------\n",
            "-0.10|-0.10|-0.10| 1.00|\n",
            "---------------------------\n",
            "-0.10| 0.00|-0.10|-1.00|\n",
            "---------------------------\n",
            "-0.10|-0.10|-0.10|-0.10|\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAT2P9Zv3t_-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "24b5c97a-fc6a-4325-cd4c-3662041afc9f"
      },
      "source": [
        "  # repeat until convergence\n",
        "  t = 1.0\n",
        "  deltas = []\n",
        "  for it in range(10000):\n",
        "    if it % 100 == 0:\n",
        "      t += 1e-2\n",
        "    if it % 2000 == 0:\n",
        "      print(\"it:\", it)\n",
        "\n",
        "    # instead of 'generating' an epsiode, we will PLAY\n",
        "    # an episode within this loop\n",
        "    s = (2, 0) # start state\n",
        "    grid.set_state(s)\n",
        "\n",
        "    # the first (s, r) tuple is the state we start in and 0\n",
        "    # (since we don't get a reward) for simply starting the game\n",
        "    # the last (s, r) tuple is the terminal state and the final reward\n",
        "    # the value for the terminal state is by definition 0, so we don't\n",
        "    # care about updating it.\n",
        "    a = max_dict(Q[s])[0]\n",
        "    a = random_action(a, eps=0.5/t)\n",
        "    biggest_change = 0\n",
        "    while not grid.game_over():\n",
        "      r = grid.move(a)\n",
        "      s2 = grid.current_state()\n",
        "\n",
        "      # we need the next action as well since Q(s,a) depends on Q(s',a')\n",
        "      # if s2 not in policy then it's a terminal state, all Q are 0\n",
        "      a2 = max_dict(Q[s2])[0]\n",
        "      a2 = random_action(a2, eps=0.5/t) # epsilon-greedy\n",
        "\n",
        "      # we will update Q(s,a) AS we experience the episode\n",
        "      alpha = ALPHA / update_counts_sa[s][a]\n",
        "      update_counts_sa[s][a] += 0.005\n",
        "      old_qsa = Q[s][a]\n",
        "      Q[s][a] = Q[s][a] + alpha*(r + GAMMA*Q[s2][a2] - Q[s][a])\n",
        "      biggest_change = max(biggest_change, np.abs(old_qsa - Q[s][a]))\n",
        "\n",
        "      # we would like to know how often Q(s) has been updated too\n",
        "      update_counts[s] = update_counts.get(s,0) + 1\n",
        "\n",
        "      # next state becomes current state\n",
        "      s = s2\n",
        "      a = a2\n",
        "\n",
        "    deltas.append(biggest_change)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "it: 0\n",
            "it: 2000\n",
            "it: 4000\n",
            "it: 6000\n",
            "it: 8000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFxcijuO3ym0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "1a84d94f-1f79-4603-ed84-0f06bd8be272"
      },
      "source": [
        "plt.plot(deltas)\n",
        "plt.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDCRJGAaSJAwDSRLwf8bjZh00LacQAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIQ8Bmye33I7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # determine the policy from Q*\n",
        "  # find V* from Q*\n",
        "  policy = {}\n",
        "  V = {}\n",
        "  for s in grid.actions.keys():\n",
        "    a, max_q = max_dict(Q[s])\n",
        "    policy[s] = a\n",
        "    V[s] = max_q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvQsG16u3-Er",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "b0ead842-d44b-48ee-ee37-14ab08d72473"
      },
      "source": [
        "  # what's the proportion of time we spend updating each part of Q? (for debugging purposes)\n",
        "  print(\"update counts:\")\n",
        "  total = np.sum(list(update_counts.values()))\n",
        "  for k, v in update_counts.items():\n",
        "    update_counts[k] = float(v) / total\n",
        "  print_values(update_counts, grid)\n",
        "\n",
        "  print(\"values:\")\n",
        "  print_values(V, grid)\n",
        "  print(\"policy:\")\n",
        "  print_policy(policy, grid)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "update counts:\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "values:\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "policy:\n",
            "---------------------------\n",
            "  U  |  U  |  U  |     |\n",
            "---------------------------\n",
            "  U  |     |  U  |     |\n",
            "---------------------------\n",
            "  U  |  U  |  U  |  U  |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNjofFBgYJ9k",
        "colab_type": "text"
      },
      "source": [
        "## Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGt6mx474Qzc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GAMMA = 0.9\n",
        "ALPHA = 0.1\n",
        "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faN8Qw0DYO3x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # grid = standard_grid()\n",
        "  grid = negative_grid(step_cost=-0.1)\n",
        "\n",
        "  # print rewards\n",
        "  print(\"rewards:\")\n",
        "  print_values(grid.rewards, grid)\n",
        "\n",
        "  # no policy initialization, we will derive our policy from most recent Q\n",
        "\n",
        "  # initialize Q(s,a)\n",
        "  Q = {}\n",
        "  states = grid.all_states()\n",
        "  for s in states:\n",
        "    Q[s] = {}\n",
        "    for a in ALL_POSSIBLE_ACTIONS:\n",
        "      Q[s][a] = 0\n",
        "\n",
        "  # let's also keep track of how many times Q[s] has been updated\n",
        "  update_counts = {}\n",
        "  update_counts_sa = {}\n",
        "  for s in states:\n",
        "    update_counts_sa[s] = {}\n",
        "    for a in ALL_POSSIBLE_ACTIONS:\n",
        "      update_counts_sa[s][a] = 1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2HHx_7plZKv",
        "colab_type": "text"
      },
      "source": [
        "We employ adaptive epsilon (i.e. epsilon-greedy) and adaptive learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIdoQwYwYWha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # repeat until convergence\n",
        "  t = 1.0\n",
        "  deltas = []\n",
        "  for it in range(10000):\n",
        "    if it % 100 == 0:\n",
        "      t += 1e-2\n",
        "    if it % 2000 == 0:\n",
        "      print(\"it:\", it)\n",
        "\n",
        "    # instead of 'generating' an epsiode, we will PLAY\n",
        "    # an episode within this loop\n",
        "    s = (2, 0) # start state\n",
        "    grid.set_state(s)\n",
        "\n",
        "    # the first (s, r) tuple is the state we start in and 0\n",
        "    # (since we don't get a reward) for simply starting the game\n",
        "    # the last (s, r) tuple is the terminal state and the final reward\n",
        "    # the value for the terminal state is by definition 0, so we don't\n",
        "    # care about updating it.\n",
        "    a, _ = max_dict(Q[s])\n",
        "    biggest_change = 0\n",
        "    while not grid.game_over():\n",
        "      a = random_action(a, eps=0.5/t) # epsilon-greedy\n",
        "      # random action also works, but slower since you can bump into walls\n",
        "      # a = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
        "      r = grid.move(a)\n",
        "      s2 = grid.current_state()\n",
        "\n",
        "      # adaptive learning rate\n",
        "      alpha = ALPHA / update_counts_sa[s][a]\n",
        "      update_counts_sa[s][a] += 0.005\n",
        "\n",
        "      # we will update Q(s,a) AS we experience the episode\n",
        "      old_qsa = Q[s][a]\n",
        "      # the difference between SARSA and Q-Learning is with Q-Learning\n",
        "      # we will use this max[a']{ Q(s',a')} in our update\n",
        "      # even if we do not end up taking this action in the next step\n",
        "      a2, max_q_s2a2 = max_dict(Q[s2])\n",
        "      Q[s][a] = Q[s][a] + alpha*(r + GAMMA*max_q_s2a2 - Q[s][a])\n",
        "      biggest_change = max(biggest_change, np.abs(old_qsa - Q[s][a]))\n",
        "\n",
        "      # we would like to know how often Q(s) has been updated too\n",
        "      update_counts[s] = update_counts.get(s,0) + 1\n",
        "\n",
        "      # next state becomes current state\n",
        "      s = s2\n",
        "      a = a2\n",
        "     \n",
        "    deltas.append(biggest_change)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLV9B04VYYbX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(deltas)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ar_-TmyXYb9m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # determine the policy from Q*\n",
        "  # find V* from Q*\n",
        "  policy = {}\n",
        "  V = {}\n",
        "  for s in grid.actions.keys():\n",
        "    a, max_q = max_dict(Q[s])\n",
        "    policy[s] = a\n",
        "    V[s] = max_q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3ZYAYT7YcBD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # what's the proportion of time we spend updating each part of Q?\n",
        "  print(\"update counts:\")\n",
        "  total = np.sum(list(update_counts.values()))\n",
        "  for k, v in update_counts.items():\n",
        "    update_counts[k] = float(v) / total\n",
        "  print_values(update_counts, grid)\n",
        "\n",
        "  print(\"values:\")\n",
        "  print_values(V, grid)\n",
        "  print(\"policy:\")\n",
        "  print_policy(policy, grid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQS6gk_vahXa",
        "colab_type": "text"
      },
      "source": [
        "## Approximation with Monte-Carlo prediction problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyUjcLI7a-qT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # use the standard grid again (0 for every step) so that we can compare\n",
        "  # to iterative policy evaluation\n",
        "  grid = standard_grid()\n",
        "\n",
        "  # print rewards\n",
        "  print(\"rewards:\")\n",
        "  print_values(grid.rewards, grid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1huTYZEbESi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  policy = {\n",
        "    (2, 0): 'U',\n",
        "    (1, 0): 'U',\n",
        "    (0, 0): 'R',\n",
        "    (0, 1): 'R',\n",
        "    (0, 2): 'R',\n",
        "    (1, 2): 'U',\n",
        "    (2, 1): 'L',\n",
        "    (2, 2): 'U',\n",
        "    (2, 3): 'L',\n",
        "  }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5YUriQMbeZr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_action(a):\n",
        "  # choose given a with probability 0.5\n",
        "  # choose some other a' != a with probability 0.5/3\n",
        "  p = np.random.random()\n",
        "  if p < 0.5:\n",
        "    return a\n",
        "  else:\n",
        "    tmp = list(ALL_POSSIBLE_ACTIONS)\n",
        "    tmp.remove(a)\n",
        "    return np.random.choice(tmp)\n",
        "\n",
        "def play_game(grid, policy):\n",
        "  # returns a list of states and corresponding returns\n",
        "\n",
        "  # reset game to start at a random position\n",
        "  # we need to do this, because given our current deterministic policy\n",
        "  # we would never end up at certain states, but we still want to measure their value\n",
        "  start_states = list(grid.actions.keys())\n",
        "  start_idx = np.random.choice(len(start_states))\n",
        "  grid.set_state(start_states[start_idx])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXFZosB1bHuG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # initialize theta\n",
        "  # our model is V_hat = theta.dot(x)\n",
        "  # where x = [row, col, row*col, 1] - 1 for bias term\n",
        "  theta = np.random.randn(4) / 2 # randomly intialize the theta vector\n",
        "  def s2x(s): # define a function that turns the state into a feature vector X.\n",
        "    return np.array([s[0] - 1, s[1] - 1.5, s[0]*s[1] - 3, 1]) # 1 for the bias term. The only non-linear feature here is the interaction effect between the i and j coordinates."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u03QeMQPbnJ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LEARNING_RATE = 0.001 # used in gradient descent\n",
        "SMALL_ENOUGH = 1e-3\n",
        "GAMMA = 0.9\n",
        "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9p5jTBjWbMN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # repeat until convergence\n",
        "  deltas = []\n",
        "  t = 1.0\n",
        "  for it in range(20000):\n",
        "    if it % 100 == 0:\n",
        "      t += 0.01\n",
        "    alpha = LEARNING_RATE/t # decaying learning rate\n",
        "    # generate an episode using pi\n",
        "    biggest_change = 0\n",
        "    states_and_returns = play_game(grid, policy)\n",
        "    seen_states = set()\n",
        "    for s, G in states_and_returns:\n",
        "      # check if we have already seen s\n",
        "      # called \"first-visit\" MC policy evaluation\n",
        "      if s not in seen_states:\n",
        "        old_theta = theta.copy()\n",
        "        x = s2x(s)\n",
        "        V_hat = theta.dot(x)\n",
        "        # grad(V_hat) wrt theta = x\n",
        "        theta += alpha*(G - V_hat)*x\n",
        "        biggest_change = max(biggest_change, np.abs(old_theta - theta).sum())\n",
        "        seen_states.add(s)\n",
        "    deltas.append(biggest_change)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbXkJSuibPE8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(deltas)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDEAVS5fbRxq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # obtain predicted values\n",
        "  V = {}\n",
        "  states = grid.all_states()\n",
        "  for s in states:\n",
        "    if s in grid.actions:\n",
        "      V[s] = theta.dot(s2x(s))\n",
        "    else:\n",
        "      # terminal state or state we can't otherwise get to\n",
        "      V[s] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bz4fnnv6bUL6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  print(\"values:\")\n",
        "  print_values(V, grid)\n",
        "  print(\"policy:\")\n",
        "  print_policy(policy, grid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_ZilCXm7LIl",
        "colab_type": "text"
      },
      "source": [
        "## TD(0) with approximation\n",
        "\n",
        "Do not need to calculate returns, use rewards directly. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h-PiD07aqdB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NOTE: this is only policy evaluation, not optimization\n",
        "\n",
        "class Model:\n",
        "  def __init__(self):\n",
        "    self.theta = np.random.randn(4) / 2 # initialize data\n",
        "  \n",
        "  def s2x(self, s): # tranforms state into a feature vector, we are using the same features as in Monte-Carlo (these may not be good features)\n",
        "    return np.array([s[0] - 1, s[1] - 1.5, s[0]*s[1] - 3, 1])\n",
        "\n",
        "  def predict(self, s): # transforms a state s into a feature x and returns a dot product between theta and x\n",
        "    x = self.s2x(s)\n",
        "    return self.theta.dot(x) # linear model\n",
        "\n",
        "  def grad(self, s):\n",
        "    return self.s2x(s) # just transforms s into x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFdB28Fp7xCI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # use the standard grid again (0 for every step) so that we can compare\n",
        "  # to iterative policy evaluation\n",
        "  grid = standard_grid()\n",
        "\n",
        "  # print rewards\n",
        "  print(\"rewards:\")\n",
        "  print_values(grid.rewards, grid)\n",
        "\n",
        "  # state -> action\n",
        "  policy = { # direct path to goal if not in that path we go directly to the loosing state.\n",
        "    (2, 0): 'U',\n",
        "    (1, 0): 'U',\n",
        "    (0, 0): 'R',\n",
        "    (0, 1): 'R',\n",
        "    (0, 2): 'R',\n",
        "    (1, 2): 'R',\n",
        "    (2, 1): 'R',\n",
        "    (2, 2): 'R',\n",
        "    (2, 3): 'U',\n",
        "  }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyEmMTYh97zN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a model\n",
        "model = Model()\n",
        "deltas = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVbjbHam_Idr",
        "colab_type": "text"
      },
      "source": [
        "**Enter main loop - Game play**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvbZ2gig9_Mz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # repeat until convergence\n",
        "  k = 1.0\n",
        "  for it in range(20000):\n",
        "    if it % 10 == 0:\n",
        "      k += 0.01\n",
        "    alpha = ALPHA/k\n",
        "    biggest_change = 0\n",
        "\n",
        "    # generate an episode using pi\n",
        "    states_and_rewards = play_game(grid, policy)\n",
        "    # the first (s, r) tuple is the state we start in and 0\n",
        "    # (since we don't get a reward) for simply starting the game\n",
        "    # the last (s, r) tuple is the terminal state and the final reward\n",
        "    # the value for the terminal state is by definition 0, so we don't\n",
        "    # care about updating it.\n",
        "    for t in range(len(states_and_rewards) - 1):\n",
        "      s, _ = states_and_rewards[t]\n",
        "      s2, r = states_and_rewards[t+1]\n",
        "      # we will update V(s) AS we experience the episode\n",
        "      old_theta = model.theta.copy()\n",
        "      if grid.is_terminal(s2):\n",
        "        target = r\n",
        "      else:\n",
        "        target = r + GAMMA*model.predict(s2)\n",
        "      # update equation for theta\n",
        "      model.theta += alpha*(target - model.predict(s))*model.grad(s)\n",
        "      biggest_change = max(biggest_change, np.abs(old_theta - model.theta).sum())\n",
        "    deltas.append(biggest_change)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7m3-wF74-Bdn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(deltas)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sd_2Svs-FF7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # obtain predicted values\n",
        "  V = {}\n",
        "  states = grid.all_states()\n",
        "  for s in states:\n",
        "    if s in grid.actions:\n",
        "      V[s] = model.predict(s)\n",
        "    else:\n",
        "      # terminal state or state we can't otherwise get to\n",
        "      V[s] = 0\n",
        "\n",
        "  print(\"values:\")\n",
        "  print_values(V, grid)\n",
        "  print(\"policy:\")\n",
        "  print_policy(policy, grid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRsrnDseLDJN",
        "colab_type": "text"
      },
      "source": [
        "## Approxation Semi-Gradient SARSA Control Problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yz8w4S8TLLNd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " GAMMA, ALPHA, ALL_POSSIBLE_ACTIONS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wsUk9tZLNiA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model:\n",
        "  def __init__(self):\n",
        "    self.theta = np.random.randn(25) / np.sqrt(25)\n",
        "    # if we use SA2IDX, a one-hot encoding for every (s,a) pair\n",
        "    # in reality we wouldn't want to do this b/c we have just\n",
        "    # as many params as before\n",
        "    # print \"D:\", IDX\n",
        "    # self.theta = np.random.randn(IDX) / np.sqrt(IDX)\n",
        "\n",
        "  def sa2x(self, s, a):\n",
        "    # NOTE: using just (r, c, r*c, u, d, l, r, 1) is not expressive enough\n",
        "    return np.array([ # dimension 25 (5 actions x 5 engineered features)\n",
        "      s[0] - 1              if a == 'U' else 0,\n",
        "      s[1] - 1.5            if a == 'U' else 0,\n",
        "      (s[0]*s[1] - 3)/3     if a == 'U' else 0,\n",
        "      (s[0]*s[0] - 2)/2     if a == 'U' else 0,\n",
        "      (s[1]*s[1] - 4.5)/4.5 if a == 'U' else 0,\n",
        "      1                     if a == 'U' else 0,\n",
        "      s[0] - 1              if a == 'D' else 0,\n",
        "      s[1] - 1.5            if a == 'D' else 0,\n",
        "      (s[0]*s[1] - 3)/3     if a == 'D' else 0,\n",
        "      (s[0]*s[0] - 2)/2     if a == 'D' else 0,\n",
        "      (s[1]*s[1] - 4.5)/4.5 if a == 'D' else 0,\n",
        "      1                     if a == 'D' else 0,\n",
        "      s[0] - 1              if a == 'L' else 0,\n",
        "      s[1] - 1.5            if a == 'L' else 0,\n",
        "      (s[0]*s[1] - 3)/3     if a == 'L' else 0,\n",
        "      (s[0]*s[0] - 2)/2     if a == 'L' else 0,\n",
        "      (s[1]*s[1] - 4.5)/4.5 if a == 'L' else 0,\n",
        "      1                     if a == 'L' else 0,\n",
        "      s[0] - 1              if a == 'R' else 0,\n",
        "      s[1] - 1.5            if a == 'R' else 0,\n",
        "      (s[0]*s[1] - 3)/3     if a == 'R' else 0,\n",
        "      (s[0]*s[0] - 2)/2     if a == 'R' else 0,\n",
        "      (s[1]*s[1] - 4.5)/4.5 if a == 'R' else 0,\n",
        "      1                     if a == 'R' else 0,\n",
        "      1\n",
        "    ])\n",
        "    # UNCOMMENT TO DO REGULAR SARSA METHOD\n",
        "    # if we use SA2IDX, a one-hot encoding for every (s,a) pair\n",
        "    # in reality we wouldn't want to do this b/c we have just\n",
        "    # as many params as before\n",
        "    # x = np.zeros(len(self.theta))\n",
        "    # idx = SA2IDX[s][a]\n",
        "    # x[idx] = 1\n",
        "    # return x\n",
        "\n",
        "  def predict(self, s, a):\n",
        "    x = self.sa2x(s, a)\n",
        "    return self.theta.dot(x)\n",
        "\n",
        "  def grad(self, s, a):\n",
        "    return self.sa2x(s, a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx6h264vLNfK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getQs(model, s): # job of this fuction is to turn Q-predictions into a dictionary given some state s. \n",
        "  # we need Q(s,a) to choose an action\n",
        "  # i.e. a = argmax[a]{ Q(s,a) }\n",
        "  Qs = {}\n",
        "  for a in ALL_POSSIBLE_ACTIONS:\n",
        "    q_sa = model.predict(s, a)\n",
        "    Qs[a] = q_sa\n",
        "  return Qs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dfdi_qkyLXgB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # grid = standard_grid()\n",
        "  grid = negative_grid(step_cost=-0.1)\n",
        "\n",
        "  # print rewards\n",
        "  print(\"rewards:\")\n",
        "  print_values(grid.rewards, grid)\n",
        "\n",
        "  # no policy initialization, we will derive our policy from most recent Q\n",
        "  # enumerate all (s,a) pairs, each will have its own weight in our \"dumb\" model\n",
        "  # essentially each weight will be a measure of Q(s,a) itself\n",
        "  states = grid.all_states()\n",
        "  for s in states:\n",
        "    SA2IDX[s] = {}\n",
        "    for a in ALL_POSSIBLE_ACTIONS: # populate the SA2IDX dictionary\n",
        "      SA2IDX[s][a] = IDX\n",
        "      IDX += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RF8EYl6iLZ3w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize model\n",
        "model = Model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApeOK2aKPGX8",
        "colab_type": "text"
      },
      "source": [
        "**Enter the main loop**\n",
        "\n",
        "Here we have two time variables t1 and t2 (these are hyper-parameters), since we want the learning rate and  epsilon to decrease at different rates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ch0RZsLyLf2K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # repeat until convergence\n",
        "  t = 1.0\n",
        "  t2 = 1.0\n",
        "  deltas = []\n",
        "  for it in range(20000):\n",
        "    if it % 100 == 0:\n",
        "      t += 0.01\n",
        "      t2 += 0.01\n",
        "    if it % 1000 == 0:\n",
        "      print(\"it:\", it)\n",
        "    alpha = ALPHA / t2\n",
        "\n",
        "    # instead of 'generating' an epsiode, we will PLAY\n",
        "    # an episode within this loop\n",
        "    s = (2, 0) # start state\n",
        "    grid.set_state(s)\n",
        "\n",
        "    # get Q(s) so we can choose the first action\n",
        "    Qs = getQs(model, s)\n",
        "\n",
        "    # the first (s, r) tuple is the state we start in and 0\n",
        "    # (since we don't get a reward) for simply starting the game\n",
        "    # the last (s, r) tuple is the terminal state and the final reward\n",
        "    # the value for the terminal state is by definition 0, so we don't\n",
        "    # care about updating it.\n",
        "    a = max_dict(Qs)[0]\n",
        "    a = random_action(a, eps=0.5/t) # epsilon-greedy\n",
        "    biggest_change = 0\n",
        "    while not grid.game_over():\n",
        "      r = grid.move(a)\n",
        "      s2 = grid.current_state()\n",
        "\n",
        "      # we need the next action as well since Q(s,a) depends on Q(s',a')\n",
        "      # if s2 not in policy then it's a terminal state, all Q are 0\n",
        "      old_theta = model.theta.copy()\n",
        "      if grid.is_terminal(s2):\n",
        "        model.theta += alpha*(r - model.predict(s, a))*model.grad(s, a)\n",
        "      else:\n",
        "        # not terminal\n",
        "        Qs2 = getQs(model, s2)\n",
        "        a2 = max_dict(Qs2)[0]\n",
        "        a2 = random_action(a2, eps=0.5/t) # epsilon-greedy\n",
        "\n",
        "        # we will update Q(s,a) AS we experience the episode\n",
        "        model.theta += alpha*(r + GAMMA*model.predict(s2, a2) - model.predict(s, a))*model.grad(s, a)\n",
        "        \n",
        "        # next state becomes current state\n",
        "        s = s2\n",
        "        a = a2\n",
        "\n",
        "      biggest_change = max(biggest_change, np.abs(model.theta - old_theta).sum())\n",
        "    deltas.append(biggest_change)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7w1NVguWLNck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(deltas)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItPeFef2LkPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # determine the policy from Q*\n",
        "  # find V* from Q*\n",
        "  policy = {}\n",
        "  V = {}\n",
        "  Q = {}\n",
        "  for s in grid.actions.keys():\n",
        "    Qs = getQs(model, s)\n",
        "    Q[s] = Qs\n",
        "    a, max_q = max_dict(Qs)\n",
        "    policy[s] = a\n",
        "    V[s] = max_q\n",
        "\n",
        "  print(\"values:\")\n",
        "  print_values(V, grid)\n",
        "  print(\"policy:\")\n",
        "  print_policy(policy, grid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwgCdQECL6Hu",
        "colab_type": "text"
      },
      "source": [
        "## Approxation Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnYvS3f5L_UJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GAMMA, ALPHA, ALL_POSSIBLE_ACTIONS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epxy5tOSMO3J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "SA2IDX = {}\n",
        "IDX = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QMVEIMMM4Qn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_action(a, eps=0.1):\n",
        "  # we'll use epsilon-soft to ensure all states are visited\n",
        "  # what happens if you don't do this? i.e. eps=0\n",
        "  p = np.random.random()\n",
        "  if p < (1 - eps):\n",
        "    return a\n",
        "  else:\n",
        "    return np.random.choice(ALL_POSSIBLE_ACTIONS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xgoh2-WpMO7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model:\n",
        "  def __init__(self):\n",
        "    self.theta = np.random.randn(25) / np.sqrt(25)\n",
        "    # if we use SA2IDX, a one-hot encoding for every (s,a) pair\n",
        "    # in reality we wouldn't want to do this b/c we have just\n",
        "    # as many params as before\n",
        "    # print \"D:\", IDX\n",
        "    # self.theta = np.random.randn(IDX) / np.sqrt(IDX)\n",
        "\n",
        "  def sa2x(self, s, a):\n",
        "    # NOTE: using just (r, c, r*c, u, d, l, r, 1) is not expressive enough\n",
        "    return np.array([\n",
        "      s[0] - 1              if a == 'U' else 0,\n",
        "      s[1] - 1.5            if a == 'U' else 0,\n",
        "      (s[0]*s[1] - 3)/3     if a == 'U' else 0,\n",
        "      (s[0]*s[0] - 2)/2     if a == 'U' else 0,\n",
        "      (s[1]*s[1] - 4.5)/4.5 if a == 'U' else 0,\n",
        "      1                     if a == 'U' else 0,\n",
        "      s[0] - 1              if a == 'D' else 0,\n",
        "      s[1] - 1.5            if a == 'D' else 0,\n",
        "      (s[0]*s[1] - 3)/3     if a == 'D' else 0,\n",
        "      (s[0]*s[0] - 2)/2     if a == 'D' else 0,\n",
        "      (s[1]*s[1] - 4.5)/4.5 if a == 'D' else 0,\n",
        "      1                     if a == 'D' else 0,\n",
        "      s[0] - 1              if a == 'L' else 0,\n",
        "      s[1] - 1.5            if a == 'L' else 0,\n",
        "      (s[0]*s[1] - 3)/3     if a == 'L' else 0,\n",
        "      (s[0]*s[0] - 2)/2     if a == 'L' else 0,\n",
        "      (s[1]*s[1] - 4.5)/4.5 if a == 'L' else 0,\n",
        "      1                     if a == 'L' else 0,\n",
        "      s[0] - 1              if a == 'R' else 0,\n",
        "      s[1] - 1.5            if a == 'R' else 0,\n",
        "      (s[0]*s[1] - 3)/3     if a == 'R' else 0,\n",
        "      (s[0]*s[0] - 2)/2     if a == 'R' else 0,\n",
        "      (s[1]*s[1] - 4.5)/4.5 if a == 'R' else 0,\n",
        "      1                     if a == 'R' else 0,\n",
        "      1\n",
        "    ])\n",
        "    # if we use SA2IDX, a one-hot encoding for every (s,a) pair\n",
        "    # in reality we wouldn't want to do this b/c we have just\n",
        "    # as many params as before\n",
        "    # x = np.zeros(len(self.theta))\n",
        "    # idx = SA2IDX[s][a]\n",
        "    # x[idx] = 1\n",
        "    # return x\n",
        "\n",
        "  def predict(self, s, a):\n",
        "    x = self.sa2x(s, a)\n",
        "    return self.theta.dot(x)\n",
        "\n",
        "  def grad(self, s, a):\n",
        "    return self.sa2x(s, a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SME7IsvWMO0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getQs(model, s):\n",
        "  # we need Q(s,a) to choose an action\n",
        "  # i.e. a = argmax[a]{ Q(s,a) }\n",
        "  Qs = {}\n",
        "  for a in ALL_POSSIBLE_ACTIONS:\n",
        "    q_sa = model.predict(s, a)\n",
        "    Qs[a] = q_sa\n",
        "  return Qs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVeUMt7rNAAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # grid = standard_grid()\n",
        "  grid = negative_grid(step_cost=-0.1)\n",
        "\n",
        "  # print rewards\n",
        "  print(\"rewards:\")\n",
        "  print_values(grid.rewards, grid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8hU4QArNB6_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # no policy initialization, we will derive our policy from most recent Q\n",
        "  # enumerate all (s,a) pairs, each will have its own weight in our \"dumb\" model\n",
        "  # essentially each weight will be a measure of Q(s,a) itself\n",
        "  states = grid.all_states()\n",
        "  for s in states:\n",
        "    SA2IDX[s] = {}\n",
        "    for a in ALL_POSSIBLE_ACTIONS:\n",
        "      SA2IDX[s][a] = IDX\n",
        "      IDX += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgjqc5rpNB34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize model\n",
        "  model = Model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSg0PivHNK7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # repeat until convergence\n",
        "  t = 1.0\n",
        "  t2 = 1.0\n",
        "  deltas = []\n",
        "  for it in range(20000):\n",
        "    if it % 100 == 0:\n",
        "      t += 0.01\n",
        "      t2 += 0.01\n",
        "    if it % 1000 == 0:\n",
        "      print(\"it:\", it)\n",
        "    alpha = ALPHA / t2\n",
        "\n",
        "    # instead of 'generating' an epsiode, we will PLAY\n",
        "    # an episode within this loop\n",
        "    s = (2, 0) # start state\n",
        "    grid.set_state(s)\n",
        "\n",
        "    # get Q(s) so we can choose the first action\n",
        "    Qs = getQs(model, s)\n",
        "\n",
        "    # the first (s, r) tuple is the state we start in and 0\n",
        "    # (since we don't get a reward) for simply starting the game\n",
        "    # the last (s, r) tuple is the terminal state and the final reward\n",
        "    # the value for the terminal state is by definition 0, so we don't\n",
        "    # care about updating it.\n",
        "    a = max_dict(Qs)[0]\n",
        "    a = random_action(a, eps=0.5/t) # epsilon-greedy\n",
        "    biggest_change = 0\n",
        "    while not grid.game_over():\n",
        "      r = grid.move(a)\n",
        "      s2 = grid.current_state()\n",
        "\n",
        "      # we need the next action as well since Q(s,a) depends on Q(s',a')\n",
        "      # if s2 not in policy then it's a terminal state, all Q are 0\n",
        "      old_theta = model.theta.copy()\n",
        "      if grid.is_terminal(s2):\n",
        "        model.theta += alpha*(r - model.predict(s, a))*model.grad(s, a)\n",
        "      else:\n",
        "        # not terminal\n",
        "        Qs2 = getQs(model, s2)\n",
        "        a2, maxQs2a2 = max_dict(Qs2)\n",
        "        a2 = random_action(a2, eps=0.5/t) # epsilon-greedy\n",
        "\n",
        "        # we will update Q(s,a) AS we experience the episode\n",
        "        model.theta += alpha*(r + GAMMA*maxQs2a2 - model.predict(s, a))*model.grad(s, a)\n",
        "        \n",
        "        # next state becomes current state\n",
        "        s = s2\n",
        "        a = a2\n",
        "\n",
        "      biggest_change = max(biggest_change, np.abs(model.theta - old_theta).sum())\n",
        "    deltas.append(biggest_change)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prsbSlKcNM_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(deltas)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KvCWohGNM8d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # determine the policy from Q*\n",
        "  # find V* from Q*\n",
        "  policy = {}\n",
        "  V = {}\n",
        "  Q = {}\n",
        "  for s in grid.actions.keys():\n",
        "    Qs = getQs(model, s)\n",
        "    Q[s] = Qs\n",
        "    a, max_q = max_dict(Qs)\n",
        "    policy[s] = a\n",
        "    V[s] = max_q\n",
        "\n",
        "  print(\"values:\")\n",
        "  print_values(V, grid)\n",
        "  print(\"policy:\")\n",
        "  print_policy(policy, grid)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}