{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning for NChain with Open AI Gym\n",
    "\n",
    "The NChain example on Open AI Gym is a simple 5 state environment. There are two possible actions in each state, move forward (action 0) and move backwards (action 1). \n",
    "\n",
    "The diagram below demonstrates this environment:\n",
    "\n",
    "![Open AI Gym's NChain environment](https://i0.wp.com/adventuresinmachinelearning.com/wp-content/uploads/2018/02/NChain-illustration.png?w=906&ssl=1)\n",
    "\n",
    "In order to train the agent effectively, we need to find a good policy $\\pi$ which maps states to actions in an optimal way to maximize reward. There are various ways of going about finding a good or optimal policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, InputLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment is loaded\n",
    "env = gym.make('NChain-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2, False, {})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# action ID == 1\n",
    "action_id = 1\n",
    "env.step(action_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment returns the new state, the reward for this action, whether the game is “done” at this stage and the debugging information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0, False, {})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returns 4 variables: new state after the action, reward due to the action, \n",
    "# Whether the game is “done” or not – the NChain game is done after 1,000 steps, Debugging information \n",
    "env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 0, False, {})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 0, False, {})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive reinforcement learning\n",
    "\n",
    "The agent training model using Q learning, the action selection policy is based solely on the maximum Q value in any given state. It is conceivable that, given the random nature of the environment, that the agent initially makes “bad” decisions. The Q values arising from these decisions may easily be “locked in” – and from that time forward, bad decisions may continue to be made by the agent because it can only ever select the maximum Q value in any given state, even if these values are not necessarily optimal. This action selection policy is called a greedy policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy – choose the action resulting in the greatest previous summated reward\n",
    "def naive_sum_reward_agent(env, num_episodes=500):\n",
    "    # this is the table that will hold our summated rewards for\n",
    "    # each action in each state\n",
    "    r_table = np.zeros((5, 2))\n",
    "    \n",
    "    for g in range(num_episodes):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if np.sum(r_table[s, :]) == 0:\n",
    "                # make a random selection of actions\n",
    "                a = np.random.randint(0, 2)\n",
    "            else:\n",
    "                # select the action with highest cummulative reward\n",
    "                a = np.argmax(r_table[s, :])\n",
    "                \n",
    "            new_s, r, done, _ = env.step(a)\n",
    "            # naive learning rule\n",
    "            r_table[s, a] += r\n",
    "            s = new_s\n",
    "            \n",
    "    return r_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wit this approach will lead to the table being “locked in” with respect to actions after just a few steps in the game, and this model for training the agent has no way to encourage acting on delayed reward signal when it is appropriate for it to do so.\n",
    "\n",
    "### Delayed reward reinforcement learning\n",
    "\n",
    "This idea of propagating possible reward from the best possible actions in future states is a core component of what is called Q learning. In Q learning, the Q value for each action in each state is updated when the relevant information is made available. Return of the maximum Q value for the best possible action in the next state. In this way, the agent is looking forward to determine the best possible future rewards before making the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_with_table(env, num_episodes=500):\n",
    "    y = 0.95\n",
    "    lr = 0.8\n",
    "    q_table = np.zeros((5, 2))\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if np.sum(q_table[s,:]) == 0:\n",
    "                # make a random selection of actions\n",
    "                a = np.random.randint(0, 2)\n",
    "            else:\n",
    "                # select the action with largest q value in state s\n",
    "                a = np.argmax(q_table[s, :]) # the action with the highest q value.\n",
    "                \n",
    "            new_s, r, done, _ = env.step(a)\n",
    "            # Q learning rule\n",
    "            q_table[s, a] += r + lr*(y*np.max(q_table[new_s, :]) - q_table[s, a])\n",
    "            s = new_s\n",
    "            \n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this approach there isn’t enough exploration going on within the agent training method.\n",
    "\n",
    "### Q learning with ϵ-greedy action selection reinforcement learning\n",
    "\n",
    "We need a way for the agent to eventually always choose the “best” set of actions in the environment, yet at the same time allowing the agent to not get “locked in” and giving it some space to explore alternatives. What is required is the  ϵ-greedy policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy_q_learning_with_table(env, num_episodes=500):\n",
    "    y = 0.95\n",
    "    eps = 0.5\n",
    "    lr = 0.8\n",
    "    decay_factor = 0.999\n",
    "    q_table = np.zeros((5, 2))\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        s = env.reset()\n",
    "        eps *= decay_factor\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # select the action with highest cummulative reward\n",
    "            if np.random.random() < eps or np.sum(q_table[s, :]) == 0:\n",
    "                a = np.random.randint(0, 2) # the action will be selected randomly from the two possible actions in each state. \n",
    "            else:\n",
    "                a = np.argmax(q_table[s, :]) # taking the action with the highest q value.\n",
    "                \n",
    "            # pdb.set_trace()\n",
    "            new_s, r, done, _ = env.step(a)\n",
    "            q_table[s, a] += r + lr * (y * np.max(q_table[new_s, :]) - q_table[s, a])\n",
    "            s = new_s\n",
    "            \n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ϵ-greedy policy in reinforcement learning is basically the same as the greedy policy, except that there is a value ϵ (which may be set to decay over time) where, if a random number is selected which is less than this value, an action is chosen completely at random. This step allows some random exploration of the value of various actions in various states, and can be scaled back over time to allow the algorithm to concentrate more on exploiting the best strategies that it has found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the methods\n",
    "\n",
    "Check which of the last agent training models actually produces an agent that gathers the most rewards in any given game, to see which agent performs the best over a test game.\n",
    "\n",
    " The models are trained as well as tested in each iteration because there is significant variability in the environment which messes around with the efficacy of the training – so this is an attempt to understand average performance of the different models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_game(table, env):\n",
    "    \n",
    "    s = env.reset()\n",
    "    tot_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        a = np.argmax(table[s, :])\n",
    "        s, r, done, _ = env.step(a)      \n",
    "        tot_reward += r\n",
    "        \n",
    "    return tot_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_methods(env, num_iterations=100):\n",
    "    winner = np.zeros((3,))\n",
    "    \n",
    "    for g in range(num_iterations):\n",
    "        \n",
    "        m0_table = naive_sum_reward_agent(env, 500)\n",
    "        m1_table = q_learning_with_table(env, 500)\n",
    "        m2_table = eps_greedy_q_learning_with_table(env, 500)\n",
    "        \n",
    "        m0 = run_game(m0_table, env)\n",
    "        m1 = run_game(m1_table, env)\n",
    "        m2 = run_game(m2_table, env)\n",
    "        \n",
    "        w = np.argmax(np.array([m0, m1, m2]))\n",
    "        winner[w] += 1\n",
    "        \n",
    "        print(\"Game {} of {}\".format(g + 1, num_iterations))\n",
    "        \n",
    "    return winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 1 of 100\n",
      "Game 2 of 100\n",
      "Game 3 of 100\n",
      "Game 4 of 100\n",
      "Game 5 of 100\n",
      "Game 6 of 100\n",
      "Game 7 of 100\n",
      "Game 8 of 100\n",
      "Game 9 of 100\n",
      "Game 10 of 100\n",
      "Game 11 of 100\n",
      "Game 12 of 100\n",
      "Game 13 of 100\n",
      "Game 14 of 100\n",
      "Game 15 of 100\n",
      "Game 16 of 100\n",
      "Game 17 of 100\n",
      "Game 18 of 100\n",
      "Game 19 of 100\n",
      "Game 20 of 100\n",
      "Game 21 of 100\n",
      "Game 22 of 100\n",
      "Game 23 of 100\n",
      "Game 24 of 100\n",
      "Game 25 of 100\n",
      "Game 26 of 100\n",
      "Game 27 of 100\n",
      "Game 28 of 100\n",
      "Game 29 of 100\n",
      "Game 30 of 100\n",
      "Game 31 of 100\n",
      "Game 32 of 100\n",
      "Game 33 of 100\n",
      "Game 34 of 100\n",
      "Game 35 of 100\n",
      "Game 36 of 100\n",
      "Game 37 of 100\n",
      "Game 38 of 100\n",
      "Game 39 of 100\n",
      "Game 40 of 100\n",
      "Game 41 of 100\n",
      "Game 42 of 100\n",
      "Game 43 of 100\n",
      "Game 44 of 100\n",
      "Game 45 of 100\n",
      "Game 46 of 100\n",
      "Game 47 of 100\n",
      "Game 48 of 100\n",
      "Game 49 of 100\n",
      "Game 50 of 100\n",
      "Game 51 of 100\n",
      "Game 52 of 100\n",
      "Game 53 of 100\n",
      "Game 54 of 100\n",
      "Game 55 of 100\n",
      "Game 56 of 100\n",
      "Game 57 of 100\n",
      "Game 58 of 100\n",
      "Game 59 of 100\n",
      "Game 60 of 100\n",
      "Game 61 of 100\n",
      "Game 62 of 100\n",
      "Game 63 of 100\n",
      "Game 64 of 100\n",
      "Game 65 of 100\n",
      "Game 66 of 100\n",
      "Game 67 of 100\n",
      "Game 68 of 100\n",
      "Game 69 of 100\n",
      "Game 70 of 100\n",
      "Game 71 of 100\n",
      "Game 72 of 100\n",
      "Game 73 of 100\n",
      "Game 74 of 100\n",
      "Game 75 of 100\n",
      "Game 76 of 100\n",
      "Game 77 of 100\n",
      "Game 78 of 100\n",
      "Game 79 of 100\n",
      "Game 80 of 100\n",
      "Game 81 of 100\n",
      "Game 82 of 100\n",
      "Game 83 of 100\n",
      "Game 84 of 100\n",
      "Game 85 of 100\n",
      "Game 86 of 100\n",
      "Game 87 of 100\n",
      "Game 88 of 100\n",
      "Game 89 of 100\n",
      "Game 90 of 100\n",
      "Game 91 of 100\n",
      "Game 92 of 100\n",
      "Game 93 of 100\n",
      "Game 94 of 100\n",
      "Game 95 of 100\n",
      "Game 96 of 100\n",
      "Game 97 of 100\n",
      "Game 98 of 100\n",
      "Game 99 of 100\n",
      "Game 100 of 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 8., 21., 71.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# executing reinforcement learning to run NChain game\n",
    "test_methods(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be observed, of the 100 experiments the ϵ-greedy, Q learning algorithm wins 65 of them. This is followed by the standard greedy implementation of Q learning, which won 22 of the experiments. Finally the naive accumulated rewards method only won 13 experiments. So as can be seen, the ϵ-greedy Q learning method is quite an effective way of executing reinforcement learning.\n",
    "\n",
    "Here, it can be observed that the trained table given to the function is used for action selection, and the total reward accumulated during the game is returned.\n",
    "\n",
    "We have been dealing with explicit tables to hold information about the best actions and which actions to choose in any given state, perfectly reasonable approach for a small environment, the table gets far too large and unwieldy for more complicated environments which have a huge number of states and potential actions.\n",
    "\n",
    "This is where neural networks can be used in reinforcement learning. Instead of having explicit tables, instead we can train a neural network to predict Q values for each action in a given state.\n",
    "\n",
    "## Reinforcement learning with neural networks\n",
    "\n",
    "To develop a neural network which can perform Q learning, the input needs to be the current state (plus potentially some other information about the environment) and it needs to output the relevant Q values for each action in that state. The Q values which are output should approach, as training progresses, the values produced in the Q learning updating rule. \n",
    "\n",
    " The loss function for the neural network should be:\n",
    " \n",
    " $loss=(r+\\gamma max_{a'} Q'(s',a') - Q(s,a))$,\n",
    " \n",
    " where $r+\\gamma max_{a'} Q'(s',a')$ is the target and Q(s,a) is the prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# The input to the network is the one-hot encoded state vector (e.g. state 1 is [0, 1, 0, 0, 0] and state 3 is [0, 0, 0, 1, 0]).\n",
    "model.add(InputLayer(batch_input_shape=(1, 5)))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "# Two Q values assigned to each state to represent the two possible actions.\n",
    "model.add(Dense(2, activation='linear'))\n",
    "#model.add(BatchNormalization(axis = 3))\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a fuction too call this model in the training environment, ϵ -greedy Q learning methodology with a deep neural network as opposed to an explicit Q table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy_deep_q_learning(env, num_episodes=500):\n",
    "    # now execute the q learning\n",
    "    y = 0.95\n",
    "    eps = 0.5\n",
    "    decay_factor = 0.999\n",
    "    r_avg_list = []\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        s = env.reset()\n",
    "        eps *= decay_factor\n",
    "        if i % 100 == 0:\n",
    "            print(\"Episode {} of {}\".format(i + 1, num_episodes))\n",
    "        done = False\n",
    "        r_sum = 0\n",
    "        while not done:\n",
    "            if np.random.random() < eps:\n",
    "                a = np.random.randint(0, 2)\n",
    "            else:\n",
    "                a = np.argmax(model.predict(np.identity(5)[s:s + 1]))\n",
    "            new_s, r, done, _ = env.step(a)\n",
    "            target = r + y * np.max(model.predict(np.identity(5)[new_s:new_s + 1]))\n",
    "            target_vec = model.predict(np.identity(5)[s:s + 1])[0]\n",
    "            target_vec[a] = target\n",
    "            model.fit(np.identity(5)[s:s + 1], target_vec.reshape(-1, 2), epochs=1, verbose=0)\n",
    "            s = new_s\n",
    "            r_sum += r\n",
    "        r_avg_list.append(r_sum / 1000)\n",
    "    return r_avg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rList = eps_greedy_deep_q_learning(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this training over 1000 game episodes reveals the following average reward for each step in the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be observed, the average reward per step in the game increases over each game episode. We also get an output of the Q values for each of the states - we can see that the Q values for each state will favor choosing action 0 (moving forward) to shoot for those big, repeated rewards in state 4. Intuitively, this seems like the best strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
