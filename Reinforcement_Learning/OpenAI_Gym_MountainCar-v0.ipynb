{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving OpenAI Gym MountainCar-v0 problem\n",
    "\n",
    "In this reinforcement learning notebook, the deep Q network that will be created will be trained on the Mountain Car environment/game. This can be accessed through the open source reinforcement learning library called Open AI Gym.\n",
    "\n",
    "The object of this game is to get the car to go up the right-side hill to get to the flag. There’s one problem however, the car doesn’t have enough power to motor all the way up the hill. Instead, the car / agent needs to learn that it must motor up one hill for a bit, then accelerate down the hill and back up the other side, and repeat until it builds up enough momentum to make it to the top of the hill.\n",
    "\n",
    "\n",
    "## Q-Learning: OpenAI gym Mountain Car\n",
    "\n",
    "Example using OpenAI gym Mountain Car enviornment.\n",
    "\n",
    "#### Description\n",
    "\n",
    "Get an under powered car to the top of a hill (top = 0.5 position)\n",
    "\n",
    "#### Observation\n",
    "\n",
    "Num | Observation  | Min  | Max  \n",
    "----|--------------|------|----   \n",
    "0   | position     | -1.2 | 0.6\n",
    "1   | velocity     | -0.07| 0.07\n",
    "\n",
    "#### Actions\n",
    "\n",
    "Num | Action|\n",
    "----|-------------|\n",
    "0   | push left   |\n",
    "1   | no push     |\n",
    "2   | push right  |\n",
    "\n",
    "\n",
    "\n",
    "#### Reward\n",
    "-1 for each time step, until the goal position of 0.5 is reached. There is no penalty for climbing the left hill, which upon reached acts as a wall.\n",
    "\n",
    "#### Episode Termination\n",
    "The episode ends when you reach 0.5 position, or if 200 iterations are reached.\n",
    "\n",
    "**Source**:\n",
    "    - https://github.com/llSourcell/Q_Learning_Explained/blob/master/q_learning.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "import tensorflow as tf\n",
    "import random \n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 40\n",
    "iter_max = 10000\n",
    "\n",
    "initial_lr = 1.0 # Learning rate\n",
    "min_lr = 0.003\n",
    "gamma = 1.0\n",
    "t_max = 10000\n",
    "eps = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, policy=None, render=False):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    for _ in range(t_max):\n",
    "        if render:\n",
    "            env.render()\n",
    "        if policy is None:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            a,b = obs_to_state(env, obs)\n",
    "            action = policy[a][b]\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += gamma ** step_idx * reward\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obs_to_state(env, obs):\n",
    "    \"\"\" Maps an observation to state \"\"\"\n",
    "    env_low = env.observation_space.low\n",
    "    env_high = env.observation_space.high\n",
    "    env_dx = (env_high - env_low) / n_states\n",
    "    a = int((obs[0] - env_low[0])/env_dx[0])\n",
    "    b = int((obs[1] - env_low[1])/env_dx[1])\n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'MountainCar-v0'\n",
    "env = gym.make(env_name)\n",
    "env.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "----- using Q Learning -----\n",
      "Iteration #1 -- Total reward = -200.\n",
      "Iteration #101 -- Total reward = -200.\n",
      "Iteration #201 -- Total reward = -200.\n",
      "Iteration #301 -- Total reward = -200.\n",
      "Iteration #401 -- Total reward = -200.\n",
      "Iteration #501 -- Total reward = -200.\n",
      "Iteration #601 -- Total reward = -200.\n",
      "Iteration #701 -- Total reward = -200.\n",
      "Iteration #801 -- Total reward = -200.\n",
      "Iteration #901 -- Total reward = -200.\n",
      "Iteration #1001 -- Total reward = -200.\n",
      "Iteration #1101 -- Total reward = -200.\n",
      "Iteration #1201 -- Total reward = -200.\n",
      "Iteration #1301 -- Total reward = -200.\n",
      "Iteration #1401 -- Total reward = -200.\n",
      "Iteration #1501 -- Total reward = -200.\n",
      "Iteration #1601 -- Total reward = -200.\n",
      "Iteration #1701 -- Total reward = -200.\n",
      "Iteration #1801 -- Total reward = -200.\n",
      "Iteration #1901 -- Total reward = -200.\n",
      "Iteration #2001 -- Total reward = -200.\n",
      "Iteration #2101 -- Total reward = -200.\n",
      "Iteration #2201 -- Total reward = -200.\n",
      "Iteration #2301 -- Total reward = -200.\n",
      "Iteration #2401 -- Total reward = -200.\n",
      "Iteration #2501 -- Total reward = -200.\n",
      "Iteration #2601 -- Total reward = -200.\n",
      "Iteration #2701 -- Total reward = -200.\n",
      "Iteration #2801 -- Total reward = -200.\n",
      "Iteration #2901 -- Total reward = -200.\n",
      "Iteration #3001 -- Total reward = -200.\n",
      "Iteration #3101 -- Total reward = -200.\n",
      "Iteration #3201 -- Total reward = -200.\n",
      "Iteration #3301 -- Total reward = -200.\n",
      "Iteration #3401 -- Total reward = -200.\n",
      "Iteration #3501 -- Total reward = -200.\n",
      "Iteration #3601 -- Total reward = -200.\n",
      "Iteration #3701 -- Total reward = -200.\n",
      "Iteration #3801 -- Total reward = -200.\n",
      "Iteration #3901 -- Total reward = -200.\n",
      "Iteration #4001 -- Total reward = -200.\n",
      "Iteration #4101 -- Total reward = -200.\n",
      "Iteration #4201 -- Total reward = -200.\n",
      "Iteration #4301 -- Total reward = -200.\n",
      "Iteration #4401 -- Total reward = -200.\n",
      "Iteration #4501 -- Total reward = -200.\n",
      "Iteration #4601 -- Total reward = -200.\n",
      "Iteration #4701 -- Total reward = -200.\n",
      "Iteration #4801 -- Total reward = -200.\n",
      "Iteration #4901 -- Total reward = -200.\n",
      "Iteration #5001 -- Total reward = -200.\n",
      "Iteration #5101 -- Total reward = -200.\n",
      "Iteration #5201 -- Total reward = -200.\n",
      "Iteration #5301 -- Total reward = -200.\n",
      "Iteration #5401 -- Total reward = -200.\n",
      "Iteration #5501 -- Total reward = -200.\n",
      "Iteration #5601 -- Total reward = -200.\n",
      "Iteration #5701 -- Total reward = -200.\n",
      "Iteration #5801 -- Total reward = -200.\n",
      "Iteration #5901 -- Total reward = -200.\n",
      "Iteration #6001 -- Total reward = -200.\n",
      "Iteration #6101 -- Total reward = -200.\n",
      "Iteration #6201 -- Total reward = -200.\n",
      "Iteration #6301 -- Total reward = -200.\n",
      "Iteration #6401 -- Total reward = -200.\n",
      "Iteration #6501 -- Total reward = -200.\n",
      "Iteration #6601 -- Total reward = -200.\n",
      "Iteration #6701 -- Total reward = -200.\n",
      "Iteration #6801 -- Total reward = -200.\n",
      "Iteration #6901 -- Total reward = -200.\n",
      "Iteration #7001 -- Total reward = -200.\n",
      "Iteration #7101 -- Total reward = -200.\n",
      "Iteration #7201 -- Total reward = -200.\n",
      "Iteration #7301 -- Total reward = -200.\n",
      "Iteration #7401 -- Total reward = -200.\n",
      "Iteration #7501 -- Total reward = -200.\n",
      "Iteration #7601 -- Total reward = -200.\n",
      "Iteration #7701 -- Total reward = -200.\n",
      "Iteration #7801 -- Total reward = -200.\n",
      "Iteration #7901 -- Total reward = -200.\n",
      "Iteration #8001 -- Total reward = -198.\n",
      "Iteration #8101 -- Total reward = -200.\n",
      "Iteration #8201 -- Total reward = -200.\n",
      "Iteration #8301 -- Total reward = -200.\n",
      "Iteration #8401 -- Total reward = -200.\n",
      "Iteration #8501 -- Total reward = -200.\n",
      "Iteration #8601 -- Total reward = -200.\n",
      "Iteration #8701 -- Total reward = -200.\n",
      "Iteration #8801 -- Total reward = -200.\n",
      "Iteration #8901 -- Total reward = -200.\n",
      "Iteration #9001 -- Total reward = -200.\n",
      "Iteration #9101 -- Total reward = -200.\n",
      "Iteration #9201 -- Total reward = -200.\n",
      "Iteration #9301 -- Total reward = -200.\n",
      "Iteration #9401 -- Total reward = -200.\n",
      "Iteration #9501 -- Total reward = -200.\n",
      "Iteration #9601 -- Total reward = -200.\n",
      "Iteration #9701 -- Total reward = -200.\n",
      "Iteration #9801 -- Total reward = -200.\n",
      "Iteration #9901 -- Total reward = -200.\n",
      "Average score of solution =  -129.96\n"
     ]
    }
   ],
   "source": [
    "print ('----- using Q Learning -----')\n",
    "q_table = np.zeros((n_states, n_states, 3))\n",
    "for i in range(iter_max):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    ## eta: learning rate is decreased at each step\n",
    "    eta = max(min_lr, initial_lr * (0.85 ** (i//100)))\n",
    "    for j in range(t_max):\n",
    "        a, b = obs_to_state(env, obs)\n",
    "        if np.random.uniform(0, 1) < eps:\n",
    "            action = np.random.choice(env.action_space.n)\n",
    "        else:\n",
    "            logits = q_table[a][b]\n",
    "            logits_exp = np.exp(logits)\n",
    "            probs = logits_exp / np.sum(logits_exp)\n",
    "            action = np.random.choice(env.action_space.n, p=probs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        # update q table\n",
    "        a_, b_ = obs_to_state(env, obs)\n",
    "        q_table[a][b][action] = q_table[a][b][action] + eta * (reward + gamma *  np.max(q_table[a_][b_]) - q_table[a][b][action])\n",
    "        if done:\n",
    "            break\n",
    "    if i % 100 == 0:\n",
    "        print('Iteration #%d -- Total reward = %d.' %(i+1, total_reward))\n",
    "solution_policy = np.argmax(q_table, axis=2)\n",
    "solution_policy_scores = [run_episode(env, solution_policy, False) for _ in range(100)]\n",
    "print(\"Average score of solution = \", np.mean(solution_policy_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-125.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Animate it\n",
    "run_episode(env, solution_policy, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6  0.07]\n",
      "[-1.2  -0.07]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.high) # use to discritize space later\n",
    "print(env.observation_space.low)\n",
    "print(env.action_space.n) # number of possible action in Gym environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will discretize the observation space (i.e. position, velocity) in order to be able to create a Q-table with reasonable size.\n",
    "\n",
    "We need to convert continue states into discrete states. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09  0.007]\n"
     ]
    }
   ],
   "source": [
    "# Discrete Observation Space (OS = observation space)\n",
    "bins = 20\n",
    "DISCRETE_OS_SIZE = [bins]*len(env.observation_space.high) # to be functional in every environment\n",
    "discrete_os_win_size = (env.observation_space.high - env.observation_space.low) / DISCRETE_OS_SIZE\n",
    "# window size (win)\n",
    "print(discrete_os_win_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 20, 3)\n"
     ]
    }
   ],
   "source": [
    "# 20 x 20 x 3 (3dim table) \n",
    "# every possible combination of possible evironment and observation times 3 actions we can take\n",
    "q_table = np.random.uniform(low=-2, high=0, \n",
    "                            size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
    "print(q_table.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Learning settings\n",
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT = 0.95\n",
    "EPISODES = 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discrete_state(state):\n",
    "    discrete_state= (state - env.observation_space.low) / discrete_os_win_size\n",
    "    return tuple(discrete_state.astype(np.int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 10)\n"
     ]
    }
   ],
   "source": [
    "discrete_state = get_discrete_state(env.reset()) # reset returns us just the initial state\n",
    "print(discrete_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.79074422, -1.30304802, -0.51980495])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out the arbitrary random starting values\n",
    "q_table[discrete_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we want to go off the maximum one we can do\n",
    "np.argmax(q_table[discrete_state]) # action 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy approach\n",
    "\n",
    "We make use of only max fucntion to extract max-Q value, resulting in only exploitation. Our model is greedy and exploiting for max Q values always"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EPISODES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d91af892aeda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mSHOW_EVERY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPISODES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mSHOW_EVERY\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EPISODES' is not defined"
     ]
    }
   ],
   "source": [
    "SHOW_EVERY = 1000\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    \n",
    "    if episode % SHOW_EVERY == 0:\n",
    "        render = True\n",
    "        print(episode)\n",
    "    else:\n",
    "        render = False\n",
    "        \n",
    "     # reset returns us just the initial state\n",
    "    discrete_state = get_discrete_state(env.reset())\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done: # discrete_state --> tuple\n",
    "        action = np.argmax(q_table[discrete_state])\n",
    "    \n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "        # we will use this in our formulation of q-values\n",
    "        new_discrete_state = get_discrete_state(new_state)\n",
    "    \n",
    "        if episode % SHOW_EVERY == 0:\n",
    "            env.render()\n",
    "        \n",
    "        # If simulation did not end yet after last step - update Q table\n",
    "        if not done:\n",
    "            # Maximum possible Q value in next step (for new state)\n",
    "            max_future_q = np.max(q_table[new_discrete_state])\n",
    "            # Current Q value (for current state and performed action)\n",
    "            curret_q = q_table[discrete_state+(action, )]\n",
    "    \n",
    "            # And here's our equation for a new Q value for current state and action\n",
    "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "            # Update Q table with new Q value (Q-formula) \n",
    "            q_table[discrete_state + (action, )] = new_q\n",
    "        \n",
    "        # Simulation ended (for any reson) - if goal position is achived - update Q value with reward directly\n",
    "        elif new_state[0] > env.goal_position:\n",
    "            #q_table[discrete_state + (action,)] = reward\n",
    "            q_table[discrete_state + (action, )] = 0\n",
    "            print(f\"We made it on episode {episode}\")\n",
    "        discrete_state = new_discrete_state\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon-greedy approach\n",
    "\n",
    "By introducing and epsilon-greedy approach we can take advantage of doing exploration during the episode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration settings\n",
    "epsilon = 0.5  # not a constant, qoing to be decayed\n",
    "START_EPSILON_DECAYING = 1\n",
    "END_EPSILON_DECAYING = EPSILON // 2 # divide out to an integer\n",
    "epsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to decay the epsilon value every episode until we're done decaying it. We'll do this at the end of each episode. Now we just need to use epsilon. We'll use np.random.random() to randomly pick a number 0 to 1. If np.random.random() is greater than the epsilon value, then we'll go based off the max q value as usual. Otherwise, we will just move randomly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objective is to get the cart to the flag.\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    discrete_state = get_discrete_state(env.reset())\n",
    "    done = False\n",
    "\n",
    "    if episode % SHOW_EVERY == 0:\n",
    "        render = True\n",
    "        print(episode)\n",
    "    else:\n",
    "        render = False\n",
    "\n",
    "    while not done:\n",
    "        # Check whether we want to exploit or explore state/action space\n",
    "        if np.random.random() > epsilon:\n",
    "            # Get action from Q table\n",
    "            action = np.argmax(q_table[discrete_state])\n",
    "        else:\n",
    "            # Get random action (set/take a random action)\n",
    "            action = np.random.randint(0, env.action_space.n)\n",
    "\n",
    "\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        new_discrete_state = get_discrete_state(new_state)\n",
    "\n",
    "        if episode % SHOW_EVERY == 0:\n",
    "            env.render()\n",
    "        #new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "\n",
    "        # If simulation did not end yet after last step - update Q table\n",
    "        if not done:\n",
    "\n",
    "            # Maximum possible Q value in next step (for new state)\n",
    "            max_future_q = np.max(q_table[new_discrete_state])\n",
    "\n",
    "            # Current Q value (for current state and performed action)\n",
    "            current_q = q_table[discrete_state + (action,)]\n",
    "\n",
    "            # And here's our equation for a new Q value for current state and action\n",
    "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "\n",
    "            # Update Q table with new Q value\n",
    "            q_table[discrete_state + (action,)] = new_q\n",
    "\n",
    "\n",
    "        # Simulation ended (for any reson) - if goal position is achived - update Q value with reward directly\n",
    "        elif new_state[0] >= env.goal_position:\n",
    "            #q_table[discrete_state + (action,)] = reward\n",
    "            q_table[discrete_state + (action,)] = 0\n",
    "\n",
    "        discrete_state = new_discrete_state\n",
    "\n",
    "    # Decaying is being done every episode if episode number is within decaying range\n",
    "    if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n",
    "        epsilon -= epsilon_decay_value\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pythonprogramming.net/q-learning-algorithm-reinforcement-learning-python-tutorial/\n",
    "\n",
    "### Boltzmann Exploration Policy\n",
    "\n",
    "https://github.com/lukedottec/QLearnGrid/blob/master/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objective is to get the cart to the flag.\n",
    "temperature = 10 #[2000, 1000, 100, 10, 1]\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    discrete_state = get_discrete_state(env.reset())\n",
    "    done = False\n",
    "\n",
    "    if episode % SHOW_EVERY == 0:\n",
    "        render = True\n",
    "        print(episode)\n",
    "    else:\n",
    "        render = False\n",
    "\n",
    "    while not done:\n",
    "        moves = q_table[discrete_state]\n",
    "        \n",
    "        # Check whether we want to exploit or explore state/action space\n",
    "         # Circumvent math issues with temperature actually being 0\n",
    "        if temperature > 0:\n",
    "            # Compute action probabilities using temperature; when\n",
    "            # temperature is high, we're treating values of very different\n",
    "            # Q-values as more equally choosable\n",
    "            \n",
    "            action_probs_numes = []\n",
    "            denom = 0\n",
    "            for m in moves:\n",
    "                val = math.exp(m / temperature)\n",
    "                action_probs_numes.append(val)\n",
    "                denom += val\n",
    "            action_probs = [x / denom for x in action_probs_numes]\n",
    "\n",
    "            # Pick random move, in which moves with higher probability are\n",
    "            # more likely to be chosen, but it is obviously not guaranteed\n",
    "            rand_val = random.uniform(0, 1)\n",
    "            prob_sum = 0\n",
    "            \n",
    "            for i, prob in enumerate(action_probs):\n",
    "                prob_sum += prob\n",
    "                if rand_val <= prob_sum:\n",
    "                    picked_move = i\n",
    "                    break\n",
    "        else:\n",
    "            # Here, we're totally cold; meaning, we're just exploitin\n",
    "            #picked_move, picked_move_q = get_best_action(orig_state)\n",
    "            new_discrete_state = get_discrete_state(new_state)\n",
    "\n",
    "        \n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        #new_discrete_state = get_discrete_state(new_state)\n",
    "\n",
    "        if episode % SHOW_EVERY == 0:\n",
    "            env.render()\n",
    "        #new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "\n",
    "        # If simulation did not end yet after last step - update Q table\n",
    "        if not done:\n",
    "\n",
    "            # Maximum possible Q value in next step (for new state)\n",
    "            max_future_q = np.max(q_table[new_discrete_state])\n",
    "\n",
    "            # Current Q value (for current state and performed action)\n",
    "            current_q = q_table[discrete_state + (action,)]\n",
    "\n",
    "            # And here's our equation for a new Q value for current state and action\n",
    "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "\n",
    "            # Update Q table with new Q value\n",
    "            q_table[discrete_state + (action,)] = new_q\n",
    "\n",
    "\n",
    "        # Simulation ended (for any reson) - if goal position is achived - update Q value with reward directly\n",
    "        elif new_state[0] >= env.goal_position:\n",
    "            #q_table[discrete_state + (action,)] = reward\n",
    "            q_table[discrete_state + (action,)] = 0\n",
    "\n",
    "        discrete_state = new_discrete_state\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Learning Network (DQN) approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "import os # for creating directories\n",
    "\n",
    "env.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise environment to version 0 of Cart Pole problem\n",
    "env = gym.make('CartPole-v0') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Obtain state and action size\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "print(state_size)\n",
    "print(action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "batch_size = 32\n",
    "\n",
    "n_episodes = 200 # n games we want agent to play (default 1001)\n",
    "output_dir = 'model_output/cartpole/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DQN agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000) # double-ended queue; acts like list, but elements can be added/removed from either end\n",
    "        self.gamma = 0.95 # decay or discount rate: enables agent to take into account future actions in addition to the immediate ones, but discounted at this rate\n",
    "        self.epsilon = 1.0 # exploration rate: how much to act randomly; more initially than later due to epsilon decay\n",
    "        self.epsilon_decay = 0.995 # decrease number of random explorations as the agent's performance (hopefully) improves over time\n",
    "        self.epsilon_min = 0.01 # minimum amount of random exploration permitted\n",
    "        self.learning_rate = 0.001 # rate at which NN adjusts models parameters via SGD to reduce cost \n",
    "        self.model = self.build_model() # private method \n",
    "    \n",
    "    def build_model(self):\n",
    "        # neural net to approximate Q-value function:\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=24, input_dim=self.state_size, activation='relu')) # 1st hidden layer; states as input\n",
    "        model.add(Dense(units=24, activation='relu')) # 2nd hidden layer\n",
    "        model.add(Dense(units=self.action_size, activation='linear')) # 2 actions, so 2 output neurons: 0 and 1 (L/R)\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        # list of previous experiences, enabling re-training later\n",
    "        self.memory.append((state, action, reward, next_state, done)) \n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon: # if acting randomly, take random action\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        # if not acting randomly, predict reward value based on current state\n",
    "        act_values = self.model.predict(state) \n",
    "         # pick the action that will give the highest reward (i.e., go left or right?)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self, batch_size): # method that trains NN with experiences sampled from memory\n",
    "        minibatch = random.sample(self.memory, batch_size) # sample a minibatch from memory\n",
    "        \n",
    "        for state, action, reward, next_state, done in minibatch: # extract data for each minibatch sample\n",
    "            target = reward # if done (boolean whether game ended or not, i.e., whether final state or not), then target = reward\n",
    "            \n",
    "            if not done: # if not done, then predict future discounted reward\n",
    "                target = (reward + self.gamma * # (target) = reward + (discount rate gamma) * \n",
    "                          np.amax(self.model.predict(next_state)[0])) # (maximum target Q based on future action a')\n",
    "                \n",
    "            target_f = self.model.predict(state) # approximately map current state to future discounted reward\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0) # single epoch of training with x=state, y=target_f; fit decreases loss between target_f and y_hat\n",
    "            \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact with environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise agent\n",
    "agent = DQNAgent(state_size, action_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/200, score: 97, e: 0.37\n",
      "episode: 1/200, score: 136, e: 0.37\n",
      "episode: 2/200, score: 199, e: 0.37\n",
      "episode: 3/200, score: 184, e: 0.36\n",
      "episode: 4/200, score: 33, e: 0.36\n",
      "episode: 5/200, score: 86, e: 0.36\n",
      "episode: 6/200, score: 56, e: 0.36\n",
      "episode: 7/200, score: 113, e: 0.36\n",
      "episode: 8/200, score: 62, e: 0.35\n",
      "episode: 9/200, score: 174, e: 0.35\n",
      "episode: 10/200, score: 137, e: 0.35\n",
      "episode: 11/200, score: 113, e: 0.35\n",
      "episode: 12/200, score: 70, e: 0.35\n",
      "episode: 13/200, score: 59, e: 0.35\n",
      "episode: 14/200, score: 42, e: 0.34\n",
      "episode: 15/200, score: 94, e: 0.34\n",
      "episode: 16/200, score: 199, e: 0.34\n",
      "episode: 17/200, score: 29, e: 0.34\n",
      "episode: 18/200, score: 111, e: 0.34\n",
      "episode: 19/200, score: 155, e: 0.34\n",
      "episode: 20/200, score: 143, e: 0.33\n",
      "episode: 21/200, score: 199, e: 0.33\n",
      "episode: 22/200, score: 56, e: 0.33\n",
      "episode: 23/200, score: 93, e: 0.33\n",
      "episode: 24/200, score: 90, e: 0.33\n",
      "episode: 25/200, score: 160, e: 0.33\n",
      "episode: 26/200, score: 49, e: 0.32\n",
      "episode: 27/200, score: 162, e: 0.32\n",
      "episode: 28/200, score: 189, e: 0.32\n",
      "episode: 29/200, score: 169, e: 0.32\n",
      "episode: 30/200, score: 105, e: 0.32\n",
      "episode: 31/200, score: 196, e: 0.32\n",
      "episode: 32/200, score: 155, e: 0.31\n",
      "episode: 33/200, score: 44, e: 0.31\n",
      "episode: 34/200, score: 62, e: 0.31\n",
      "episode: 35/200, score: 77, e: 0.31\n",
      "episode: 36/200, score: 65, e: 0.31\n",
      "episode: 37/200, score: 89, e: 0.31\n",
      "episode: 38/200, score: 72, e: 0.3\n",
      "episode: 39/200, score: 48, e: 0.3\n",
      "episode: 40/200, score: 59, e: 0.3\n",
      "episode: 41/200, score: 133, e: 0.3\n",
      "episode: 42/200, score: 199, e: 0.3\n",
      "episode: 43/200, score: 118, e: 0.3\n",
      "episode: 44/200, score: 158, e: 0.3\n",
      "episode: 45/200, score: 147, e: 0.29\n",
      "episode: 46/200, score: 134, e: 0.29\n",
      "episode: 47/200, score: 129, e: 0.29\n",
      "episode: 48/200, score: 199, e: 0.29\n",
      "episode: 49/200, score: 119, e: 0.29\n",
      "episode: 50/200, score: 199, e: 0.29\n",
      "episode: 51/200, score: 136, e: 0.29\n",
      "episode: 52/200, score: 105, e: 0.28\n",
      "episode: 53/200, score: 142, e: 0.28\n",
      "episode: 54/200, score: 96, e: 0.28\n",
      "episode: 55/200, score: 70, e: 0.28\n",
      "episode: 56/200, score: 102, e: 0.28\n",
      "episode: 57/200, score: 62, e: 0.28\n",
      "episode: 58/200, score: 66, e: 0.28\n",
      "episode: 59/200, score: 89, e: 0.27\n",
      "episode: 60/200, score: 152, e: 0.27\n",
      "episode: 61/200, score: 56, e: 0.27\n",
      "episode: 62/200, score: 61, e: 0.27\n",
      "episode: 63/200, score: 118, e: 0.27\n",
      "episode: 64/200, score: 155, e: 0.27\n",
      "episode: 65/200, score: 199, e: 0.27\n",
      "episode: 66/200, score: 149, e: 0.26\n",
      "episode: 67/200, score: 92, e: 0.26\n",
      "episode: 68/200, score: 96, e: 0.26\n",
      "episode: 69/200, score: 105, e: 0.26\n",
      "episode: 70/200, score: 188, e: 0.26\n",
      "episode: 71/200, score: 199, e: 0.26\n",
      "episode: 72/200, score: 143, e: 0.26\n",
      "episode: 73/200, score: 106, e: 0.26\n",
      "episode: 74/200, score: 188, e: 0.25\n",
      "episode: 75/200, score: 110, e: 0.25\n",
      "episode: 76/200, score: 101, e: 0.25\n",
      "episode: 77/200, score: 199, e: 0.25\n",
      "episode: 78/200, score: 111, e: 0.25\n",
      "episode: 79/200, score: 199, e: 0.25\n",
      "episode: 80/200, score: 191, e: 0.25\n",
      "episode: 81/200, score: 35, e: 0.25\n",
      "episode: 82/200, score: 199, e: 0.24\n",
      "episode: 83/200, score: 199, e: 0.24\n",
      "episode: 84/200, score: 142, e: 0.24\n",
      "episode: 85/200, score: 119, e: 0.24\n",
      "episode: 86/200, score: 199, e: 0.24\n",
      "episode: 87/200, score: 199, e: 0.24\n",
      "episode: 88/200, score: 199, e: 0.24\n",
      "episode: 89/200, score: 97, e: 0.24\n",
      "episode: 90/200, score: 128, e: 0.23\n",
      "episode: 91/200, score: 185, e: 0.23\n",
      "episode: 92/200, score: 199, e: 0.23\n",
      "episode: 93/200, score: 199, e: 0.23\n",
      "episode: 94/200, score: 199, e: 0.23\n",
      "episode: 95/200, score: 199, e: 0.23\n",
      "episode: 96/200, score: 166, e: 0.23\n",
      "episode: 97/200, score: 199, e: 0.23\n",
      "episode: 98/200, score: 66, e: 0.23\n",
      "episode: 99/200, score: 145, e: 0.22\n",
      "episode: 100/200, score: 177, e: 0.22\n",
      "episode: 101/200, score: 199, e: 0.22\n",
      "episode: 102/200, score: 179, e: 0.22\n",
      "episode: 103/200, score: 199, e: 0.22\n",
      "episode: 104/200, score: 125, e: 0.22\n",
      "episode: 105/200, score: 110, e: 0.22\n",
      "episode: 106/200, score: 114, e: 0.22\n",
      "episode: 107/200, score: 199, e: 0.22\n",
      "episode: 108/200, score: 199, e: 0.21\n",
      "episode: 109/200, score: 152, e: 0.21\n",
      "episode: 110/200, score: 185, e: 0.21\n",
      "episode: 111/200, score: 194, e: 0.21\n",
      "episode: 112/200, score: 199, e: 0.21\n",
      "episode: 113/200, score: 199, e: 0.21\n",
      "episode: 114/200, score: 145, e: 0.21\n",
      "episode: 115/200, score: 122, e: 0.21\n",
      "episode: 116/200, score: 199, e: 0.21\n",
      "episode: 117/200, score: 106, e: 0.21\n",
      "episode: 118/200, score: 129, e: 0.2\n",
      "episode: 119/200, score: 138, e: 0.2\n",
      "episode: 120/200, score: 43, e: 0.2\n",
      "episode: 121/200, score: 101, e: 0.2\n",
      "episode: 122/200, score: 97, e: 0.2\n",
      "episode: 123/200, score: 106, e: 0.2\n",
      "episode: 124/200, score: 185, e: 0.2\n",
      "episode: 125/200, score: 199, e: 0.2\n",
      "episode: 126/200, score: 151, e: 0.2\n",
      "episode: 127/200, score: 169, e: 0.2\n",
      "episode: 128/200, score: 141, e: 0.19\n",
      "episode: 129/200, score: 199, e: 0.19\n",
      "episode: 130/200, score: 169, e: 0.19\n",
      "episode: 131/200, score: 169, e: 0.19\n",
      "episode: 132/200, score: 199, e: 0.19\n",
      "episode: 133/200, score: 90, e: 0.19\n",
      "episode: 134/200, score: 51, e: 0.19\n",
      "episode: 135/200, score: 199, e: 0.19\n",
      "episode: 136/200, score: 199, e: 0.19\n",
      "episode: 137/200, score: 175, e: 0.19\n",
      "episode: 138/200, score: 168, e: 0.18\n",
      "episode: 139/200, score: 148, e: 0.18\n",
      "episode: 140/200, score: 72, e: 0.18\n",
      "episode: 141/200, score: 94, e: 0.18\n",
      "episode: 142/200, score: 138, e: 0.18\n",
      "episode: 143/200, score: 117, e: 0.18\n",
      "episode: 144/200, score: 76, e: 0.18\n",
      "episode: 145/200, score: 81, e: 0.18\n",
      "episode: 146/200, score: 112, e: 0.18\n",
      "episode: 147/200, score: 95, e: 0.18\n",
      "episode: 148/200, score: 199, e: 0.18\n",
      "episode: 149/200, score: 199, e: 0.17\n",
      "episode: 150/200, score: 158, e: 0.17\n",
      "episode: 151/200, score: 171, e: 0.17\n",
      "episode: 152/200, score: 184, e: 0.17\n",
      "episode: 153/200, score: 41, e: 0.17\n",
      "episode: 154/200, score: 71, e: 0.17\n",
      "episode: 155/200, score: 134, e: 0.17\n",
      "episode: 156/200, score: 71, e: 0.17\n",
      "episode: 157/200, score: 199, e: 0.17\n",
      "episode: 158/200, score: 199, e: 0.17\n",
      "episode: 159/200, score: 77, e: 0.17\n",
      "episode: 160/200, score: 79, e: 0.17\n",
      "episode: 161/200, score: 112, e: 0.16\n",
      "episode: 162/200, score: 85, e: 0.16\n",
      "episode: 163/200, score: 104, e: 0.16\n",
      "episode: 164/200, score: 113, e: 0.16\n",
      "episode: 165/200, score: 199, e: 0.16\n",
      "episode: 166/200, score: 199, e: 0.16\n",
      "episode: 167/200, score: 137, e: 0.16\n",
      "episode: 168/200, score: 173, e: 0.16\n",
      "episode: 169/200, score: 156, e: 0.16\n",
      "episode: 170/200, score: 199, e: 0.16\n",
      "episode: 171/200, score: 76, e: 0.16\n",
      "episode: 172/200, score: 199, e: 0.16\n",
      "episode: 173/200, score: 110, e: 0.15\n",
      "episode: 174/200, score: 55, e: 0.15\n",
      "episode: 175/200, score: 199, e: 0.15\n",
      "episode: 176/200, score: 199, e: 0.15\n",
      "episode: 177/200, score: 199, e: 0.15\n",
      "episode: 178/200, score: 199, e: 0.15\n",
      "episode: 179/200, score: 160, e: 0.15\n",
      "episode: 180/200, score: 174, e: 0.15\n",
      "episode: 181/200, score: 164, e: 0.15\n",
      "episode: 182/200, score: 108, e: 0.15\n",
      "episode: 183/200, score: 181, e: 0.15\n",
      "episode: 184/200, score: 199, e: 0.15\n",
      "episode: 185/200, score: 178, e: 0.15\n",
      "episode: 186/200, score: 143, e: 0.15\n",
      "episode: 187/200, score: 73, e: 0.14\n",
      "episode: 188/200, score: 199, e: 0.14\n",
      "episode: 189/200, score: 199, e: 0.14\n",
      "episode: 190/200, score: 199, e: 0.14\n",
      "episode: 191/200, score: 199, e: 0.14\n",
      "episode: 192/200, score: 199, e: 0.14\n",
      "episode: 193/200, score: 89, e: 0.14\n",
      "episode: 194/200, score: 113, e: 0.14\n",
      "episode: 195/200, score: 116, e: 0.14\n",
      "episode: 196/200, score: 117, e: 0.14\n",
      "episode: 197/200, score: 88, e: 0.14\n",
      "episode: 198/200, score: 107, e: 0.14\n",
      "episode: 199/200, score: 178, e: 0.14\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "for e in range(n_episodes): # iterate over new episodes of the game\n",
    "    state = env.reset() # reset state at start of each new episode of the game\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    \n",
    "    for time in range(5000):  # time represents a frame of the game; goal is to keep pole upright as long as possible up to range, e.g., 500 or 5000 timesteps\n",
    "        env.render() # comment out for faster training\n",
    "        \n",
    "        action = agent.act(state) # action is either 0 or 1 (move cart left or right); decide on one or other here\n",
    "        next_state, reward, done, _ = env.step(action) # agent interacts with env, gets feedback; 4 state data points, e.g., pole angle, cart position        \n",
    "        reward = reward if not done else -10 # reward +1 for each additional frame with pole upright        \n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        \n",
    "        agent.remember(state, action, reward, next_state, done) # remember the previous timestep's state, actions, reward, etc.        \n",
    "        \n",
    "        state = next_state # set \"current state\" for upcoming iteration to the current next state        \n",
    "        \n",
    "        if done: # episode ends if agent drops pole or we reach timestep 5000\n",
    "            print(\"episode: {}/{}, score: {}, e: {:.2}\" # print the episode's score and agent's epsilon\n",
    "                  .format(e, n_episodes, time, agent.epsilon))\n",
    "            break # exit loop\n",
    "            \n",
    "    if len(agent.memory) > batch_size:\n",
    "        agent.replay(batch_size) # train the agent by replaying the experiences of the episode\n",
    "    if e % 50 == 0:\n",
    "        agent.save(output_dir + \"weights_\" + '{:04d}'.format(e) + \".hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saved agents can be loaded with agent.load(\"./path/filename.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric to track over time\n",
    "ep_rewards = []\n",
    "aggr_ep_rewards = {'ep': [], 'avg': [], 'min': [], 'max': []}\n",
    "# Note: 'avg' is the average for a fixed window during every for say 500 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objective is to get the cart to the flag.\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    discrete_state = get_discrete_state(env.reset())\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    if episode % SHOW_EVERY == 0:\n",
    "        render = True\n",
    "        print(episode)\n",
    "    else:\n",
    "        render = False\n",
    "\n",
    "    while not done:\n",
    "        # Check whether we want to exploit or explore state/action space\n",
    "        if np.random.random() > epsilon:\n",
    "            # Get action from Q table\n",
    "            action = np.argmax(q_table[discrete_state])\n",
    "        else:\n",
    "            # Get random action (set/take a random action)\n",
    "            action = np.random.randint(0, env.action_space.n)\n",
    "\n",
    "\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        episode_reward += reward\n",
    "        \n",
    "        new_discrete_state = get_discrete_state(new_state)\n",
    "\n",
    "        if episode % SHOW_EVERY == 0:\n",
    "            env.render()\n",
    "        #new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "\n",
    "        # If simulation did not end yet after last step - update Q table\n",
    "        if not done:\n",
    "\n",
    "            # Maximum possible Q value in next step (for new state)\n",
    "            max_future_q = np.max(q_table[new_discrete_state])\n",
    "\n",
    "            # Current Q value (for current state and performed action)\n",
    "            current_q = q_table[discrete_state + (action,)]\n",
    "\n",
    "            # And here's our equation for a new Q value for current state and action\n",
    "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "\n",
    "            # Update Q table with new Q value\n",
    "            q_table[discrete_state + (action,)] = new_q\n",
    "\n",
    "\n",
    "        # Simulation ended (for any reson) - if goal position is achived - update Q value with reward directly\n",
    "        elif new_state[0] >= env.goal_position:\n",
    "            #q_table[discrete_state + (action,)] = reward\n",
    "            q_table[discrete_state + (action,)] = 0\n",
    "\n",
    "        discrete_state = new_discrete_state\n",
    "\n",
    "    # Decaying is being done every episode if episode number is within decaying range\n",
    "    if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n",
    "        epsilon -= epsilon_decay_value\n",
    "\n",
    "    ep_rewards.append(episode_reward)\n",
    "    \n",
    "    if not episode % SHOW_EVERY:\n",
    "        average_reward = sum(ep_rewards[-SHOW_EVERY:]/len(ep_rewards[-SHOW_EVERY:]))\n",
    "        aggr_ep_rewards['ep'].append(episode)\n",
    "        aggr_ep_rewards['avg'].append(average_reward)\n",
    "        aggr_ep_rewards['min'].append(min(ep_rewards[-SHOW_EVERY:]))\n",
    "        aggr_ep_rewards['max'].append(max(ep_rewards[-SHOW_EVERY:]))\n",
    "        \n",
    "        print(f'Episode: {episode} avg: {average_reward} min: {min(ep_rewards[-SHOW_EVERY:])}\\\n",
    "        max: {max(ep_rewards[-SHOW_EVERY:])}')\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(aggr_ep_rewards['ep'], aggr_ep_rewards['avg'], label='avg')\n",
    "plt.plot(aggr_ep_rewards['ep'], aggr_ep_rewards['min'], label='min')\n",
    "plt.plot(aggr_ep_rewards['ep'], aggr_ep_rewards['max'], label='max')\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameRunner:\n",
    "  def __init__(self, sess, model, env, memory, max_eps, min_eps,\n",
    "                 decay, render=True):\n",
    "        self._sess  = sess\n",
    "        self._env   = env\n",
    "        self._model = model\n",
    "        self._memory  = memory\n",
    "        self._render  = render\n",
    "        self._max_eps = max_eps\n",
    "        self._min_eps = min_eps\n",
    "        self._decay   = decay\n",
    "        self._eps   = self._max_eps\n",
    "        self._steps = 0\n",
    "        self._reward_store = []\n",
    "        self._max_x_store  = []\n",
    "        \n",
    "  def run(self):\n",
    "      state = self._env.reset()\n",
    "      tot_reward = 0\n",
    "      max_x = -100\n",
    "\n",
    "      while True:\n",
    "        #if self._render:      \n",
    "          #self._env.render()\n",
    "          \n",
    "        action = self._choose_action(state)        \n",
    "        next_state, reward, done, info = self._env.step(action)\n",
    "        \n",
    "        if next_state[0] >= 0.1:\n",
    "          reward += 10\n",
    "        elif next_state[0] >= 0.25:\n",
    "          reward += 20\n",
    "        elif next_state[0] >= 0.5:\n",
    "          reward += 100\n",
    "\n",
    "        if next_state[0] > max_x:\n",
    "          max_x = next_state[0]\n",
    "        # is the game complete? If so, set the next state to\n",
    "        # None for storage sake\n",
    "        if done:\n",
    "          next_state = None\n",
    "\n",
    "        self._memory.add_sample((state, action, reward, next_state))\n",
    "        self._replay()\n",
    "\n",
    "        # exponentially decay the eps value\n",
    "        self._steps += 1\n",
    "        self._eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) \\\n",
    "                                * math.exp(-LAMBDA * self._steps)\n",
    "\n",
    "        # move the agent to the next state and accumulate the reward\n",
    "        state = next_state\n",
    "        tot_reward += reward\n",
    "\n",
    "        # if the game is done, break the loop\n",
    "        if done:\n",
    "          self._reward_store.append(tot_reward)\n",
    "          self._max_x_store.append(max_x)\n",
    "          break\n",
    "        \n",
    "      print(\"Step {}, Total reward: {}, Eps: {}\".format(self._steps, tot_reward, self._eps))\n",
    "\n",
    "  def _choose_action(self, state):\n",
    "    if random.random() < self._eps:\n",
    "        return random.randint(0, self._model._num_actions - 1)\n",
    "    else:\n",
    "        return np.argmax(self._model.predict_one(state, self._sess))\n",
    "      \n",
    "  def _replay(self):\n",
    "    GAMMA=0.99 #added\n",
    "    batch = self._memory.sample(self._model._batch_size)\n",
    "    states = np.array([val[0] for val in batch])\n",
    "    next_states = np.array([(np.zeros(self._model._num_states)\n",
    "                             if val[3] is None else val[3]) for val in batch])\n",
    "    # predict Q(s,a) given the batch of states\n",
    "    q_s_a = self._model.predict_batch(states, self._sess)\n",
    "    # predict Q(s',a') - so that we can do gamma * max(Q(s'a')) below\n",
    "    q_s_a_d = self._model.predict_batch(next_states, self._sess)\n",
    "    # setup training arrays\n",
    "    x = np.zeros((len(batch), self._model._num_states))\n",
    "    y = np.zeros((len(batch), self._model._num_actions))\n",
    "    for i, b in enumerate(batch):\n",
    "        state, action, reward, next_state = b[0], b[1], b[2], b[3]\n",
    "        # get the current q values for all actions in state\n",
    "        current_q = q_s_a[i]\n",
    "        # update the q value for action\n",
    "        if next_state is None:\n",
    "            # in this case, the game completed after action, so there is no max Q(s',a')\n",
    "            # prediction possible\n",
    "            current_q[action] = reward\n",
    "        else:\n",
    "            current_q[action] = reward + GAMMA * np.amax(q_s_a_d[i])\n",
    "        x[i] = state\n",
    "        y[i] = current_q\n",
    "    self._model.train_batch(self._sess, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, num_states, num_actions, batch_size):\n",
    "        self._num_states = num_states\n",
    "        self._num_actions = num_actions\n",
    "        self._batch_size = batch_size\n",
    "        # define the placeholders\n",
    "        self._states = None\n",
    "        self._actions = None\n",
    "        # the output operations\n",
    "        self._logits = None\n",
    "        self._optimizer = None\n",
    "        self._var_init = None\n",
    "        # now setup the model\n",
    "        self._define_model()\n",
    "\n",
    "    def _define_model(self):\n",
    "        self._states = tf.placeholder(shape=[None, self._num_states], dtype=tf.float32)\n",
    "        self._q_s_a = tf.placeholder(shape=[None, self._num_actions], dtype=tf.float32)\n",
    "        # create a couple of fully connected hidden layers\n",
    "        fc1 = tf.layers.dense(self._states, 50, activation=tf.nn.relu)\n",
    "        fc2 = tf.layers.dense(fc1, 50, activation=tf.nn.relu)\n",
    "        self._logits = tf.layers.dense(fc2, self._num_actions)\n",
    "        loss = tf.losses.mean_squared_error(self._q_s_a, self._logits)\n",
    "        self._optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "        self._var_init = tf.global_variables_initializer()\n",
    "        \n",
    "    def predict_one(self, state, sess):\n",
    "      return sess.run(self._logits, feed_dict={self._states:\n",
    "                                               state.reshape(1, self._num_states)})\n",
    "\n",
    "    def predict_batch(self, states, sess):\n",
    "      return sess.run(self._logits, feed_dict={self._states: states})\n",
    "\n",
    "    def train_batch(self, sess, x_batch, y_batch):\n",
    "      sess.run(self._optimizer, feed_dict={self._states: x_batch, self._q_s_a: y_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "  def __init__(self, max_memory):\n",
    "    self._max_memory = max_memory\n",
    "    self._samples = []\n",
    "\n",
    "  def add_sample(self, sample):\n",
    "    self._samples.append(sample)\n",
    "    if len(self._samples) > self._max_memory:\n",
    "      self._samples.pop(0)\n",
    "\n",
    "  def sample(self, no_samples):\n",
    "    if no_samples > len(self._samples):\n",
    "      return random.sample(self._samples, len(self._samples))\n",
    "    else:\n",
    "      return random.sample(self._samples, no_samples)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets up the environment and run multiple games to perform the learning\n",
    "env_name = 'MountainCar-v0'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of state and action\n",
    "num_states  = env.env.observation_space.shape[0]\n",
    "num_actions = env.env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build RL model\n",
    "BATCH_SIZE=50\n",
    "MAX_EPSILON=0.9\n",
    "MIN_EPSILON=0.1\n",
    "LAMBDA=0.2\n",
    "\n",
    "model = Model(num_states, num_actions, BATCH_SIZE)\n",
    "mem = Memory(50000)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run(model._var_init)\n",
    "  gr = GameRunner(sess, model, env, mem, \n",
    "                  MAX_EPSILON, MIN_EPSILON, LAMBDA)\n",
    "  num_episodes = 300\n",
    "  cnt = 0\n",
    "  while cnt < num_episodes:\n",
    "    if cnt % 10 == 0:\n",
    "      print('Episode {} of {}'.format(cnt+1, num_episodes))\n",
    "    gr.run()\n",
    "    cnt += 1\n",
    "  plt.plot(gr._reward_store)\n",
    "  plt.show()\n",
    "  plt.close(\"all\")\n",
    "  plt.plot(gr._max_x_store)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code creates a virtual display to draw game images on. \n",
    "# If you are running locally, just ignore it\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(env.render('rgb_array'))\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs0 = env.reset()\n",
    "print(\"initial observation code:\", obs0)\n",
    "\n",
    "# Note: in MountainCar, observation is just two numbers: car position and velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"taking action 2 (right)\")\n",
    "new_obs, reward, is_done, _ = env.step(2)\n",
    "\n",
    "print(\"new observation code:\", new_obs)\n",
    "print(\"reward:\", reward)\n",
    "print(\"is game over?:\", is_done)\n",
    "\n",
    "# Note: as you can see, the car has moved to the riht slightly (around 0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_LIMIT = 250\n",
    "env = gym.wrappers.TimeLimit(gym.envs.classic_control.MountainCarEnv(),\n",
    "                             max_episode_steps=TIME_LIMIT + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset()\n",
    "actions = {'left': 0, 'stop': 1, 'right': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare \"display\"\n",
    "#%matplotlib notebook\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(s, t):\n",
    "    # YOUR CODE HERE\n",
    "    if t>50 and t<100:\n",
    "        return actions['left']\n",
    "    else:\n",
    "        return actions['right']\n",
    "    \n",
    "    #return actions['right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(t):\n",
    "    if t < 50:\n",
    "        return actions['left']\n",
    "    elif t < 100:\n",
    "        return actions['right']\n",
    "    elif t < 150:\n",
    "        return actions['left']\n",
    "    else:\n",
    "        return actions['right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(TIME_LIMIT):\n",
    "    \n",
    "    s, r, done, _ = env.step(policy(s, t))\n",
    "    \n",
    "    #draw game image on display\n",
    "    ax.clear()\n",
    "    ax.imshow(env.render('rgb_array'))\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "    if done:\n",
    "        print(\"Well done!\")\n",
    "        break\n",
    "else:    \n",
    "    print(\"Time limit exceeded. Try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
