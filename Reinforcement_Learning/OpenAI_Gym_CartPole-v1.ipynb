{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI GYM CartPole-v1 problem\n",
    "\n",
    "The description of the CartPole-v1 as given on the OpenAI gym website -\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson.\n",
    "\n",
    "We train an agent to solve OpenAI Gym's Cartpole-v0 environment. The implementation is in the most recent version of the PyTorch frameworkfor building deep learning models.\n",
    "\n",
    "https://medium.com/@thechrisyoon/deriving-policy-gradients-and-implementing-reinforce-f887949bd63\n",
    "\n",
    "- Actor Critic Methods (A2C) - OpenAI Gym CartPole-v0 \n",
    "https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f\n",
    "\n",
    "- Deep Deterministic Policy Gradients - OpenAI Gym  Pendulum-v0\n",
    "\n",
    "https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch as T\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDeepQNetwork(nn.Module):\n",
    "    def __init__(self, lr, n_actions, input_dims):\n",
    "        super(LinearDeepQNetwork, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(*input_dims, 128)\n",
    "        self.fc2 = nn.Linear(128, n_actions)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device) # device selection\n",
    "        \n",
    "    def forward(self, state):\n",
    "        layer1  = F.relu(self.fc1(state))\n",
    "        actions = self.fc2(layer1) # no activation necessary for regression=output\n",
    "        \n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    '''\n",
    "    Agent is capable to choose actions the ability to learn from it's experiences\n",
    "    as well as the ability to decrement the agent's epsilon over time.\n",
    "    '''\n",
    "    def __init__(self, input_dims, n_actions, lr=0.0001, gamma=0.99, epsilon=1.0, eps_dec=1e-5, \n",
    "                eps_min=0.01):\n",
    "        self.lr = lr\n",
    "        self.input_dims = input_dims\n",
    "        self.n_actions  = n_actions\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_dec = eps_dec\n",
    "        self.eps_min = eps_min\n",
    "        self.action_space = [i for i in range(self.n_actions)]\n",
    "        \n",
    "        # Q-value function for the Agent \n",
    "        self.Q = LinearDeepQNetwork(self.lr, self.n_actions, self.input_dims)\n",
    "        \n",
    "    def choose_action(self, observation):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            state = T.tensor(observation, dtype=T.float).to(self.Q.device)\n",
    "            actions = self.Q.forward(state)\n",
    "             # action with maximum Q-value\n",
    "            action = T.argmax(actions).item() # dereference to numpy array for Gym API\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def decrement_epsilon(self): # linear annealing\n",
    "        self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min else self.eps_min\n",
    "    \n",
    "    def learn(self, state, action, reward, state_):\n",
    "        self.Q.optimizer.zero_grad()\n",
    "        states  = T.tensor(state, dtype=T.float).to(self.Q.device)\n",
    "        actions = T.tensor(action).to(self.Q.device)\n",
    "        rewards = T.tensor(reward).to(self.Q.device)\n",
    "        states_ = T.tensor(state_, dtype=T.float).to(self.Q.device)\n",
    "        \n",
    "        # prediction valeus for the current state of the environment\n",
    "        q_pred = self.Q.forward(states)[actions]\n",
    "        \n",
    "        # target value for the maximum action of the agents estimate of the value of the resulting states\n",
    "        q_next = self.Q.forward(states_).max()\n",
    "        \n",
    "        # the target that is the direction we want to move in is going to be\n",
    "        q_target = reward + self.gamma * q_next\n",
    "        \n",
    "        loss = self.Q.loss(q_target, q_pred).to(self.Q.device)\n",
    "        loss.backward()\n",
    "        self.Q.optimizer.step()\n",
    "        self.decrement_epsilon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chrisjcc/.virtualenvs/ml/lib/python3.7/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cartpole random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward = 0.0\n",
    "total_steps = 0\n",
    "\n",
    "obs = env.reset() # random initialization of environment\n",
    "print(obs) # random observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run through episodes\n",
    "while True:\n",
    "    # select random action from action space\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # execute selected action\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    \n",
    "    total_reward += reward\n",
    "    total_steps += 1\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(\"Episode done in %d steps, total reward %.2f\" % (total_steps, total_reward))\n",
    "env.close()\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random actionwrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomActionWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env, epsilon=0.1):\n",
    "        super(RandomActionWrapper, self).__init__(env)\n",
    "        self.epsilon = epsilon\n",
    "    def action(self, action):\n",
    "        if random.random() < self.epsilon:\n",
    "            print(\"Random action taken!\")\n",
    "            return self.env.action_space.sample()\n",
    "        # else original action taken\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_cartpole = gym.make(\"CartPole-v0\")\n",
    "env = RandomActionWrapper(env_cartpole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward = 0.0\n",
    "total_steps = 0\n",
    "\n",
    "obs = env.reset()\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run through episodes\n",
    "while True:\n",
    "    # select predefined action (move left)\n",
    "    action = 0\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    \n",
    "    total_reward += reward\n",
    "    total_steps += 1\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(\"Episode done in %d steps, total reward %.2f\" % (total_steps, total_reward))\n",
    "env.close()\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cartpole random monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "env = gym.wrappers.Monitor(env, \"log_recording\", force=True) # log folder called recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run through episodes\n",
    "while True:\n",
    "    # select random action from action space\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    \n",
    "    total_reward += reward\n",
    "    total_steps += 1\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(\"Episode done in %d steps, total reward %.2f\" % (total_steps, total_reward))\n",
    "env.close()\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole solved with Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default parameter configuration\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70 # percentile of episodes' total rewards that are used for elite episode filtering(top 30% of episodes sorted by reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the observation from environment and action agent completed for one step agent made in the episode\n",
    "# use episode step from elite episodes as training data\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "# single episode stored as total undiscounted reward and a collection of episode\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates batches with episodes\n",
    "def iterate_batches(env, net, batch_size): # count of episodes to generate on every generation. \n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    \n",
    "    obs = env.reset()\n",
    "    \n",
    "    # softmax\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    \n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs])\n",
    "        # pass current obervation to the net, sample the action to perform,\n",
    "        # ask the environment to process the action, and remember the result of this processing.\n",
    "        # use softmax (sm) to convert the network's output to a probability distribution of actions\n",
    "        act_probs_v = sm(net(obs_v))\n",
    "        act_probs = act_probs_v.data.numpy()[0]\n",
    "        # sampling from action probability distribution\n",
    "        action = np.random.choice(len(act_probs), p=act_probs)\n",
    "        \n",
    "        next_obs, reward, is_done, _ = env.step(action)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        \n",
    "        episode_steps.append(EpisodeStep(observation=obs, action=action))\n",
    "        \n",
    "        if is_done:\n",
    "            \n",
    "            batch.append(Episode(reward=episode_reward, steps=episode_steps))\n",
    "\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            \n",
    "            next_obs = env.reset()\n",
    "            \n",
    "            # in case batch reaches desired count of episodes return it to caller\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        # assign observation obtained from environment to the current observation         \n",
    "        obs = next_obs\n",
    "        \n",
    "def filter_batch(batch ,percentile):\n",
    "    '''\n",
    "    function at the core of the cross-entropy method, from the given batch episodes\n",
    "    and percentile value, it calculates a boundary reward, which is used to \n",
    "    filter elite episodes to train on. \n",
    "    \n",
    "    '''\n",
    "    # from list of values and desired percentile \n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    reward_bound = np.percentile(rewards, percentile) # to obtain the boundary reward \n",
    "    reward_mean = float(np.mean(rewards)) # mean reward used for monitor\n",
    "    \n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    \n",
    "    for example in batch: \n",
    "        # filter off episode for training \n",
    "        # for every episode in the batch, we will check that the episode has high total reward \n",
    "        # then our reward boundary and if it has, we will populate list of observations and actions\n",
    "        # that we train on.\n",
    "        if example.reward < reward_bound: \n",
    "            continue\n",
    "        # observation and actions from elite episode  \n",
    "        train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "        train_act.extend(map(lambda step: step.action, example.steps))\n",
    "        \n",
    "    train_obs_v = torch.FloatTensor(train_obs)\n",
    "    train_act_v = torch.LongTensor(train_act)\n",
    "    \n",
    "    # reward boudary and reward mean only used to check and monitor agent performance\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "To check the agent in action we enable Monitor to create videos recorded at different training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_size  = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hidden-layer neural network, with ReLU and 128 hidden neurons\n",
    "net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "objective = nn.CrossEntropyLoss() # expects raw scores (logit) and applies log-softmax on them as opposed to the log probabilities\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(comment=\"-cartpole\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training of the neural network and the generation of the episodes are performed at the same time. They are not completely in parallel, but every time the loop accumulates enough episodes (16), it passes control to the function supposed to train the network using the gradient descent. The network will have different, slightly better behavior, hopefully.\n",
    "\n",
    "We do not need to explore proper synchronization, as the training and data gathering activities are performed at the same thrad of execution, but need to understand those constant jumps from network training to its utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In training loop, we iterate over batches of episodes,\n",
    "# then perform filtering of the elite episodes. \n",
    "# Teh result is variables of observations and taken actions, reward boundary \n",
    "# used for filtering and mean reward.\n",
    "for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "    obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)\n",
    "    optimizer.zero_grad()\n",
    "    # pass observations to the network, obtaining its actions scores.\n",
    "    action_scores_v = net(obs_v)\n",
    "    # These action scores are passed to the objection function, \n",
    "    # which calculates cross-entropy between the network output and the actions that \n",
    "    # the action agent took.\n",
    "    loss_v = objective(action_scores_v, acts_v)\n",
    "    \n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(\"%d: loss=%.3f, reward_mean=%.1f, reward_bound=%.1f\" % (\n",
    "            iter_no, loss_v.item(), reward_m, reward_b))\n",
    "    \n",
    "    # Agent's learning performance recorded\n",
    "    # monitoring progress of interation number, loss, mean reward of batch, and reward boundary\n",
    "    writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "    writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
    "    writer.add_scalar(\"reward_mean\",  reward_m, iter_no)\n",
    "    \n",
    "    # comparison of the mean rewards of the batch episodes solved when the mean reward \n",
    "    # for the last 100 episodes is greater than 195. \n",
    "    # Gym, environment considers\n",
    "    if reward_m > 199:\n",
    "        print(\"Solved!\")\n",
    "        break\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "env.close()\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you see \"<classname> has no attribute .env\", remove .env or update gym\n",
    "env = gym.make(\"CartPole-v0\").env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create agent\n",
    "agent = MLPClassifier(hidden_layer_sizes=(20,20),\n",
    "                      activation='tanh',\n",
    "                      warm_start=True, # keep progress between .fit(...) calls\n",
    "                      max_iter=1 # make only 1 iteration on each .fit(...)\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [env.reset()]*n_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = list(range(n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize agent to the dimension of state an amount of actions\n",
    "#agent.fit([env.reset()]*n_actions, list(range(n_actions)));\n",
    "agent.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \n",
    "    states,actions = [],[]\n",
    "    total_reward = 0\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        # a vector of action probabilities in current state\n",
    "        probs = agent.predict_proba([s])[0] \n",
    "        \n",
    "        #a = <sample action with such probabilities>\n",
    "        a = np.random.choice(2, 1, p=probs)[0]\n",
    "        # Version 2.\n",
    "        #a = get_action(s, epsilon=epsilon) \n",
    "        \n",
    "        new_s, r, done, info = env.step(a)\n",
    "        \n",
    "        # Version 2.\n",
    "        #epsilon=0\n",
    "        #sess.run(train_step,{\n",
    "        #        states_ph: [s], actions_ph: [a], rewards_ph: [r], \n",
    "        #        next_states_ph: [new_s], is_done_ph: [done]\n",
    "        #    })\n",
    "        \n",
    "        #record sessions like you did before\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward += r\n",
    "        \n",
    "        s = new_s\n",
    "        if done: break\n",
    "    return states, actions, total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epsilon = 0.5\n",
    "#for i in range(1000):\n",
    "#    session_rewards = [generate_session(epsilon=epsilon, train=True) for _ in range(100)]\n",
    "#    print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(i, np.mean(session_rewards), epsilon))\n",
    "    \n",
    "#    epsilon *= 0.99\n",
    "#    assert epsilon >= 1e-4, \"Make sure epsilon is always nonzero during training\"\n",
    "    \n",
    "#    if np.mean(session_rewards) > 300:\n",
    "#        print (\"You Win!\")\n",
    "#        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-entropy method (CEM) steps\n",
    "\n",
    "Deep CEM uses exactly the same strategy as the regular CEM. The only difference is that now each observation is not a number but a float32 vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_elites(states_batch,actions_batch,rewards_batch,percentile=50):\n",
    "    \"\"\"\n",
    "    Select states and actions from games that have rewards >= percentile\n",
    "    :param states_batch: list of lists of states, states_batch[session_i][t]\n",
    "    :param actions_batch: list of lists of actions, actions_batch[session_i][t]\n",
    "    :param rewards_batch: list of rewards, rewards_batch[session_i][t]\n",
    "    \n",
    "    :returns: elite_states,elite_actions, both 1D lists of states and respective actions from elite sessions\n",
    "    \n",
    "    Please return elite states and actions in their original order \n",
    "    [i.e. sorted by session number and timestep within session]\n",
    "    \n",
    "    If you're confused, see examples below. Please don't assume that states are integers (they'll get different later).\n",
    "    \"\"\"\n",
    "    \n",
    "    #reward_threshold = <Compute minimum reward for elite sessions. Hint: use np.percentile>\n",
    "    reward_threshold = np.percentile(rewards_batch, percentile)\n",
    "    \n",
    "    \n",
    "    #elite_states  = <your code here>\n",
    "    #elite_actions = <your code here>\n",
    "    # Version 1.\n",
    "    elite_states  = [s for i in range(len(states_batch)) if rewards_batch[i]>=reward_threshold for s in states_batch[i]]    \n",
    "    elite_actions = [a for i in range(len(actions_batch)) if rewards_batch[i]>=reward_threshold for a in actions_batch[i]]\n",
    "\n",
    "    # Version 2.\n",
    "    #elite_states  = [state for i in range(len(rewards_batch))   if rewards_batch[i]> reward_threshold for state in states_batch[i] ]\n",
    "    #elite_actions = [action for i in range(len(rewards_batch))  if rewards_batch[i] > reward_threshold for action in actions_batch[i]]\n",
    "\n",
    "    # Version 3.\n",
    "    #elite_states  = list(chain(*[s for s, _ in zip(states_batch,rewards_batch) if _ >= reward_threshold])) \n",
    "    #elite_actions = list(chain(*[s for s, _ in zip(actions_batch,rewards_batch) if _ >= reward_threshold])) \n",
    "\n",
    "    # Version 4.\n",
    "    #elite_states = []\n",
    "    #elite_actions = []\n",
    "    #for i in range(len(rewards_batch)):\n",
    "    #    reward = rewards_batch[i]\n",
    "    #    if reward>=reward_threshold:\n",
    "    #        for state_element, action_element in zip(states_batch[i], actions_batch[i]):\n",
    "    #            elite_states.append(state_element)\n",
    "    #            elite_actions.append(action_element)\n",
    "    \n",
    "    return elite_states, elite_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training loop\n",
    "\n",
    "Generate sessions, select N best and fit to those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_progress(batch_rewards, log, percentile, reward_range=[-990,+10]):\n",
    "    \"\"\"\n",
    "    A convenience function that displays training progress. \n",
    "    No cool math here, just charts.\n",
    "    \"\"\"\n",
    "    \n",
    "    mean_reward, threshold = np.mean(batch_rewards), np.percentile(batch_rewards, percentile)\n",
    "    log.append([mean_reward, threshold])\n",
    "\n",
    "    clear_output(True)\n",
    "    print(\"mean reward = %.3f, threshold=%.3f\"%(mean_reward, threshold))\n",
    "    plt.figure(figsize=[8,4])\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(list(zip(*log))[0], label='Mean rewards')\n",
    "    plt.plot(list(zip(*log))[1], label='Reward thresholds')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.hist(batch_rewards, range=reward_range);\n",
    "    plt.vlines([np.percentile(batch_rewards, percentile)], [0], [100], label=\"percentile\", color='red')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sessions = 100\n",
    "percentile = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "log = []\n",
    "\n",
    "for i in range(100):\n",
    "    #generate new sessions\n",
    "    #sessions = [<generate a list of n_sessions new sessions>]\n",
    "    sessions = [generate_session() for _ in range(n_sessions)]\n",
    "\n",
    "    batch_states, batch_actions, batch_rewards = map(np.array, zip(*sessions))\n",
    "\n",
    "    #elite_states, elite_actions = <select elite actions just like before>\n",
    "    # Version 1. \n",
    "    elite_states, elite_actions = select_elites(batch_states,\n",
    "                                                batch_actions,\n",
    "                                                batch_rewards,\n",
    "                                                percentile=percentile)\n",
    "    \n",
    "    # Version 2. choose threshold on rewards\n",
    "    #threshold = np.percentile(batch_rewards,percentile)\n",
    "    #elite_states = np.concatenate(batch_states[batch_rewards>=threshold])\n",
    "    #elite_actions = np.concatenate(batch_actions[batch_rewards>=threshold])\n",
    "\n",
    "    #<fit agent to predict elite_actions(y) from elite_states(X)>\n",
    "    agent.fit(elite_states, elite_actions)\n",
    "    \n",
    "    show_progress(batch_rewards, log, percentile, reward_range=[0,np.max(batch_rewards)])\n",
    "    \n",
    "    if np.mean(batch_rewards)> 190:\n",
    "        print(\"You Win! You may stop training now via KeyboardInterrupt.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True)\n",
    "sessions = [generate_session() for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"), os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving CartPole-V1 using REINFORCE Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# MXNET\n",
    "from mxnet import nd, gluon, init, autograd\n",
    "from mxnet.gluon import nn\n",
    "import mxnet as mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE(object):\n",
    "    def __init__(self, env, lr=1e-3, seed=42):\n",
    "        \"\"\"\n",
    "        REINFORCE algorithm implementation.\n",
    "        \n",
    "        Args:\n",
    "            env (Gym environment) : the environment that we are training our reinforcement learning.\n",
    "            lr (float) : the learning rate used for to update the neural network.\n",
    "            seed (int) : the random seed used to generate data from the environment.\n",
    "        \"\"\"\n",
    "\n",
    "        self.env = env\n",
    "        self.lr = lr\n",
    "        self.seed = seed\n",
    "        self.env.seed(self.seed)\n",
    "        \n",
    "        print('Random seed: {} '.format(seed))\n",
    "\n",
    "        self.build_network()\n",
    "        \n",
    "        \n",
    "    def build_network(self, hidden_size=20):\n",
    "        \"\"\"\n",
    "        Build the neural network and set up the trainer.\n",
    "        \n",
    "        Args:\n",
    "            hidden_size (int) : the size of the hidden layers in the neural network.\n",
    "        \"\"\"    \n",
    "        \n",
    "        self.policy_net = nn.Sequential()\n",
    "        self.policy_net.add(nn.Dense(hidden_size, activation=\"relu\"),\n",
    "                            nn.Dense(hidden_size, activation=\"relu\"),\n",
    "                            nn.Dense(self.env.action_space.n))\n",
    "        self.policy_net.initialize(init=init.Xavier())\n",
    "\n",
    "        self.trainer = gluon.Trainer(self.policy_net.collect_params(), 'adam', {'learning_rate': self.lr})\n",
    "\n",
    "        \n",
    "    def update(self, lr_coeff=0.999):\n",
    "        \"\"\"\n",
    "        Perform an update on a batch of data collected during an episode. It will also reduce the learning rate \n",
    "        after the update as a way to improve convergence.\n",
    "        \n",
    "        Args:\n",
    "            lr_coeff (float) : the coefficient with which we multiply the current learning rate.\n",
    "        \"\"\"\n",
    "        \n",
    "        returns    = self.get_returns()\n",
    "        batch_size = len(self.actions)\n",
    "\n",
    "        with autograd.record():\n",
    "            all_actions = nd.softmax(self.policy_net(nd.array(self.states[:-1])))\n",
    "            \n",
    "            loss = - nd.log(all_actions[np.array(range(batch_size)), np.array(self.actions)]) * returns\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        self.trainer.step(batch_size)\n",
    "        self.trainer.set_learning_rate(self.trainer.learning_rate * lr_coeff) \n",
    "      \n",
    "    \n",
    "    def predict(self,  state):\n",
    "        \"\"\"\n",
    "        Output the probabilities for all actions and choose stochastically one of them.\n",
    "        \n",
    "        Args:\n",
    "            state (array of floats) : the state for which we want to select an action.\n",
    "        Returns:\n",
    "            action (int) : the selected action given the state.\n",
    "        \"\"\"\n",
    "        \n",
    "        actions = nd.softmax(self.policy_net(nd.array([state]))).asnumpy()[0]\n",
    "\n",
    "        return np.random.choice(len(actions), p=actions)\n",
    "    \n",
    "    \n",
    "    def get_returns(self, discount_factor=0.99):\n",
    "        \"\"\"\n",
    "        Calculate the return for every state. This is defined as the discounted \n",
    "        sum of rewards after visiting the state. \n",
    "        \n",
    "        Args:\n",
    "            discount_factor (float) : determines how much we care about distant \n",
    "                                        rewards (1.0) vs immediate rewards (0.).\n",
    "        Returns:\n",
    "            normalized_returns (array of float) : the returns, from which the mean is \n",
    "                                                 substracted to reduce the variance.\n",
    "        \"\"\"\n",
    "        returns=[]\n",
    "        curr_sum = 0.\n",
    "        for r in reversed(self.rewards):\n",
    "            curr_sum = r + discount_factor*curr_sum\n",
    "            returns.append(curr_sum)\n",
    "            \n",
    "        returns.reverse()\n",
    "        normalized_returns = nd.array(returns) - nd.mean(nd.array(returns))\n",
    "        \n",
    "        return normalized_returns\n",
    "    \n",
    "    \n",
    "    def setup_saving(self):\n",
    "        \"\"\"\n",
    "        Store results.\n",
    "        \n",
    "        Args:\n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            printout of location of stored file.\n",
    "        \"\"\"\n",
    "        \n",
    "        directory= os.getcwd() + '/res/'\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "            \n",
    "        save_file = \"{}cartpole_seed{}.csv\".format(directory,self.seed)\n",
    "        \n",
    "        return save_file, []   \n",
    "    \n",
    "    \n",
    "    def initialize_episode(self):\n",
    "        \"\"\"\n",
    "        Initialiazes the variables total_rewards, ewards, actions and states, and\n",
    "        resets the environment.\n",
    "        \n",
    "        Returns:\n",
    "            state (array of float) : the first state of the episode.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.rewards,self.actions,self.states = [],[],[]\n",
    "        self.total_rewards = 0.\n",
    "\n",
    "        state = self.env.reset()\n",
    "        self.states.append(state) \n",
    "\n",
    "        return state\n",
    "\n",
    "    \n",
    "    def add_to_trajectory(self, action, next_state, reward):\n",
    "        \"\"\"\n",
    "        Stores in memory the action, next_state and reward. This will later be used for updates.\n",
    "        \n",
    "        Args:\n",
    "            action (int) : the selected action in the current state.\n",
    "            action (int) : the reward after selectin the action.\n",
    "            next_state (array of floats) : the next state returned by the environment after selecting the action.\n",
    "        Returns:\n",
    "            next_state (array of float) : the next state returned by the environment after selecting the action.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.total_rewards += reward\n",
    "        \n",
    "        self.rewards.append(reward)\n",
    "        self.actions.append(action)   \n",
    "        self.states.append(next_state)\n",
    "\n",
    "        return next_state\n",
    "    \n",
    "\n",
    "    def fit(self, num_episodes=1000, save_every=5):\n",
    "        \"\"\"\n",
    "        Implements the training loop. \n",
    "        \n",
    "        Args:\n",
    "            num_episodes (int) : the number of episodes we train the agent.\n",
    "            save_every (int) : the rate at which we save the results, which will be used for visualization.\n",
    "        \"\"\"\n",
    "        \n",
    "        save_file, stats = self.setup_saving()\n",
    "\n",
    "        for i_episode in range(num_episodes):\n",
    "            if i_episode % save_every == 0 and i_episode != 0:\n",
    "                np.savetxt(save_file,stats,delimiter=',') \n",
    "\n",
    "            state = self.initialize_episode()\n",
    "            done=False\n",
    "            t=0\n",
    "\n",
    "            while not done:\n",
    "                t+=1\n",
    "                action = self.predict(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                state = self.add_to_trajectory(action, next_state, reward)\n",
    "                if i_episode%50 ==0:self.env.render()\n",
    "\n",
    "            print(\"\\rEpisode {} Total Rewards {} \".format(i_episode, self.total_rewards) )\n",
    "            stats.append(t)\n",
    "            self.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REINFORCE(env).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent taking random actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = wrappers.Monitor(env, \"./gym-results\", force=True)\n",
    "observation = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(200):\n",
    "    for timestep in range(100):\n",
    "        env.render()\n",
    "    \n",
    "        print(observation)\n",
    "    \n",
    "        # Here we’ve chosen to “sample” the action space to get a random action, of which,\n",
    "        # there are only two: move left or move right.\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)  # take a random action\n",
    "\n",
    "        #if env_opt:\n",
    "        #    env.reset()\n",
    "        if done:\n",
    "            print(\"Completed after {} timesteps.\".format(timestep + 1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close environment\n",
    "env.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "video = io.open('./gym-results/openaigym.video.%s.video000000.mp4' % env.file_infix, 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''\n",
    "    <video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''\n",
    ".format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want to do is train an agent to find a policy. we want to train our agent to find a good policy for the CartPole problem. Specifically, we want our agent to learn an ideally optimal policy that takes the four observation values and then make a decision as to what action to take (i.e. move right or move left) given the values the agent is observing at any given time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reading material:\n",
    "\n",
    "- http://testerstories.com/2017/12/the-tester-role-in-machine-learning-part-1/\n",
    "- http://testerstories.com/2017/12/the-tester-role-in-machine-learning-part-2/\n",
    "- http://testerstories.com/2017/12/the-tester-role-in-machine-learning-part-3/\n",
    "- http://testerstories.com/2017/12/the-tester-role-in-machine-learning-part-4/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cartpole Environment Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(0) \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Watch a Random Agent\n",
    "\n",
    "It is useful to see how well the agent performs, before training train the agent. We can run the code cell below to watch how well the agent does, if it just pushes the cart randomly at each timestep. We can think of it as flipping a fair coin when deciding whether to push the cart to the left or to the right.\n",
    "\n",
    "We run the code cell multiple times, to check the score for different episodes (or game rounds). It likely won't get a score above 30, and this is to be expected! Later we'll train the agent to consistently get a score larger than 195!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "img = plt.imshow(env.render(mode='rgb_array'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(1000):\n",
    "\n",
    "    action = env.action_space.sample()\n",
    "    img.set_data(env.render(mode='rgb_array')) \n",
    "\n",
    "    plt.axis('off')\n",
    "\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    state, reward, done, _ = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        print('Score: ', t+1)\n",
    "        break\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Architecture of the Policy\n",
    "\n",
    "We will define a neural network that encodes the agent's stochastic policy.\n",
    "\n",
    "CartPole environment details:\n",
    "\n",
    "- The agent has two possible actions: it can either push the cart to the left or to the right.\n",
    "- The state at each timestep always has four numbers, corresponding to the position and velocity of the cart, along with the pole angle and velocity.\n",
    "- The network that you will define takes the environment state as input. It returns as output the probability that the agent should select each possible action.\n",
    "\n",
    "For example, when the agent observes a new state, it passes the state as input to the network. The network returns two numbers, corresponding to the probability that the agent will select each action. So, for instance, if the network returns [0.9, 0.1], the agent pushes the car to the left with 90% probability, and otherwise pushes the car to the right. Then the agent samples from the action space using these probabilities - say it ends up selecting the action that pushes the cart to the left. After selecting this action, it sends the action to the environment and receives a reward and next state. This next state is then fed as input to the network, and so on.\n",
    "\n",
    "When we initialize the neural network, all of the weights are random. Our agent's goal then will be to figure out the appropriate weights in the neural network, so that for each state, the network always outputs probabilities that encode a good game-playing strategy, and help the agent get a high score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size=4, h_size=16, a_size=2):\n",
    "        \"\"\"Neural network that encodes the policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            s_size (int): dimension of each state (also size of input layer)\n",
    "            h_size (int): size of hidden layer\n",
    "            a_size (int): number of potential actions (also size of output layer)\n",
    "        \"\"\"\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        \n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent with Reinforcement Learning\n",
    "\n",
    "We'll use the reinforcement algorithm, also known as Monte Carlo Policy Gradients, to guide the agent to train the weights of the neural network, while it's playing the game.\n",
    "\n",
    "For now, run the training on the network. The OpenAI Gym considers the environment as \"solved\", if the average score over 100 episodes is at least 195.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy().to(device)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(n_episodes=1000, max_t=200, gamma=1.0, print_every=100):\n",
    "    \"\"\"PyTorch implementation of the REINFORCE algorithm.\n",
    "        \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        gamma (float): discount rate\n",
    "        print_every (int): how often to print average score (over last 100 episodes)\n",
    "    \"\"\"\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        for t in range(max_t):\n",
    "            action, log_prob = policy.act(state)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break \n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "        \n",
    "        discounts = [gamma**i for i in range(len(rewards)+1)]\n",
    "        R = sum([a*b for a,b in zip(discounts, rewards)])\n",
    "        \n",
    "        policy_loss = []\n",
    "        for log_prob in saved_log_probs:\n",
    "            policy_loss.append(-log_prob * R)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        if np.mean(scores_deque)>=195.0:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_deque)))\n",
    "            break\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot the Scores\n",
    "scores = reinforce()\n",
    "Run the code cell below to plot the scores that were received by the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Watch a Smart Agent\n",
    "Finally, we can watch our smart agent! Doesn't it do much better than the random agent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "img = plt.imshow(env.render(mode='rgb_array'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(1000):\n",
    "    action, _ = policy.act(state)\n",
    "    img.set_data(env.render(mode='rgb_array')) \n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        print('Score: ', t+1)\n",
    "        break\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "\n",
    "We are now in the position of trying to train our agent using much more advanced reinforcement learning methods.\n",
    "\n",
    "https://github.com/y2ee201/Coursera-Practical-RL-NRUSHE/blob/master/practice_approx_qlearning.ipynb\n",
    "\n",
    "\n",
    "https://github.com/y2ee201/Coursera-Practical-RL-NRUSHE/blob/master/sarsa.ipynb\n",
    "\n",
    "https://github.com/y2ee201/Coursera-Practical-RL-NRUSHE/blob/master/qlearning.ipynb\n",
    "\n",
    "https://github.com/y2ee201/Coursera-Practical-RL-NRUSHE/blob/master/practice_mcts.ipynb\n",
    "\n",
    "https://github.com/y2ee201/Coursera-Practical-RL-NRUSHE/blob/master/practice_reinforce.ipynb\n",
    "\n",
    "https://github.com/y2ee201/Coursera-Practical-RL-NRUSHE/blob/master/practice_vi.ipynb\n",
    "\n",
    "https://github.com/y2ee201/Coursera-Practical-RL-NRUSHE/blob/master/crossentropy_method.ipynb\n",
    "\n",
    "https://github.com/y2ee201/Coursera-Practical-RL-NRUSHE/blob/master/bandits.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearDeepQNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_games = 10000\n",
    "scores = []\n",
    "eps_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(input_dims=env.observation_space.shape,\n",
    "              n_actions=env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0 score 23.0 avg score 23.0 epsilon  1.00\n",
      "episode  100 score 90.0 avg score 24.8 epsilon  0.97\n",
      "episode  200 score 13.0 avg score 21.8 epsilon  0.95\n",
      "episode  300 score 46.0 avg score 23.3 epsilon  0.93\n",
      "episode  400 score 29.0 avg score 22.3 epsilon  0.91\n",
      "episode  500 score 11.0 avg score 22.0 epsilon  0.89\n",
      "episode  600 score 25.0 avg score 23.9 epsilon  0.86\n",
      "episode  700 score 15.0 avg score 21.7 epsilon  0.84\n",
      "episode  800 score 24.0 avg score 22.9 epsilon  0.82\n",
      "episode  900 score 15.0 avg score 24.3 epsilon  0.79\n",
      "episode  1000 score 33.0 avg score 28.4 epsilon  0.76\n",
      "episode  1100 score 68.0 avg score 22.3 epsilon  0.74\n",
      "episode  1200 score 20.0 avg score 23.4 epsilon  0.72\n",
      "episode  1300 score 20.0 avg score 24.8 epsilon  0.69\n",
      "episode  1400 score 16.0 avg score 25.0 epsilon  0.67\n",
      "episode  1500 score 24.0 avg score 25.3 epsilon  0.64\n",
      "episode  1600 score 12.0 avg score 28.2 epsilon  0.62\n",
      "episode  1700 score 35.0 avg score 28.3 epsilon  0.59\n",
      "episode  1800 score 34.0 avg score 32.0 epsilon  0.56\n",
      "episode  1900 score 31.0 avg score 25.9 epsilon  0.53\n",
      "episode  2000 score 14.0 avg score 26.6 epsilon  0.50\n",
      "episode  2100 score 90.0 avg score 33.6 epsilon  0.47\n",
      "episode  2200 score 104.0 avg score 30.6 epsilon  0.44\n",
      "episode  2300 score 9.0 avg score 34.1 epsilon  0.40\n",
      "episode  2400 score 28.0 avg score 33.9 epsilon  0.37\n",
      "episode  2500 score 28.0 avg score 35.9 epsilon  0.33\n",
      "episode  2600 score 12.0 avg score 41.4 epsilon  0.29\n",
      "episode  2700 score 27.0 avg score 45.3 epsilon  0.25\n",
      "episode  2800 score 125.0 avg score 34.9 epsilon  0.21\n",
      "episode  2900 score 66.0 avg score 54.9 epsilon  0.16\n",
      "episode  3000 score 10.0 avg score 48.2 epsilon  0.11\n",
      "episode  3100 score 115.0 avg score 70.1 epsilon  0.04\n",
      "episode  3200 score 42.0 avg score 69.0 epsilon  0.01\n",
      "episode  3300 score 42.0 avg score 45.8 epsilon  0.01\n",
      "episode  3400 score 27.0 avg score 30.5 epsilon  0.01\n",
      "episode  3500 score 17.0 avg score 21.6 epsilon  0.01\n",
      "episode  3600 score 14.0 avg score 19.7 epsilon  0.01\n",
      "episode  3700 score 23.0 avg score 21.5 epsilon  0.01\n",
      "episode  3800 score 18.0 avg score 19.3 epsilon  0.01\n",
      "episode  3900 score 16.0 avg score 16.4 epsilon  0.01\n",
      "episode  4000 score 20.0 avg score 20.2 epsilon  0.01\n",
      "episode  4100 score 19.0 avg score 22.7 epsilon  0.01\n",
      "episode  4200 score 11.0 avg score 19.8 epsilon  0.01\n",
      "episode  4300 score 9.0 avg score 9.9 epsilon  0.01\n",
      "episode  4400 score 10.0 avg score 9.3 epsilon  0.01\n",
      "episode  4500 score 60.0 avg score 25.0 epsilon  0.01\n",
      "episode  4600 score 49.0 avg score 45.2 epsilon  0.01\n",
      "episode  4700 score 43.0 avg score 41.1 epsilon  0.01\n",
      "episode  4800 score 28.0 avg score 36.6 epsilon  0.01\n",
      "episode  4900 score 35.0 avg score 29.2 epsilon  0.01\n",
      "episode  5000 score 28.0 avg score 28.5 epsilon  0.01\n",
      "episode  5100 score 28.0 avg score 25.4 epsilon  0.01\n",
      "episode  5200 score 26.0 avg score 23.6 epsilon  0.01\n",
      "episode  5300 score 22.0 avg score 24.2 epsilon  0.01\n",
      "episode  5400 score 30.0 avg score 25.0 epsilon  0.01\n",
      "episode  5500 score 21.0 avg score 27.2 epsilon  0.01\n",
      "episode  5600 score 29.0 avg score 25.3 epsilon  0.01\n",
      "episode  5700 score 28.0 avg score 26.5 epsilon  0.01\n",
      "episode  5800 score 28.0 avg score 31.3 epsilon  0.01\n",
      "episode  5900 score 42.0 avg score 39.4 epsilon  0.01\n",
      "episode  6000 score 86.0 avg score 53.2 epsilon  0.01\n",
      "episode  6100 score 58.0 avg score 57.0 epsilon  0.01\n",
      "episode  6200 score 42.0 avg score 53.5 epsilon  0.01\n",
      "episode  6300 score 64.0 avg score 49.6 epsilon  0.01\n",
      "episode  6400 score 49.0 avg score 47.8 epsilon  0.01\n",
      "episode  6500 score 40.0 avg score 45.7 epsilon  0.01\n",
      "episode  6600 score 57.0 avg score 46.9 epsilon  0.01\n",
      "episode  6700 score 28.0 avg score 51.0 epsilon  0.01\n",
      "episode  6800 score 46.0 avg score 49.9 epsilon  0.01\n",
      "episode  6900 score 49.0 avg score 44.0 epsilon  0.01\n",
      "episode  7000 score 48.0 avg score 45.7 epsilon  0.01\n",
      "episode  7100 score 38.0 avg score 47.5 epsilon  0.01\n",
      "episode  7200 score 42.0 avg score 49.0 epsilon  0.01\n",
      "episode  7300 score 51.0 avg score 54.2 epsilon  0.01\n",
      "episode  7400 score 44.0 avg score 51.7 epsilon  0.01\n",
      "episode  7500 score 56.0 avg score 50.7 epsilon  0.01\n",
      "episode  7600 score 500.0 avg score 254.3 epsilon  0.01\n",
      "episode  7700 score 11.0 avg score 173.1 epsilon  0.01\n",
      "episode  7800 score 9.0 avg score 43.1 epsilon  0.01\n",
      "episode  7900 score 10.0 avg score 9.4 epsilon  0.01\n",
      "episode  8000 score 11.0 avg score 9.3 epsilon  0.01\n",
      "episode  8100 score 10.0 avg score 9.4 epsilon  0.01\n",
      "episode  8200 score 500.0 avg score 145.5 epsilon  0.01\n",
      "episode  8300 score 59.0 avg score 182.5 epsilon  0.01\n",
      "episode  8400 score 48.0 avg score 50.0 epsilon  0.01\n",
      "episode  8500 score 31.0 avg score 30.8 epsilon  0.01\n",
      "episode  8600 score 33.0 avg score 24.9 epsilon  0.01\n",
      "episode  8700 score 32.0 avg score 22.4 epsilon  0.01\n",
      "episode  8800 score 21.0 avg score 21.8 epsilon  0.01\n",
      "episode  8900 score 21.0 avg score 23.0 epsilon  0.01\n",
      "episode  9000 score 22.0 avg score 23.4 epsilon  0.01\n",
      "episode  9100 score 30.0 avg score 24.8 epsilon  0.01\n",
      "episode  9200 score 28.0 avg score 26.6 epsilon  0.01\n",
      "episode  9300 score 26.0 avg score 28.8 epsilon  0.01\n",
      "episode  9400 score 24.0 avg score 29.7 epsilon  0.01\n",
      "episode  9500 score 27.0 avg score 32.9 epsilon  0.01\n",
      "episode  9600 score 48.0 avg score 35.0 epsilon  0.01\n",
      "episode  9700 score 37.0 avg score 39.4 epsilon  0.01\n",
      "episode  9800 score 31.0 avg score 40.6 epsilon  0.01\n",
      "episode  9900 score 43.0 avg score 44.2 epsilon  0.01\n"
     ]
    }
   ],
   "source": [
    "# iterate over the number of games\n",
    "for i in range(n_games):\n",
    "    score = 0\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.choose_action(obs)\n",
    "        obs_, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        # learn from the state, action, reward, and new state\n",
    "        agent.learn(obs, action, reward, obs_)\n",
    "        obs = obs_ # state old state to new state\n",
    "    # at the end of every episode append the score\n",
    "    scores.append(score)\n",
    "    eps_history.append(agent.epsilon) # agents epsilon\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        avg_score = np.mean(scores[-100:]) # last 100 episodes\n",
    "        print('episode ', i, 'score %.1f avg score %.1f epsilon  %.2f' % (score, avg_score, agent.epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_learning_curve' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5affae524f37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cartpole_naive_dqn.png'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_games\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplot_learning_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_learning_curve' is not defined"
     ]
    }
   ],
   "source": [
    "# plot learning curve\n",
    "filename = 'cartpole_naive_dqn.png'\n",
    "x = [i+1 for i in range(n_games)]\n",
    "plot_learning_curve(x, scores, eps_history, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
