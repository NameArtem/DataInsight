{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import argparse\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(n_obs, n_action, n_hidden_layer=1, n_neuron_per_layer=32,\n",
    "        activation='relu', loss='mse'):\n",
    "  \"\"\" A multi-layer perceptron \"\"\"\n",
    "  model = Sequential()\n",
    "  print(type(n_neuron_per_layer))\n",
    "  print(type(n_obs))\n",
    "  print(n_obs)\n",
    "  print(type(activation))\n",
    "  model.add(Dense(n_neuron_per_layer, input_dim=n_obs, activation=activation))\n",
    "  for _ in range(n_hidden_layer):\n",
    "    model.add(Dense(n_neuron_per_layer, activation=activation))\n",
    "  model.add(Dense(n_action, activation='linear'))\n",
    "  model.compile(loss=loss, optimizer=Adam())\n",
    "  print(model.summary())\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingEnv(gym.Env):\n",
    "  \"\"\"\n",
    "  A 3-stock (MSFT, IBM, QCOM) trading environment.\n",
    "  State: [# of stock owned, current stock prices, cash in hand]\n",
    "    - array of length n_stock * 2 + 1\n",
    "    - price is discretized (to integer) to reduce state space\n",
    "    - use close price for each stock\n",
    "    - cash in hand is evaluated at each step based on action performed\n",
    "  Action: sell (0), hold (1), and buy (2)\n",
    "    - when selling, sell all the shares\n",
    "    - when buying, buy as many as cash in hand allows\n",
    "    - if buying multiple stock, equally distribute cash in hand and then utilize the balance\n",
    "  \"\"\"\n",
    "  def __init__(self, train_data, init_invest=20000):\n",
    "    # data\n",
    "    self.stock_price_history = np.around(train_data) # round up to integer to reduce state space\n",
    "    self.n_stock, self.n_step = self.stock_price_history.shape\n",
    "\n",
    "    # instance attributes\n",
    "    self.init_invest = init_invest\n",
    "    self.cur_step = None\n",
    "    self.stock_owned = None\n",
    "    self.stock_price = None\n",
    "    self.cash_in_hand = None\n",
    "\n",
    "    # action space\n",
    "    self.action_space = spaces.Discrete(3**self.n_stock)\n",
    "\n",
    "    # observation space: give estimates in order to sample and build scaler\n",
    "    stock_max_price = self.stock_price_history.max(axis=1)\n",
    "    stock_range = [[0, init_invest * 2 // mx] for mx in stock_max_price]\n",
    "    price_range = [[0, mx] for mx in stock_max_price]\n",
    "    cash_in_hand_range = [[0, init_invest * 2]]\n",
    "    self.observation_space = spaces.MultiDiscrete(stock_range + price_range + cash_in_hand_range)\n",
    "\n",
    "    # seed and start\n",
    "    self._seed()\n",
    "    self._reset()\n",
    "\n",
    "\n",
    "  def _seed(self, seed=None):\n",
    "    self.np_random, seed = seeding.np_random(seed)\n",
    "    return [seed]\n",
    "\n",
    "\n",
    "  def _reset(self):\n",
    "    self.cur_step = 0\n",
    "    self.stock_owned = [0] * self.n_stock\n",
    "    self.stock_price = self.stock_price_history[:, self.cur_step]\n",
    "    self.cash_in_hand = self.init_invest\n",
    "    return self._get_obs()\n",
    "\n",
    "\n",
    "  def _step(self, action):\n",
    "    assert self.action_space.contains(action)\n",
    "    prev_val = self._get_val()\n",
    "    self.cur_step += 1\n",
    "    self.stock_price = self.stock_price_history[:, self.cur_step] # update price\n",
    "    self._trade(action)\n",
    "    cur_val = self._get_val()\n",
    "    reward = cur_val - prev_val\n",
    "    done = self.cur_step == self.n_step - 1\n",
    "    info = {'cur_val': cur_val}\n",
    "    return self._get_obs(), reward, done, info\n",
    "\n",
    "\n",
    "  def _get_obs(self):\n",
    "    obs = []\n",
    "    obs.extend(self.stock_owned)\n",
    "    obs.extend(list(self.stock_price))\n",
    "    obs.append(self.cash_in_hand)\n",
    "    return obs\n",
    "\n",
    "\n",
    "  def _get_val(self):\n",
    "    return np.sum(self.stock_owned * self.stock_price) + self.cash_in_hand\n",
    "\n",
    "\n",
    "  def _trade(self, action):\n",
    "    # all combo to sell(0), hold(1), or buy(2) stocks\n",
    "    action_combo = map(list, itertools.product([0, 1, 2], repeat=self.n_stock))\n",
    "    action_vec = action_combo[action]\n",
    "\n",
    "    # one pass to get sell/buy index\n",
    "    sell_index = []\n",
    "    buy_index = []\n",
    "    for i, a in enumerate(action_vec):\n",
    "      if a == 0:\n",
    "        sell_index.append(i)\n",
    "      elif a == 2:\n",
    "        buy_index.append(i)\n",
    "\n",
    "    # two passes: sell first, then buy; might be naive in real-world settings\n",
    "    if sell_index:\n",
    "      for i in sell_index:\n",
    "        self.cash_in_hand += self.stock_price[i] * self.stock_owned[i]\n",
    "        self.stock_owned[i] = 0\n",
    "    if buy_index:\n",
    "      can_buy = True\n",
    "      while can_buy:\n",
    "        for i in buy_index:\n",
    "          if self.cash_in_hand > self.stock_price[i]:\n",
    "            self.stock_owned[i] += 1 # buy one share\n",
    "            self.cash_in_hand -= self.stock_price[i]\n",
    "          else:\n",
    "            can_buy = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(object):\n",
    "  \"\"\" A simple Deep Q agent \"\"\"\n",
    "  def __init__(self, state_size, action_size):\n",
    "    self.state_size = state_size[0] #modified added [0]\n",
    "    self.action_size = action_size\n",
    "    self.memory = deque(maxlen=2000)\n",
    "    self.gamma = 0.95  # discount rate\n",
    "    self.epsilon = 1.0  # exploration rate\n",
    "    self.epsilon_min = 0.01\n",
    "    self.epsilon_decay = 0.995\n",
    "    self.model = mlp(state_size[0], action_size)  #modified added [0]\n",
    "\n",
    "\n",
    "  def remember(self, state, action, reward, next_state, done):\n",
    "    self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "\n",
    "  def act(self, state):\n",
    "    if np.random.rand() <= self.epsilon:\n",
    "      return random.randrange(self.action_size)\n",
    "    act_values = self.model.predict(state)\n",
    "    return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "\n",
    "  def replay(self, batch_size=32):\n",
    "    \"\"\" vectorized implementation; 30x speed up compared with for loop \"\"\"\n",
    "    minibatch = random.sample(self.memory, batch_size)\n",
    "\n",
    "    states = np.array([tup[0][0] for tup in minibatch])\n",
    "    actions = np.array([tup[1] for tup in minibatch])\n",
    "    rewards = np.array([tup[2] for tup in minibatch])\n",
    "    next_states = np.array([tup[3][0] for tup in minibatch])\n",
    "    done = np.array([tup[4] for tup in minibatch])\n",
    "\n",
    "    # Q(s', a)\n",
    "    target = rewards + self.gamma * np.amax(self.model.predict(next_states), axis=1)\n",
    "    # end state target is reward itself (no lookahead)\n",
    "    target[done] = rewards[done]\n",
    "\n",
    "    # Q(s, a)\n",
    "    target_f = self.model.predict(states)\n",
    "    # make the agent to approximately map the current state to future discounted reward\n",
    "    target_f[range(batch_size), actions] = target\n",
    "\n",
    "    self.model.fit(states, target_f, epochs=1, verbose=0)\n",
    "\n",
    "    if self.epsilon > self.epsilon_min:\n",
    "      self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "  def load(self, name):\n",
    "    self.model.load_weights(name)\n",
    "\n",
    "\n",
    "  def save(self, name):\n",
    "    self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(col='close'):\n",
    "  \"\"\" Returns a 3 x n_step array \"\"\"\n",
    "  msft = pd.read_csv('data/daily_MSFT.csv', usecols=[col])\n",
    "  ibm = pd.read_csv('data/daily_IBM.csv', usecols=[col])\n",
    "  qcom = pd.read_csv('data/daily_QCOM.csv', usecols=[col])\n",
    "  # recent price are at top; reverse it\n",
    "  return np.array([msft[col].values[::-1],\n",
    "                   ibm[col].values[::-1],\n",
    "                   qcom[col].values[::-1]])\n",
    "\n",
    "\n",
    "def get_scaler(env):\n",
    "  \"\"\" Takes a env and returns a scaler for its observation space \"\"\"\n",
    "  low = [0] * (env.n_stock * 2 + 1)\n",
    "\n",
    "  high = []\n",
    "  max_price = env.stock_price_history.max(axis=1)\n",
    "  min_price = env.stock_price_history.min(axis=1)\n",
    "  max_cash = env.init_invest * 3 # 3 is a magic number...\n",
    "  max_stock_owned = max_cash // min_price\n",
    "  for i in max_stock_owned:\n",
    "    high.append(i)\n",
    "  for i in max_price:\n",
    "    high.append(i)\n",
    "  high.append(max_cash)\n",
    "\n",
    "  scaler = StandardScaler()\n",
    "  scaler.fit([low, high])\n",
    "  return scaler\n",
    "\n",
    "\n",
    "def maybe_make_dir(directory):\n",
    "  if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['-w', '--weights'], dest='weights', nargs=None, const=None, default=None, type=<class 'str'>, choices=None, help='a trained model weights', metavar=None)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-e', '--episode', type=int, default=2000,\n",
    "                    help='number of episode to run')\n",
    "parser.add_argument('-b', '--batch_size', type=int, default=32,\n",
    "                    help='batch size for experience replay')\n",
    "parser.add_argument('-i', '--initial_invest', type=int, default=20000,\n",
    "                    help='initial investment amount')\n",
    "parser.add_argument('-m', '--mode', type=str, required=True,\n",
    "                    help='either \"train\" or \"test\"')\n",
    "parser.add_argument('-w', '--weights', type=str, help='a trained model weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_size= (7, 2)\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "7\n",
      "<class 'str'>\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 32)                256       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 27)                891       \n",
      "=================================================================\n",
      "Total params: 2,203\n",
      "Trainable params: 2,203\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-e3c341959b28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m   \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m   \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/wibas/lib/python3.7/site-packages/gym/core.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \"\"\"\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "maybe_make_dir('weights')\n",
    "maybe_make_dir('portfolio_val')\n",
    "\n",
    "timestamp = time.strftime('%Y%m%d%H%M')\n",
    "\n",
    "data = np.around(get_data())\n",
    "train_data = data[:, :3526]\n",
    "test_data = data[:, 3526:]\n",
    "\n",
    "args_initial_invest=20000 #args.initial_invest\n",
    "args_weights=\"weights\" #args.weights\n",
    "args_mode=\"train\" #args.mode\n",
    "args_episode=2000 #args.episode\n",
    "args_batch_size=32 #args.batch_size\n",
    "\n",
    "env = TradingEnv(train_data, args_initial_invest)\n",
    "state_size = env.observation_space.shape\n",
    "\n",
    "action_size = env.action_space.n\n",
    "print('state_size=',state_size)\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "scaler = get_scaler(env)\n",
    "\n",
    "portfolio_value = []\n",
    "\n",
    "if args_mode == 'test':\n",
    "  # remake the env with test data\n",
    "  env = TradingEnv(test_data, args_initial_invest)\n",
    "  # load trained weights\n",
    "  agent.load(args_weights)\n",
    "  # when test, the timestamp is same as time when weights was trained\n",
    "  timestamp = re.findall(r'\\d{12}', args_weights)[0]\n",
    "\n",
    "for e in range(args_episode):\n",
    "  state = env.reset()\n",
    "  state = scaler.transform([state])\n",
    "  for time in range(env.n_step):\n",
    "    action = agent.act(state)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    next_state = scaler.transform([next_state])\n",
    "    if args_mode == 'train':\n",
    "      agent.remember(state, action, reward, next_state, done)\n",
    "    state = next_state\n",
    "    if done:\n",
    "      print(\"episode: {}/{}, episode end value: {}\".format(\n",
    "       e + 1, args_episode, info['cur_val']))\n",
    "      portfolio_value.append(info['cur_val']) # append episode end portfolio value\n",
    "      break\n",
    "    if args_mode == 'train' and len(agent.memory) > args_batch_size:\n",
    "      agent.replay(args_batch_size)\n",
    "  if args_mode == 'train' and (e + 1) % 10 == 0:  # checkpoint weights\n",
    "    agent.save('weights/{}-dqn.h5'.format(timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save portfolio value history to disk\n",
    "with open('portfolio_val/{}-{}.p'.format(timestamp, args_mode), 'wb') as fp:\n",
    "    pickle.dump(portfolio_value, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
